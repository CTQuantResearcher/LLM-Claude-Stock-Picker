{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzP_zCtmke1i",
        "outputId": "0a700a9f-c90b-44d3-b45c-42ae53480df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2024.12.14)\n",
            "Requirement already satisfied: yfinance[optional] in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "\u001b[33mWARNING: yfinance 0.2.52 does not provide the extra 'optional'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance[optional]) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance[optional]) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance[optional]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance[optional]) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance[optional]) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance[optional]) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance[optional]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance[optional]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance[optional]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance[optional]) (2024.12.14)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=645a07eeb0cfd6daff39c41bebf4a800344b4174c5735e73ffd1e5a3aac51e80\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Collecting ripser\n",
            "  Downloading ripser-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from ripser) (3.0.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ripser) (1.26.4)\n",
            "Collecting persim (from ripser)\n",
            "  Downloading persim-0.3.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ripser) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from ripser) (1.6.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from persim->ripser) (1.2.15)\n",
            "Collecting hopcroftkarp (from persim->ripser)\n",
            "  Downloading hopcroftkarp-1.2.5.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from persim->ripser) (1.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from persim->ripser) (3.10.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->ripser) (3.5.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->persim->ripser) (1.17.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (4.55.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->persim->ripser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->persim->ripser) (1.17.0)\n",
            "Downloading ripser-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (841 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.3/841.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading persim-0.3.7-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hopcroftkarp\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18102 sha256=aacc5c9d316d6830dc34015303a864939e2b5eedfc7d6a8810e13952b47b4298\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/cc/2d/de23a8b9ae586817b0b44de4a4b1a08f23473e248a644b312f\n",
            "Successfully built hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, persim, ripser\n",
            "Successfully installed hopcroftkarp-1.2.5 persim-0.3.7 ripser-0.6.10\n",
            "Collecting fredapi\n",
            "  Downloading fredapi-0.5.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fredapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.17.0)\n",
            "Downloading fredapi-0.5.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: fredapi\n",
            "Successfully installed fredapi-0.5.2\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n",
            "Collecting arch\n",
            "  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n",
            "Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.3/985.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: arch\n",
            "Successfully installed arch-7.2.0\n",
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.11)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.3.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.1.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n",
            "Collecting pykalman\n",
            "  Downloading pykalman-0.9.7-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.13.1)\n",
            "Downloading pykalman-0.9.7-py2.py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykalman\n",
            "Successfully installed pykalman-0.9.7\n",
            "Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pandas_datareader) (5.3.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.11/dist-packages (from pandas_datareader) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pandas_datareader) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas_datareader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas_datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas_datareader) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas_datareader) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas_datareader) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas_datareader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas_datareader) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas_datareader) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas_datareader) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance requests beautifulsoup4\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "!pip install \"yfinance[optional]\"\n",
        "!pip install ta\n",
        "!pip install --upgrade ta\n",
        "!pip install ripser\n",
        "!pip install fredapi\n",
        "!pip install plotly scikit-learn\n",
        "!pip install PyWavelets\n",
        "!pip install arch\n",
        "!pip install pmdarima\n",
        "!pip install --upgrade pandas\n",
        "!pip install pykalman\n",
        "!pip install --upgrade pandas_datareader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wua3sy3kedH"
      },
      "outputs": [],
      "source": [
        "ANTHROPIC_API_KEY = \"get Anthropic Key\"  # Replace with your Anthropic API key\n",
        "FRED_API_KEY = \"Get Fred Key\"\n",
        "api_key = \"Get Financial Model Prep Key\" #Model Prep\n",
        "\n",
        "# Alpha Vantage API key\n",
        "API_KEY = 'Get Alpha Vantage Key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc3dVPHNkbTH",
        "outputId": "d0930f50-7dad-446b-9017-b4a7a9314729"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching news: 100%|██████████| 500/500 [01:11<00:00,  7.01it/s]\n",
            "Analyzing QQQ:   0%|          | 0/4 [00:00<?, ?ticker/s]    ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary//QQQ?modules=institutionOwnership%2CfundOwnership%2CmajorDirectHolders%2CmajorHoldersBreakdown%2CinsiderTransactions%2CinsiderHolders%2CnetSharePurchaseActivity&corsDomain=finance.yahoo.com&formatted=false&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/QQQ?modules=recommendationTrend&corsDomain=finance.yahoo.com&formatted=false&symbol=QQQ&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/QQQ?modules=calendarEvents&corsDomain=finance.yahoo.com&formatted=false&symbol=QQQ&crumb=ZUEEpcpNK47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No transcripts found for QQQ in 2023\n",
            "No transcripts found for QQQ in 2024\n",
            "No transcripts found for QQQ in 2025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing SPY:  25%|██▌       | 1/4 [01:24<04:14, 84.70s/ticker]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyses for QQQ:\n",
            "1. Ticker symbol: QQQ\n",
            "2. Current price: $514.21\n",
            "3. Estimated upside/downside potential:\n",
            "   - 1 year: 7.40%\n",
            "   - 2 years: 2.69%\n",
            "   - 3 years: 4.69%\n",
            "   - 4 years: 14.50%\n",
            "   - 5 years: 33.39%\n",
            "4. Estimated Probabilities of achieving upside/downside potential:\n",
            "   - 1 year: 45.22% (25th-75th: 44.20% - 46.40%)\n",
            "   - 2 years: 55.82% (25th-75th: 54.70% - 56.80%)\n",
            "   - 3 years: 56.94% (25th-75th: 55.40% - 57.60%)\n",
            "   - 4 years: 52.32% (25th-75th: 50.90% - 53.10%)\n",
            "   - 5 years: 43.99% (25th-75th: 43.00% - 45.10%)\n",
            "5. Weighted average probability of achieving the target price: 49.92%\n",
            "6. Investment thesis: QQQ is a well-diversified ETF that provides exposure to the largest non-financial companies listed on the Nasdaq exchange, making it an attractive long-term investment opportunity given the strong growth potential of the technology and consumer discretionary sectors.\n",
            "7. Key catalyst or trend supporting the thesis: The continued expansion of artificial intelligence, cloud computing, and the Internet of Things are expected to drive significant growth in the technology sector, which makes up over 50% of QQQ's portfolio. Additionally, the rising demand for electric vehicles and renewable energy solutions presents opportunities for QQQ's holdings in these emerging industries.\n",
            "8. Investment score: 85/100. QQQ's strong performance history, diversified portfolio, and exposure to high-growth sectors make it a compelling investment. However, the fund's concentration in a few large tech companies and potential market volatility due to macroeconomic factors present some risks that investors should consider.\n",
            "9. Industry and sector: Technology sector. The global technology industry is estimated to be worth over $5 trillion, with a projected annual growth rate of 8-10% over the next 5 years. The sector is characterized by high barriers to entry, rapid technological advancements, and a dynamic regulatory environment that requires companies to continuously innovate and adapt.\n",
            "10. Comprehensive Market Score: 8.5/10. QQQ's growth profile, financial health, and sector dynamics are strong, with the fund's diversification and exposure to high-growth industries offsetting potential macroeconomic and regulatory risks. The positive sentiment and institutional demand for the fund further support its market position.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary//SPY?modules=institutionOwnership%2CfundOwnership%2CmajorDirectHolders%2CmajorHoldersBreakdown%2CinsiderTransactions%2CinsiderHolders%2CnetSharePurchaseActivity&corsDomain=finance.yahoo.com&formatted=false&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/SPY?modules=recommendationTrend&corsDomain=finance.yahoo.com&formatted=false&symbol=SPY&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/SPY?modules=calendarEvents&corsDomain=finance.yahoo.com&formatted=false&symbol=SPY&crumb=ZUEEpcpNK47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No transcripts found for SPY in 2023\n",
            "No transcripts found for SPY in 2024\n",
            "No transcripts found for SPY in 2025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing MGK:  50%|█████     | 2/4 [03:09<03:12, 96.25s/ticker]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyses for SPY:\n",
            "1. Ticker symbol: SPY\n",
            "2. Current price: $599.37\n",
            "3. Estimated upside/downside potential:\n",
            "   - 1 year: 7.86%\n",
            "   - 2 years: 1.98%\n",
            "   - 3 years: 0.85%\n",
            "   - 4 years: 4.88%\n",
            "   - 5 years: 14.43%\n",
            "4. Estimated Probabilities of achieving upside/downside potential:\n",
            "   - 1 year: 46.06% (25th-75th: 45.20% - 47.30%)\n",
            "   - 2 years: 65.27% (25th-75th: 64.20% - 66.20%)\n",
            "   - 3 years: 70.71% (25th-75th: 69.70% - 71.60%)\n",
            "   - 4 years: 70.23% (25th-75th: 69.00% - 70.90%)\n",
            "   - 5 years: 65.39% (25th-75th: 64.40% - 66.50%)\n",
            "5. Weighted average probability of achieving the target price: 58.21%\n",
            "6. Investment thesis: SPY is a well-diversified, large-cap index fund that provides exposure to the broad U.S. equity market. The fund's strong historical performance, low expense ratio, and potential for long-term capital appreciation make it an attractive investment option for investors seeking broad market exposure.\n",
            "7. Key catalyst or trend supporting the thesis: The continued growth and resilience of the U.S. economy, as evidenced by positive economic indicators such as low unemployment, steady consumer spending, and moderate inflation, are expected to support the overall performance of the S&P 500 index and, consequently, SPY.\n",
            "8. Investment score: 82/100. The analysis indicates that SPY is a relatively low-risk investment with a high probability of achieving moderate long-term returns. The fund's diversification, low fees, and exposure to the broader U.S. equity market make it a solid core holding in a well-balanced portfolio. However, the analysis also highlights the potential for market volatility and the need to monitor economic and geopolitical developments that could impact the fund's performance.\n",
            "9. Industry and sector: SPY is a broad-based index fund that tracks the S&P 500 index, which represents a diverse range of sectors and industries within the U.S. equity market. The overall U.S. equity market is expected to continue its long-term growth trajectory, supported by a resilient economy, technological advancements, and the ongoing shift towards a more sustainable and innovative business landscape.\n",
            "10. Comprehensive Market Score: 8.5/10. The analysis indicates that SPY is a well-diversified, low-cost index fund that provides exposure to the broad U.S. equity market. The fund's strong growth profile, solid financial health, and favorable sector dynamics, combined with the positive macroeconomic outlook and generally positive market sentiment, make it an attractive investment option for long-term investors seeking broad market exposure.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary//MGK?modules=institutionOwnership%2CfundOwnership%2CmajorDirectHolders%2CmajorHoldersBreakdown%2CinsiderTransactions%2CinsiderHolders%2CnetSharePurchaseActivity&corsDomain=finance.yahoo.com&formatted=false&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/MGK?modules=recommendationTrend&corsDomain=finance.yahoo.com&formatted=false&symbol=MGK&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/MGK?modules=calendarEvents&corsDomain=finance.yahoo.com&formatted=false&symbol=MGK&crumb=ZUEEpcpNK47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No transcripts found for MGK in 2023\n",
            "No transcripts found for MGK in 2024\n",
            "No transcripts found for MGK in 2025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing DIA:  75%|███████▌  | 3/4 [04:27<01:28, 88.14s/ticker]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyses for MGK:\n",
            "1. Ticker symbol: MGK\n",
            "2. Current price: $343.85\n",
            "3. Estimated upside/downside potential:\n",
            "   - 1 year: 13.82%\n",
            "   - 2 years: 12.71%\n",
            "   - 3 years: 16.10%\n",
            "   - 4 years: 25.15%\n",
            "   - 5 years: 41.05%\n",
            "4. Estimated Probabilities of achieving upside/downside potential:\n",
            "   - 1 year: 47.14% (25th-75th: 46.00% - 48.20%)\n",
            "   - 2 years: 65.73% (25th-75th: 64.70% - 66.80%)\n",
            "   - 3 years: 72.03% (25th-75th: 71.10% - 73.10%)\n",
            "   - 4 years: 73.12% (25th-75th: 72.00% - 73.83%)\n",
            "   - 5 years: 70.43% (25th-75th: 69.50% - 71.30%)\n",
            "5. Weighted average probability of achieving the target price: 59.73%\n",
            "6. Investment thesis: MGK is a well-diversified large-cap growth ETF that has consistently outperformed the broader market, driven by its exposure to leading technology and consumer discretionary companies poised for long-term growth.\n",
            "7. Key catalyst or trend supporting the thesis: The ongoing shift towards emerging technologies, such as artificial intelligence and 5G, is expected to drive strong demand for the products and services offered by MGK's top holdings, creating a favorable tailwind for the fund's performance.\n",
            "8. Investment score: 85/100 - MGK's strong long-term growth potential, low expense ratio, and diversified portfolio of high-quality companies make it an attractive investment opportunity. However, the fund's heavy exposure to the technology sector and concentration in a few mega-cap stocks pose some risks that should be monitored.\n",
            "9. Industry and sector: The global semiconductor manufacturing industry, which is a key component of MGK's portfolio, is expected to experience robust growth driven by the increasing demand for advanced semiconductor chips across various applications, including 5G, AI, and electric vehicles. However, the industry also faces challenges such as geopolitical tensions, technological disruptions, and the need for continuous innovation.\n",
            "10. Comprehensive Market Score: 8.5/10 - MGK's growth profile, financial health, and sector dynamics are strong, with the fund's exposure to high-growth technology and consumer discretionary companies providing a favorable outlook. While macroeconomic factors and sentiment indicators pose some risks, the overall market environment is supportive of MGK's long-term performance.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary//DIA?modules=institutionOwnership%2CfundOwnership%2CmajorDirectHolders%2CmajorHoldersBreakdown%2CinsiderTransactions%2CinsiderHolders%2CnetSharePurchaseActivity&corsDomain=finance.yahoo.com&formatted=false&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/DIA?modules=recommendationTrend&corsDomain=finance.yahoo.com&formatted=false&symbol=DIA&crumb=ZUEEpcpNK47\n",
            "ERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/DIA?modules=calendarEvents&corsDomain=finance.yahoo.com&formatted=false&symbol=DIA&crumb=ZUEEpcpNK47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No transcripts found for DIA in 2023\n",
            "No transcripts found for DIA in 2024\n",
            "No transcripts found for DIA in 2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing DIA: 100%|██████████| 4/4 [06:08<00:00, 92.00s/ticker]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyses for DIA:\n",
            "1. Ticker symbol: DIA\n",
            "2. Current Price: $447.12\n",
            "3. Estimated Upside/Downside Potential:\n",
            "   - 1 year: 7.47%\n",
            "   - 2 years: 1.21% \n",
            "   - 3 years: -0.69%\n",
            "   - 4 years: 2.00%\n",
            "   - 5 years: 9.51%\n",
            "4. Estimated Probabilities of Achieving Target Price:\n",
            "   - 1 year: 47.03% (25th-75th: 45.80% - 47.80%)\n",
            "   - 2 years: 66.47% (25th-75th: 65.50% - 67.50%) \n",
            "   - 3 years: 72.45% (25th-75th: 71.70% - 73.60%)\n",
            "   - 4 years: 73.26% (25th-75th: 72.00% - 74.00%)\n",
            "   - 5 years: 69.63% (25th-75th: 68.80% - 70.80%)\n",
            "5. Weighted Average Probability: 59.85%\n",
            "6. Investment Thesis: DIA, a leading aerospace and defense company, has consistently outperformed industry benchmarks in terms of financial performance, market positioning, and competitive advantages. The index's diversified sector exposure, value-oriented composition, and potential to benefit from macroeconomic and industry-specific tailwinds make it an attractive investment option for investors seeking exposure to large-cap US equities.\n",
            "7. Key Catalyst: The inclusion of Amazon in the Dow Jones index is seen as a positive catalyst for DIA's performance, as the e-commerce giant's strong growth and market dominance could drive the index's overall gains.\n",
            "8. Investment Score: 85/100. DIA's strong brand recognition, diversified product portfolio, and global presence have allowed it to maintain a competitive edge in the aerospace and defense industry. The index's positive sentiment, emerging trends in the industry, and favorable financial data analysis support a high investment score. However, potential risks such as geopolitical tensions, regulatory changes, and environmental concerns may pose challenges.\n",
            "9. Industry and Sector: Aerospace and Defense. The global aerospace and defense industry is a large and diverse market, valued at approximately $1.5 trillion in 2022 and expected to grow at a CAGR of 4.5% from 2022 to 2027. The industry is characterized by high barriers to entry, technological advancements, and increasing demand for unmanned aerial vehicles and drones.\n",
            "10. Comprehensive Market Score: 8.5/10. DIA's strong growth profile, financial health, and favorable industry dynamics outweigh potential macroeconomic and sentiment-related risks. The index's diversification, competitive positioning, and exposure to emerging trends in the aerospace and defense sector make it an attractive investment opportunity.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import quad\n",
        "from scipy.optimize import minimize\n",
        "from scipy import stats\n",
        "from ta import add_all_ta_features\n",
        "from ta.utils import dropna\n",
        "from ta.volume import OnBalanceVolumeIndicator\n",
        "from ta.trend import ADXIndicator\n",
        "from ta.momentum import StochasticOscillator\n",
        "from ta.volatility import BollingerBands\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volume import ForceIndexIndicator\n",
        "from ta.trend import MassIndex\n",
        "from ta.volume import ChaikinMoneyFlowIndicator\n",
        "from ta.trend import MACD\n",
        "from ta.momentum import RSIIndicator\n",
        "from ta.volume import MFIIndicator\n",
        "from ta.trend import IchimokuIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import pandas_datareader as pdr\n",
        "from pandas_datareader import fred\n",
        "from ripser import ripser\n",
        "from persim import plot_diagrams\n",
        "from scipy.stats import linregress\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import itertools\n",
        "from scipy.stats import entropy\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from scipy.stats import pearsonr\n",
        "from numpy.random import permutation\n",
        "from scipy.signal import argrelextrema\n",
        "import pywt\n",
        "from scipy.signal import find_peaks\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.cluster import KMeans\n",
        "from pmdarima import auto_arima\n",
        "from arch import arch_model\n",
        "from statsmodels.tsa.api import VAR\n",
        "from scipy.stats import norm\n",
        "from pmdarima import auto_arima\n",
        "import warnings\n",
        "import pmdarima as pm\n",
        "import copy\n",
        "from scipy.stats import linregress\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from pykalman import KalmanFilter\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import genpareto\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from arch.utility.exceptions import DataScaleWarning\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from urllib.request import urlopen\n",
        "import certifi\n",
        "import numpy as np\n",
        "from arch import arch_model\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"ta.trend\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def requests_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504)):\n",
        "    session = requests.Session()\n",
        "    retry = Retry(\n",
        "        total=retries,\n",
        "        read=retries,\n",
        "        connect=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "def safe_mean(arr):\n",
        "    if len(arr) == 0:\n",
        "        return 0\n",
        "    return np.mean(arr)\n",
        "\n",
        "# Use this session for all requests\n",
        "session = requests_retry_session()\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, (np.integer, np.floating)):\n",
        "        return obj.item()\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, pd.Series):\n",
        "        return obj.to_dict()\n",
        "    elif isinstance(obj, pd.DataFrame):\n",
        "        return obj.to_dict(orient='records')\n",
        "    elif isinstance(obj, dict):\n",
        "        return {str(k): convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif isinstance(obj, (pd.Timestamp, datetime)):\n",
        "        return obj.isoformat()\n",
        "    else:\n",
        "        return str(obj)\n",
        "\n",
        "def get_current_price(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    todays_data = stock.history(period='1d')\n",
        "    if todays_data.empty:\n",
        "        raise ValueError(f\"No current price data available for {ticker}\")\n",
        "    return todays_data['Close'].iloc[-1]\n",
        "\n",
        "def get_article_text(url):\n",
        "    try:\n",
        "        response = session.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "        return article_text\n",
        "    except:\n",
        "        return \"Error retrieving article text.\"\n",
        "\n",
        "def get_economic_indicators():\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=30*365) # 20 years of data\n",
        "\n",
        "    indicators = {\n",
        "        'GDP': 'GDP',\n",
        "        'Inflation': 'CPIAUCSL',\n",
        "        'Unemployment': 'UNRATE',\n",
        "        'Interest Rate': 'DFF',\n",
        "        'Consumer Confidence': 'UMCSENT',\n",
        "        'Housing Starts': 'HOUST',\n",
        "        'Building Permits': 'PERMIT',\n",
        "        'Retail Sales': 'RSAFS',\n",
        "        'Industrial Production': 'INDPRO',\n",
        "        'Durable Goods Orders': 'DGORDER',\n",
        "        'Trade Balance': 'BOPGSTB',\n",
        "        'PPI': 'PPIACO',\n",
        "        'Capacity Utilization': 'TCU',\n",
        "        'JOLTS': 'JTSJOL',\n",
        "        'Personal Income': 'PI',\n",
        "        'Personal Spending': 'PCE',\n",
        "        'Business Inventories': 'BUSINV',\n",
        "        'Leading Economic Index': 'USSLIND',\n",
        "        'Yield Curve': 'T10Y2Y',\n",
        "        'Non-Farm Payrolls': 'PAYEMS',\n",
        "        'Core PCE Price Index': 'PCEPILFE',\n",
        "        'Initial Jobless Claims': 'ICSA',\n",
        "        'Existing Home Sales': 'EXHOSLUSM495S',\n",
        "        'New Home Sales': 'HSN1F',\n",
        "        'Balance of Trade': 'BOPGSTB',\n",
        "        'Government Debt to GDP': 'GFDEGDQ188S',\n",
        "        'Manufacturing Production': 'IPMAN',\n",
        "        'Energy Production': 'PCEC96',\n",
        "        'Real Estate Sales': 'RETAIL',\n",
        "        'M2': 'MYAGM2USM052S',\n",
        "        'Labor Force Participation Rate': 'CIVPART',\n",
        "        'S&P 500 Index': 'SP500',\n",
        "        'Average Hourly Earnings': 'CES0500000003',\n",
        "        'Productivity': 'OPHNFB',\n",
        "        'Unit Labor Costs': 'ULCNFB',\n",
        "        'Corporate Profits': 'CP',\n",
        "        'Federal Debt': 'GFDEBTN',\n",
        "        'Consumer Credit': 'TOTALSL',\n",
        "        'Household Debt to GDP': 'HDTGPDUSQ163N',\n",
        "        'Velocity of M2 Money Stock': 'M2V',\n",
        "        'Median Sales Price of Houses': 'MSPUS',\n",
        "        'Homeownership Rate': 'RHORUSQ156N',\n",
        "        'Mortgage Delinquency Rate': 'DRSFRMACBS',\n",
        "        'Commercial and Industrial Loans': 'BUSLOANS',\n",
        "        'Bank Prime Loan Rate': 'MPRIME',\n",
        "        'Total Vehicle Sales': 'TOTALSA',\n",
        "        'E-Commerce Retail Sales': 'ECOMSA',\n",
        "        'GDP Growth Rate': 'A191RL1Q225SBEA',\n",
        "        'Core Inflation Rate': 'CPILFESL',\n",
        "        'Consumer Credit Change': 'CCLACBW027SBOG',\n",
        "        'Retail Inventories': 'RETAILIMSA',\n",
        "        'Wholesale Inventories': 'WHLSLRIMSA',\n",
        "        'Factory Orders': 'AMTMNO',\n",
        "        'Construction Spending': 'TTLCONS',\n",
        "        'Private Sector Credit': 'CRDQUSAPABIS',\n",
        "        'Export Prices': 'IR',\n",
        "        'Import Prices': 'IQ',\n",
        "        'Philadelphia Fed Manufacturing Index': 'USPHCI',\n",
        "        'Chicago Fed National Activity Index': 'CFNAI',\n",
        "        'NFIB Small Business Optimism Index': 'NFCI',\n",
        "        'US Dollar Index': 'DTWEXBGS',\n",
        "        'VIX Volatility Index': 'VIXCLS',\n",
        "        'Global Crude Oil Prices': 'POILWTIUSDM',\n",
        "        'Empire State Manufacturing Index': 'GACDINA066MNFRBNY',\n",
        "        'Kansas City Fed Manufacturing Index': 'KCFSI',\n",
        "        'Henry Hub Natural Gas Spot Price': 'DHHNGSP',\n",
        "        'Copper Price': 'PCOPPUSDM',\n",
        "        'Corn Price': 'PMAIZMTUSDM',\n",
        "        'Soybean Price': 'PSOYBUSDM',\n",
        "        'Wheat Price': 'PWHEAMTUSDM',\n",
        "        'Global price of Aluminum': 'PALUMUSDM',\n",
        "        'Global price of Iron Ore': 'PIORECRUSDM',\n",
        "        'Global price of Cotton': 'PCOTTINDUSDM',\n",
        "        'Global price of Rubber': 'PRUBBUSDM',\n",
        "        'Inflation, consumer prices for the World': 'FPCPITOTLZGWLD',\n",
        "        'Global Price Index of All Commodities': 'PALLFNFINDEXM',\n",
        "        'Global price of Food and beverage index': 'PFANDBINDEXM',\n",
        "        'Global price of Metal index': 'PMETAINDEXM',\n",
        "        'Equity Market-related Economic Uncertainty Index': 'WLEMUINDXD',\n",
        "        'Economic Policy Uncertainty Index': 'USEPUINDXD',\n",
        "        'Total Business Inventories to Sales Ratio': 'ISRATIO',\n",
        "        'ISM Manufacturing PMI': 'MANEMP',\n",
        "        'Capacity Utilization: Manufacturing SIC': 'CUMFNS',\n",
        "        'China GDP Growth Rate': 'CHNGDPNQDSMEI',\n",
        "        'China Inflation Rate': 'CHNCPIALLMINMEI',\n",
        "        'China Unemployment Rate': 'LMUNRRTTCNQ156S',\n",
        "        'Japan GDP Growth Rate': 'JPNNGDP',\n",
        "        'Japan Inflation Rate': 'JPNCPIALLMINMEI',\n",
        "        'Japan Unemployment Rate': 'LRUNTTTTJPM156S',\n",
        "        'Eurozone GDP Growth Rate': 'EUNNGDP',\n",
        "        'Eurozone Inflation Rate': 'CP0000EZ19M086NEST',\n",
        "        'Eurozone Unemployment Rate': 'LRHUTTTTEZM156S',\n",
        "        'ECB Interest Rate': 'ECBDFR',\n",
        "        'World GDP Growth': 'NYGDPMKTPCDWLD',\n",
        "        'Global Inflation': 'FPCPITOTLZGWLD',\n",
        "    }\n",
        "\n",
        "    economic_data = {}\n",
        "\n",
        "    # Set the FRED API key globally\n",
        "    fred.key = FRED_API_KEY\n",
        "\n",
        "    for name, series in indicators.items():\n",
        "        try:\n",
        "            data = pdr.get_data_fred(series, start=start_date, end=end_date)\n",
        "            economic_data[name] = data\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {name} data: {str(e)}\")\n",
        "\n",
        "    return economic_data\n",
        "\n",
        "def get_index_data(indices, start_date, end_date):\n",
        "    index_data = {}\n",
        "    for index in indices:\n",
        "        try:\n",
        "            data = yf.Ticker(index).history(start=start_date, end=end_date)\n",
        "            index_data[index] = data\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for index {index}: {str(e)}\")\n",
        "    return index_data\n",
        "\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "def get_market_crash_risk():\n",
        "    try:\n",
        "        # Fetch required data\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=252*5)  # 5 years of data\n",
        "\n",
        "        # Fetch VIX and S&P 500 data\n",
        "        vix = yf.Ticker('^VIX').history(start=start_date, end=end_date)['Close']\n",
        "        sp500 = yf.Ticker('^GSPC').history(start=start_date, end=end_date)['Close']\n",
        "\n",
        "        # Fetch FRED data\n",
        "        fred_data = {}\n",
        "        fred_series = {\n",
        "            'Yield_Curve': 'T10Y2Y',\n",
        "            'Unemployment': 'UNRATE',\n",
        "            'CPI': 'CPIAUCSL',\n",
        "            'GDP': 'GDP'\n",
        "        }\n",
        "\n",
        "        for name, series in fred_series.items():\n",
        "            try:\n",
        "                fred_data[name] = pdr.get_data_fred(series, start=start_date, end=end_date)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {name} data: {str(e)}\")\n",
        "                fred_data[name] = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date))\n",
        "\n",
        "        # Convert all DatetimeIndex to timezone-naive\n",
        "        vix.index = vix.index.tz_localize(None)\n",
        "        sp500.index = sp500.index.tz_localize(None)\n",
        "        for name in fred_data:\n",
        "            fred_data[name].index = fred_data[name].index.tz_localize(None)\n",
        "\n",
        "        # Rename series for consistency\n",
        "        vix.name = 'VIX'\n",
        "        sp500.name = 'SP500'\n",
        "\n",
        "        # Ensure all series have the same frequency (daily) and align dates\n",
        "        all_data = pd.concat([vix, sp500] + [df.iloc[:, 0] for df in fred_data.values()], axis=1, join='outer')\n",
        "        all_data.columns = ['VIX', 'SP500'] + list(fred_series.keys())\n",
        "        all_data = all_data.resample('D').last().fillna(method='ffill')\n",
        "\n",
        "        # Calculate market returns and volatility\n",
        "        market_returns = all_data['SP500'].pct_change().fillna(0)\n",
        "        volatility = market_returns.rolling(window=21).std().fillna(0) * np.sqrt(252)  # 21-day rolling volatility, annualized\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'CPI' in all_data.columns and not all_data['CPI'].empty:\n",
        "            # Calculate year-over-year inflation rate\n",
        "            cpi_monthly = all_data['CPI'].resample('M').last().dropna()\n",
        "            inflation_rate = cpi_monthly.pct_change(12).fillna(0) * 100\n",
        "            inflation_rate = inflation_rate.resample('D').ffill()  # Resample back to daily frequency\n",
        "        else:\n",
        "            print(\"CPI data not available or empty. Using placeholder values.\")\n",
        "            inflation_rate = pd.Series(0, index=all_data.index)\n",
        "\n",
        "        if 'GDP' in all_data.columns and not all_data['GDP'].empty:\n",
        "            # Calculate year-over-year GDP growth rate\n",
        "            gdp_quarterly = all_data['GDP'].resample('Q').last().dropna()\n",
        "            gdp_growth_rate = gdp_quarterly.pct_change(4).fillna(0) * 100  # Year-over-year growth\n",
        "            gdp_growth_rate = gdp_growth_rate.resample('D').ffill()  # Resample back to daily frequency\n",
        "\n",
        "            # If we still don't have a valid growth rate, calculate the overall growth rate\n",
        "            if gdp_growth_rate.iloc[-1] == 0:\n",
        "                total_growth = (gdp_quarterly.iloc[-1] / gdp_quarterly.iloc[0]) - 1\n",
        "                years = (gdp_quarterly.index[-1] - gdp_quarterly.index[0]).days / 252\n",
        "                annualized_growth = ((1 + total_growth) ** (1 / years) - 1) * 100\n",
        "                gdp_growth_rate = pd.Series(annualized_growth, index=all_data.index)\n",
        "        else:\n",
        "            print(\"GDP data not available or empty. Using placeholder values.\")\n",
        "            gdp_growth_rate = pd.Series(0, index=all_data.index)\n",
        "\n",
        "        # Combine all data into a single DataFrame\n",
        "        data = pd.DataFrame({\n",
        "            'VIX': all_data['VIX'],\n",
        "            'Yield_Curve': all_data['Yield_Curve'],\n",
        "            'SP500': all_data['SP500'],\n",
        "            'Market_Returns': market_returns,\n",
        "            'Volatility': volatility,\n",
        "            'Unemployment': all_data['Unemployment'],\n",
        "            'Inflation': inflation_rate,\n",
        "            'GDP_Growth': gdp_growth_rate\n",
        "        })\n",
        "\n",
        "        # Handle any remaining NaN values\n",
        "        data = data.fillna(method='ffill').fillna(0)\n",
        "\n",
        "        # Calculate crash risk indicators\n",
        "        vix_z_score = (data['VIX'] - data['VIX'].mean()) / (data['VIX'].std() + 1e-8)\n",
        "        yield_curve_slope = data['Yield_Curve']\n",
        "        market_momentum = data['Market_Returns'].rolling(window=252).mean().fillna(0)\n",
        "        volatility_z_score = (data['Volatility'] - data['Volatility'].mean()) / (data['Volatility'].std() + 1e-8)\n",
        "        unemployment_z_score = (data['Unemployment'] - data['Unemployment'].mean()) / (data['Unemployment'].std() + 1e-8)\n",
        "        inflation_z_score = (data['Inflation'] - data['Inflation'].mean()) / (data['Inflation'].std() + 1e-8)\n",
        "        gdp_growth_z_score = (data['GDP_Growth'] - data['GDP_Growth'].mean()) / (data['GDP_Growth'].std() + 1e-8)\n",
        "\n",
        "        # Calculate weighted crash risk score\n",
        "        crash_risk = (\n",
        "            0.25 * vix_z_score +\n",
        "            0.20 * (1 - yield_curve_slope) +\n",
        "            0.15 * (1 - market_momentum) +\n",
        "            0.15 * volatility_z_score +\n",
        "            0.10 * unemployment_z_score +\n",
        "            0.10 * inflation_z_score +\n",
        "            0.05 * (-gdp_growth_z_score)\n",
        "        )\n",
        "\n",
        "        latest_crash_risk = crash_risk.iloc[-1]\n",
        "\n",
        "        # Calculate historical percentile of current crash risk\n",
        "        historical_percentile = pd.Series(crash_risk).rank(pct=True).iloc[-1]\n",
        "\n",
        "        # Estimate probability of a market crash\n",
        "        crash_threshold = crash_risk.mean() + 2 * crash_risk.std()\n",
        "        crash_probability = 1 - norm.cdf(crash_threshold, loc=latest_crash_risk, scale=crash_risk.std())\n",
        "\n",
        "        analysis = f\"\"\"\n",
        "        Market Crash Risk Analysis:\n",
        "        Current Crash Risk Score: {latest_crash_risk:.2f}\n",
        "        Historical Percentile: {historical_percentile:.2%}\n",
        "        Estimated Crash Probability: {crash_probability:.2%}\n",
        "\n",
        "        Interpretation:\n",
        "        \"\"\"\n",
        "\n",
        "        if latest_crash_risk > 2:\n",
        "            analysis += \"- The market is showing very high crash risk. Consider defensive strategies.\\n\"\n",
        "        elif latest_crash_risk > 1:\n",
        "            analysis += \"- Market crash risk is elevated. Monitor closely and consider reducing exposure to high-risk assets.\\n\"\n",
        "        elif latest_crash_risk > 0:\n",
        "            analysis += \"- Market crash risk is moderate. Maintain a balanced portfolio and stay vigilant.\\n\"\n",
        "        elif latest_crash_risk > -1:\n",
        "            analysis += \"- Market crash risk is low. Opportunities may exist, but continue to monitor risk factors.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Market crash risk is very low. The market appears stable, but remain cautious of unexpected events.\\n\"\n",
        "\n",
        "        analysis += f\"\\nKey Contributing Factors:\\n\"\n",
        "        analysis += f\"- VIX (Volatility Index): {data['VIX'].iloc[-1]:.2f}\\n\"\n",
        "        analysis += f\"- Yield Curve Slope: {data['Yield_Curve'].iloc[-1]:.4f}\\n\"\n",
        "        analysis += f\"- S&P 500 Level: {data['SP500'].iloc[-1]:.2f}\\n\"\n",
        "        analysis += f\"- Market Volatility: {data['Volatility'].iloc[-1]:.4f}\\n\"\n",
        "        analysis += f\"- Unemployment Rate: {data['Unemployment'].iloc[-1]:.2f}%\\n\"\n",
        "        analysis += f\"- Inflation Rate: {data['Inflation'].iloc[-1]:.2f}%\\n\"\n",
        "        analysis += f\"- GDP Growth Rate: {data['GDP_Growth'].iloc[-1]:.2f}%\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in market crash risk analysis: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return \"Error: Unable to analyze market crash risk.\"\n",
        "\n",
        "\n",
        "# Load spaCy model for NER and more advanced NLP tasks\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_and_categorize_news(api_key, num_pages=500):\n",
        "    all_news = fetch_news(api_key, num_pages)\n",
        "\n",
        "    # Preprocess and filter news\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "    tfidf_matrix = vectorizer.fit_transform([article['text'] for article in all_news])\n",
        "\n",
        "    filtered_news = []\n",
        "    for i, article in enumerate(all_news):\n",
        "        relevance_score = calculate_relevance_score(article, tfidf_matrix[i], vectorizer)\n",
        "        if relevance_score > 0.5:  # Adjust threshold as needed\n",
        "            processed_article = process_article(article, relevance_score)\n",
        "            if not is_duplicate(processed_article, filtered_news):\n",
        "                filtered_news.append(processed_article)\n",
        "\n",
        "    # Sort by recency and relevance\n",
        "    filtered_news.sort(key=lambda x: (x['date'], x['relevance_score']), reverse=True)\n",
        "\n",
        "    # Limit to top N most relevant articles\n",
        "    top_articles = filtered_news[:100]  # Adjust N as needed\n",
        "\n",
        "    # Categorize articles\n",
        "    categorized_articles = categorize_articles(top_articles)\n",
        "\n",
        "    return categorized_articles\n",
        "\n",
        "def fetch_news(api_key, num_pages):\n",
        "    all_news = []\n",
        "    for page in tqdm(range(num_pages), desc=\"Fetching news\"):\n",
        "        url = f\"https://financialmodelingprep.com/api/v4/general_news?page={page}&apikey={api_key}\"\n",
        "        response = requests.get(url)\n",
        "        news_data = response.json()\n",
        "        all_news.extend(news_data)\n",
        "    return all_news\n",
        "\n",
        "def calculate_relevance_score(article, tfidf_vector, vectorizer):\n",
        "    # Implement more sophisticated relevance scoring based on TF-IDF\n",
        "    market_terms = ['stock', 'market', 'economy', 'finance', 'trade', 'investor', 'nasdaq', 'dow', 'sp500', 'earnings']\n",
        "    term_scores = []\n",
        "    for term in market_terms:\n",
        "        if term in vectorizer.vocabulary_:\n",
        "            term_score = tfidf_vector[0, vectorizer.vocabulary_[term]]\n",
        "            term_scores.append(term_score)\n",
        "        elif f\"{term} market\" in vectorizer.vocabulary_:\n",
        "            term_score = tfidf_vector[0, vectorizer.vocabulary_[f\"{term} market\"]]\n",
        "            term_scores.append(term_score)\n",
        "\n",
        "    # Calculate weighted average, giving more importance to exact matches\n",
        "    exact_match_weight = 1.5\n",
        "    weighted_scores = [score * exact_match_weight if term in article['title'].lower() else score for score, term in zip(term_scores, market_terms)]\n",
        "\n",
        "    return np.mean(weighted_scores) if weighted_scores else 0\n",
        "\n",
        "def process_article(article, relevance_score):\n",
        "    doc = nlp(article['text'])\n",
        "\n",
        "    return {\n",
        "        'date': datetime.strptime(article['publishedDate'], '%Y-%m-%dT%H:%M:%S.%fZ'),\n",
        "        'title': article['title'],\n",
        "        'summary': summarize_text(doc),\n",
        "        'sentiment': analyze_sentiment(doc),\n",
        "        'relevance_score': relevance_score,\n",
        "        'key_metrics': extract_key_metrics(doc),\n",
        "        'entities': extract_entities(doc)\n",
        "    }\n",
        "\n",
        "def summarize_text(doc, max_sentences=3):\n",
        "    # Implement extractive summarization\n",
        "    sentence_scores = {}\n",
        "    for sent in doc.sents:\n",
        "        sentence_scores[sent] = sum([token.rank for token in sent if not token.is_stop])\n",
        "\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:max_sentences]\n",
        "    summary = ' '.join([sent.text for sent in summary_sentences])\n",
        "    return summary\n",
        "\n",
        "def analyze_sentiment(doc):\n",
        "    # Implement more nuanced sentiment analysis\n",
        "    sentiment_scores = [token.sentiment for token in doc if token.has_sentiment]\n",
        "    overall_sentiment = np.mean(sentiment_scores) if sentiment_scores else 0\n",
        "\n",
        "    # Map sentiment score to labels\n",
        "    if overall_sentiment > 0.2:\n",
        "        return \"Positive\"\n",
        "    elif overall_sentiment < -0.2:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def extract_key_metrics(doc):\n",
        "    metrics = {}\n",
        "\n",
        "    # Extract numerical data using regex and NER\n",
        "    price_pattern = r'\\$(\\d+(\\.\\d{2})?)'\n",
        "    percentage_pattern = r'(\\d+(\\.\\d{1,2})?)%'\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"MONEY\":\n",
        "            price_match = re.search(price_pattern, ent.text)\n",
        "            if price_match:\n",
        "                metrics['price'] = float(price_match.group(1))\n",
        "        elif ent.label_ == \"PERCENT\":\n",
        "            percent_match = re.search(percentage_pattern, ent.text)\n",
        "            if percent_match:\n",
        "                metrics['percentage'] = float(percent_match.group(1))\n",
        "        elif ent.label_ in [\"DATE\", \"TIME\"]:\n",
        "            metrics['date_mentioned'] = ent.text\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def extract_entities(doc):\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"PRODUCT\"]:\n",
        "            entities[ent.label_] = entities.get(ent.label_, []) + [ent.text]\n",
        "    return entities\n",
        "\n",
        "def is_duplicate(article, existing_articles, similarity_threshold=0.8):\n",
        "    for existing in existing_articles:\n",
        "        title_similarity = cosine_similarity(\n",
        "            [article['title']],\n",
        "            [existing['title']]\n",
        "        )[0][0]\n",
        "        summary_similarity = cosine_similarity(\n",
        "            [article['summary']],\n",
        "            [existing['summary']]\n",
        "        )[0][0]\n",
        "        if title_similarity > similarity_threshold or summary_similarity > similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def categorize_articles(articles):\n",
        "    categories = {\n",
        "        \"Economic Indicators\": [\"gdp\", \"inflation\", \"unemployment\", \"interest rate\"],\n",
        "        \"Company Earnings\": [\"earnings\", \"revenue\", \"profit\", \"loss\"],\n",
        "        \"Geopolitical Events\": [\"war\", \"conflict\", \"treaty\", \"election\"],\n",
        "        \"Market Trends\": [\"bull market\", \"bear market\", \"correction\", \"rally\"],\n",
        "        \"Technology\": [\"ai\", \"blockchain\", \"crypto\", \"cybersecurity\"],\n",
        "        \"Energy\": [\"oil\", \"gas\", \"renewable\", \"energy crisis\"]\n",
        "    }\n",
        "\n",
        "    categorized = {cat: [] for cat in categories}\n",
        "\n",
        "    for article in articles:\n",
        "        article_text = f\"{article['title']} {article['summary']}\".lower()\n",
        "        for category, keywords in categories.items():\n",
        "            if any(keyword in article_text for keyword in keywords):\n",
        "                categorized[category].append(article)\n",
        "\n",
        "    return categorized\n",
        "\n",
        "def format_news_for_claude(categorized_articles):\n",
        "    news_text = \"\"\n",
        "    for category, articles in categorized_articles.items():\n",
        "        news_text += f\"\\n\\n--- {category} ---\\n\"\n",
        "        for article in articles[:50]:  # Limit to top 5 articles per category\n",
        "            news_text += f\"\\nDate: {article['date']}\\nTitle: {article['title']}\\nSummary: {article['summary']}\\nSentiment: {article['sentiment']}\\nKey Metrics: {article['key_metrics']}\\nEntities: {article['entities']}\\n\"\n",
        "    return news_text\n",
        "\n",
        "def analyze_market_news(api_key, num_pages=500):\n",
        "    categorized_articles = preprocess_and_categorize_news(api_key, num_pages)\n",
        "\n",
        "    # Prepare news text for Claude\n",
        "    news_text = format_news_for_claude(categorized_articles)\n",
        "\n",
        "    # Get analysis from Claude\n",
        "    system_prompt = \"\"\"As an expert financial analyst, provide a comprehensive 5-year market outlook based on the given recent news articles. Your analysis should extrapolate from current trends to potential future scenarios. Include:\n",
        "\n",
        "      1. Overall market sentiment projection for the next 5 years (bullish, bearish, or fluctuating)\n",
        "      2. Key economic indicators mentioned and their potential long-term impact\n",
        "      3. Identification of current market movers that could have lasting effects\n",
        "      4. Potential risks or opportunities for investors over the 5-year period\n",
        "      5. Sectors or industries that may experience significant changes or growth\n",
        "      6. Emerging trends or patterns that could become major factors in the next 5 years\n",
        "      7. Long-term recommendations for investors based on projected market conditions\n",
        "      8. Analysis of how current geopolitical events might evolve and impact markets\n",
        "      9. Projection of global market trends and their influence on domestic markets\n",
        "      10. Identification of emerging technologies or innovations that could disrupt markets in the coming years\n",
        "      11. Consideration of potential economic cycles or historical patterns that might repeat\n",
        "      12. Possible regulatory or policy changes that could affect markets long-term\n",
        "\n",
        "Your analysis should be forward-looking, using the recent news as a starting point to project future scenarios. Provide a balanced view of both risks and opportunities in the 5-year market landscape, and explain how current events might evolve or influence long-term trends. When possible, reference historical precedents or cyclical patterns to support your projections.\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Analyze the following market news and provide comprehensive insights:\\n{news_text}\"},\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "        \"anthropic-version\": \"2023-06-01\",\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": 'claude-3-haiku-20240307',\n",
        "        \"max_tokens\": 4000,\n",
        "        \"temperature\": 0.2,\n",
        "        \"system\": system_prompt,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "    response_json = response.json()\n",
        "\n",
        "    if 'content' in response_json:\n",
        "        analysis = response_json['content'][0]['text']\n",
        "    else:\n",
        "        analysis = \"Error: Unable to parse API response.\"\n",
        "\n",
        "    return categorized_articles, analysis\n",
        "\n",
        "\n",
        "def get_stock_data(ticker, years):\n",
        "    end_date = datetime.now().date()\n",
        "    start_date = end_date - timedelta(days=years*252)\n",
        "\n",
        "    stock = yf.Ticker(ticker)\n",
        "\n",
        "    try:\n",
        "        hist_data = stock.history(start=start_date, end=end_date)\n",
        "        if hist_data.empty:\n",
        "            print(f\"No historical data available for {ticker}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving historical data for {ticker}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Add checks for missing data\n",
        "    if 'Close' not in hist_data.columns:\n",
        "        print(f\"Missing 'Close' price data for {ticker}\")\n",
        "        return None\n",
        "\n",
        "    # Retrieve insider trades\n",
        "    try:\n",
        "        insider_trades = stock.insider_transactions\n",
        "        insider_trades = insider_trades[insider_trades['Date'] >= (datetime.now() - timedelta(days=730))]  # Last 2 years\n",
        "    except Exception:\n",
        "        insider_trades = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        insider_roster = stock.insider_roster\n",
        "    except Exception:\n",
        "        insider_roster = pd.DataFrame()\n",
        "    # Retrieve top mutual fund holders\n",
        "    try:\n",
        "        mutual_fund_holders = stock.mutualfund_holders\n",
        "    except Exception:\n",
        "        mutual_fund_holders = pd.DataFrame()\n",
        "\n",
        "    # Retrieve balance sheet\n",
        "    try:\n",
        "        balance_sheet = stock.balance_sheet\n",
        "    except Exception:\n",
        "        balance_sheet = pd.DataFrame()\n",
        "\n",
        "    # Retrieve financial statements\n",
        "    try:\n",
        "        financials = stock.financials\n",
        "    except Exception:\n",
        "        financials = pd.DataFrame()\n",
        "\n",
        "    # Retrieve news articles\n",
        "    try:\n",
        "        news = stock.news\n",
        "    except Exception:\n",
        "        news = []\n",
        "\n",
        "    # New data points\n",
        "    key_stats = stock.info\n",
        "    earnings_forecasts = key_stats.get('earningsEstimate', {})\n",
        "    revenue_forecasts = key_stats.get('revenueEstimate', {})\n",
        "\n",
        "    try:\n",
        "        income_stmt = stock.income_stmt\n",
        "    except Exception:\n",
        "        income_stmt = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        cash_flow = stock.cash_flow\n",
        "    except Exception:\n",
        "        cash_flow = pd.DataFrame()\n",
        "\n",
        "    # Handle institutional_holders and major_holders gracefully\n",
        "    try:\n",
        "        institutional_holders = stock.institutional_holders\n",
        "    except Exception:\n",
        "        institutional_holders = None\n",
        "\n",
        "    try:\n",
        "        major_holders = stock.major_holders\n",
        "    except Exception:\n",
        "        major_holders = None\n",
        "\n",
        "    # Update the options data retrieval\n",
        "    try:\n",
        "        options = stock.options\n",
        "        if options:\n",
        "            options_data = {}\n",
        "            for date in options:  # Get data for all available expiration dates\n",
        "                option_chain = stock.option_chain(date)\n",
        "                options_data[date] = {\n",
        "                    'calls': option_chain.calls,\n",
        "                    'puts': option_chain.puts\n",
        "                }\n",
        "        else:\n",
        "            options_data = None\n",
        "    except Exception:\n",
        "        options_data = None\n",
        "\n",
        "    try:\n",
        "        recommendations = stock.recommendations\n",
        "    except Exception:\n",
        "        recommendations = None\n",
        "\n",
        "    try:\n",
        "        actions = stock.actions\n",
        "    except Exception:\n",
        "        actions = None\n",
        "\n",
        "    try:\n",
        "        calendar = stock.calendar\n",
        "    except Exception:\n",
        "        calendar = None\n",
        "\n",
        "    try:\n",
        "        quarterly_financials = stock.quarterly_financials\n",
        "    except Exception:\n",
        "        quarterly_financials = None\n",
        "\n",
        "    try:\n",
        "        quarterly_balance_sheet = stock.quarterly_balance_sheet\n",
        "    except Exception:\n",
        "        quarterly_balance_sheet = None\n",
        "\n",
        "    try:\n",
        "        quarterly_cashflow = stock.quarterly_cashflow\n",
        "    except Exception:\n",
        "        quarterly_cashflow = None\n",
        "\n",
        "    # Calculate risk metrics\n",
        "    risk_metrics = calculate_risk_metrics(hist_data, ticker)\n",
        "\n",
        "    # Perform technical analysis\n",
        "    hist_data = perform_technical_analysis(hist_data)\n",
        "\n",
        "    # Calculate target price\n",
        "    target_prices = estimate_target_price(ticker, {\n",
        "        'hist_data': hist_data,\n",
        "        'financials': financials\n",
        "    })\n",
        "\n",
        "    # Perform Monte Carlo analysis\n",
        "    monte_carlo_analysis, probability_stats, weighted_avg_probability = perform_monte_carlo_price_analysis(ticker, stock_data, target_prices)\n",
        "\n",
        "    # Calculate current price\n",
        "    current_price = hist_data['Close'].iloc[-1]\n",
        "\n",
        "    # Get beta\n",
        "    beta = get_beta(ticker, start_date=start_date, end_date=end_date)\n",
        "\n",
        "    # Perform topological data analysis\n",
        "    tda_results = perform_tda(hist_data)\n",
        "\n",
        "    # Perform advanced analysis (combining nonlinear, dynamical systems, and chaos analysis)\n",
        "    advanced_analysis_results = perform_advanced_analysis(hist_data)\n",
        "\n",
        "    # Add this line to define forecast_steps\n",
        "    forecast_steps = 252  # You can adjust this value as needed\n",
        "\n",
        "    # Create a date index for the forecast\n",
        "    last_date = hist_data.index[-1]\n",
        "    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps)\n",
        "\n",
        "    # Calculate index correlations\n",
        "    index_correlations = calculate_index_correlations(hist_data, index_data)\n",
        "\n",
        "    # Calculate string theory metrics\n",
        "    string_theory_metrics = calculate_string_theory_metrics(hist_data)\n",
        "\n",
        "    # Perform time series decomposition\n",
        "    decomposition_results = perform_time_series_decomposition(hist_data)\n",
        "    decomposition_summary = summarize_decomposition(decomposition_results)\n",
        "\n",
        "    # Return dictionary\n",
        "    return_dict = {\n",
        "        'hist_data': hist_data,\n",
        "        'balance_sheet': balance_sheet,\n",
        "        'financials': financials,\n",
        "        'news': news,\n",
        "        'earnings_forecasts': earnings_forecasts,\n",
        "        'revenue_forecasts': revenue_forecasts,\n",
        "        'income_stmt': income_stmt,\n",
        "        'cash_flow': cash_flow,\n",
        "        'key_stats': key_stats,\n",
        "        'institutional_holders': institutional_holders,\n",
        "        'major_holders': major_holders,\n",
        "        'options_data': options_data,\n",
        "        'recommendations': recommendations,\n",
        "        'actions': actions,\n",
        "        'calendar': calendar,\n",
        "        'quarterly_financials': quarterly_financials,\n",
        "        'quarterly_balance_sheet': quarterly_balance_sheet,\n",
        "        'quarterly_cashflow': quarterly_cashflow,\n",
        "        'risk_metrics': risk_metrics,\n",
        "        'target_price': target_prices,\n",
        "        'current_price': current_price,\n",
        "        'beta': beta,\n",
        "        'tda_results': tda_results,\n",
        "        'advanced_analysis_results': summarize_advanced_analysis(advanced_analysis_results),\n",
        "        'index_correlations': index_correlations,\n",
        "        'string_theory_metrics': string_theory_metrics,\n",
        "        'insider_trades': insider_trades,\n",
        "        'insider_roster': insider_roster,\n",
        "        'mutual_fund_holders': mutual_fund_holders,\n",
        "        'decomposition_summary': decomposition_summary,\n",
        "        'monte_carlo_analysis': monte_carlo_analysis,\n",
        "        'probability_stats': probability_stats,\n",
        "        'weighted_avg_probability': weighted_avg_probability,\n",
        "    }\n",
        "\n",
        "    return return_dict\n",
        "\n",
        "def analyze_sector_peers(ticker, api_key):\n",
        "    # Get peer companies\n",
        "    peer_url = f\"https://financialmodelingprep.com/api/v4/stock_peers?symbol={ticker}&apikey={api_key}\"\n",
        "    peer_response = requests.get(peer_url)\n",
        "    peer_data = peer_response.json()\n",
        "\n",
        "    if not peer_data:\n",
        "        return f\"No peer data available for {ticker}\"\n",
        "\n",
        "    peers = peer_data[0]['peersList']\n",
        "    peers.append(ticker)  # Include the main company in the analysis\n",
        "\n",
        "    # Get key metrics for all peers\n",
        "    metrics = {}\n",
        "    for company in peers:\n",
        "        metrics_url = f\"https://financialmodelingprep.com/api/v3/key-metrics-ttm/{company}?apikey={api_key}\"\n",
        "        metrics_response = requests.get(metrics_url)\n",
        "        company_metrics = metrics_response.json()\n",
        "        if company_metrics:\n",
        "            metrics[company] = company_metrics[0]\n",
        "\n",
        "    if not metrics:\n",
        "        return \"No metrics data available for peers\"\n",
        "\n",
        "    # Create a DataFrame with the metrics\n",
        "    df = pd.DataFrame(metrics).T\n",
        "\n",
        "    # Select important metrics for comparison\n",
        "    important_metrics = [\n",
        "        'peRatioTTM',\n",
        "        'pbRatioTTM',\n",
        "        'priceToSalesRatioTTM',\n",
        "        'evToSalesTTM',\n",
        "        'enterpriseValueOverEBITDATTM',\n",
        "        'roeTTM',\n",
        "        'returnOnTangibleAssetsTTM',\n",
        "        'netIncomePerShareTTM',\n",
        "        'operatingCashFlowPerShareTTM',\n",
        "        'freeCashFlowPerShareTTM',\n",
        "        'debtToEquityTTM',\n",
        "        'currentRatioTTM',\n",
        "        'interestCoverageTTM',\n",
        "        'dividendYieldPercentageTTM',\n",
        "        'payoutRatioTTM',\n",
        "        'capexToRevenueTTM',\n",
        "        'roicTTM'\n",
        "    ]\n",
        "\n",
        "    comparison_df = df[important_metrics]\n",
        "\n",
        "    # Calculate percentile ranks for the main company\n",
        "    percentile_ranks = {}\n",
        "    for metric in important_metrics:\n",
        "        percentile_ranks[metric] = stats.percentileofscore(comparison_df[metric], comparison_df.loc[ticker, metric])\n",
        "\n",
        "    # Generate analysis\n",
        "    analysis = f\"Sector Peer Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "    for metric in important_metrics:\n",
        "        company_value = comparison_df.loc[ticker, metric]\n",
        "        peer_median = comparison_df[metric].median()\n",
        "        percentile = percentile_ranks[metric]\n",
        "\n",
        "        analysis += f\"{metric}:\\n\"\n",
        "        analysis += f\"  {ticker} value: {company_value:.2f}\\n\"\n",
        "        analysis += f\"  Peer median: {peer_median:.2f}\\n\"\n",
        "        analysis += f\"  Percentile rank: {percentile:.1f}\\n\"\n",
        "\n",
        "        if percentile > 75:\n",
        "            analysis += f\"  {ticker} is in the top quartile for this metric.\\n\"\n",
        "        elif percentile < 25:\n",
        "            analysis += f\"  {ticker} is in the bottom quartile for this metric.\\n\"\n",
        "        else:\n",
        "            analysis += f\"  {ticker} is near the median for this metric.\\n\"\n",
        "\n",
        "        analysis += \"\\n\"\n",
        "\n",
        "    # Overall comparison\n",
        "    strengths = [metric for metric in important_metrics if percentile_ranks[metric] > 75]\n",
        "    weaknesses = [metric for metric in important_metrics if percentile_ranks[metric] < 25]\n",
        "\n",
        "    analysis += \"Overall Comparison:\\n\"\n",
        "    if strengths:\n",
        "        analysis += f\"Strengths: {ticker} outperforms peers in {', '.join(strengths)}.\\n\"\n",
        "    if weaknesses:\n",
        "        analysis += f\"Weaknesses: {ticker} underperforms peers in {', '.join(weaknesses)}.\\n\"\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def analyze_historical_employee_data(ticker, api_key):\n",
        "    historical_url = f\"https://financialmodelingprep.com/api/v4/historical/employee_count?symbol={ticker}&apikey={api_key}\"\n",
        "    current_url = f\"https://financialmodelingprep.com/api/v4/employee_count?symbol={ticker}&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        # Fetch historical data\n",
        "        historical_response = requests.get(historical_url)\n",
        "        historical_response.raise_for_status()\n",
        "        historical_data = historical_response.json()\n",
        "\n",
        "        # Fetch current data\n",
        "        current_response = requests.get(current_url)\n",
        "        current_response.raise_for_status()\n",
        "        current_data = current_response.json()\n",
        "\n",
        "        if not historical_data and not current_data:\n",
        "            return f\"No employee data available for {ticker}\"\n",
        "\n",
        "        analysis = f\"Employee Data Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        # Analyze current employee count\n",
        "        if current_data:\n",
        "            most_recent = current_data[0]\n",
        "            analysis += f\"Most Recent Employee Count: {most_recent['employeeCount']:,} (as of {most_recent['periodOfReport']})\\n\\n\"\n",
        "\n",
        "        # Analyze historical data\n",
        "        if historical_data:\n",
        "            df = pd.DataFrame(historical_data)\n",
        "            df['periodOfReport'] = pd.to_datetime(df['periodOfReport'])\n",
        "            df = df.sort_values('periodOfReport', ascending=False)\n",
        "\n",
        "            # Filter for the last 5 years\n",
        "            five_years_ago = pd.Timestamp.now() - pd.DateOffset(years=5)\n",
        "            df_5y = df[df['periodOfReport'] > five_years_ago].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "            analysis += \"Employee Count for the Past 5 Years:\\n\"\n",
        "            for _, row in df_5y.iterrows():\n",
        "                analysis += f\"  {row['periodOfReport'].strftime('%Y-%m-%d')}: {row['employeeCount']:,}\\n\"\n",
        "\n",
        "            analysis += \"\\n\"\n",
        "\n",
        "            # Calculate year-over-year growth rates\n",
        "            df_5y['YoY_Growth'] = df_5y['employeeCount'].pct_change(-1)  # Negative 1 because the dataframe is in descending order\n",
        "\n",
        "            # Calculate 5-year CAGR\n",
        "            if len(df_5y) >= 2:\n",
        "                start_count = df_5y['employeeCount'].iloc[-1]\n",
        "                end_count = df_5y['employeeCount'].iloc[0]\n",
        "                years = (df_5y['periodOfReport'].iloc[0] - df_5y['periodOfReport'].iloc[-1]).days / 365.25\n",
        "                if start_count > 0 and years > 0:\n",
        "                    cagr = (end_count / start_count) ** (1/years) - 1\n",
        "                    analysis += f\"5-Year CAGR: {cagr:.2%}\\n\"\n",
        "                else:\n",
        "                    analysis += \"Unable to calculate 5-Year CAGR due to insufficient or invalid data.\\n\"\n",
        "\n",
        "            # Analyze recent trends\n",
        "            recent_growth = df_5y['YoY_Growth'].mean()\n",
        "            analysis += f\"Average annual growth rate (last 5 years): {recent_growth:.2%}\\n\"\n",
        "\n",
        "            if recent_growth > 0.1:\n",
        "                analysis += \"The company has been rapidly expanding its workforce.\\n\"\n",
        "            elif recent_growth < -0.1:\n",
        "                analysis += \"The company has been significantly reducing its workforce.\\n\"\n",
        "            else:\n",
        "                analysis += \"The company's workforce has been relatively stable.\\n\"\n",
        "\n",
        "            # Identify any significant changes\n",
        "            significant_changes = df_5y[df_5y['YoY_Growth'].abs() > 0.2]\n",
        "            if not significant_changes.empty:\n",
        "                analysis += \"\\nSignificant changes in employee count:\\n\"\n",
        "                for _, row in significant_changes.iterrows():\n",
        "                    analysis += f\"  {row['periodOfReport'].strftime('%Y-%m-%d')}: {row['YoY_Growth']:.2%} change\\n\"\n",
        "\n",
        "            # Compare most recent historical data with current data\n",
        "            if current_data and len(df_5y) > 0:\n",
        "                last_historical = df_5y.iloc[0]\n",
        "                current = current_data[0]\n",
        "                if last_historical['periodOfReport'] != current['periodOfReport']:\n",
        "                    time_diff = (pd.to_datetime(current['periodOfReport']) - last_historical['periodOfReport']).days / 365.25\n",
        "                    count_change = current['employeeCount'] - last_historical['employeeCount']\n",
        "                    if last_historical['employeeCount'] != 0:\n",
        "                        percent_change = count_change / last_historical['employeeCount']\n",
        "                        if count_change != 0:  # Only include this section if there's a change\n",
        "                            analysis += f\"\\nChange since last reported data:\\n\"\n",
        "                            analysis += f\"  Time period: {time_diff:.2f} years\\n\"\n",
        "                            analysis += f\"  Employee count change: {count_change:+,}\\n\"\n",
        "                            analysis += f\"  Percent change: {percent_change:.2%}\\n\"\n",
        "                    else:\n",
        "                        analysis += \"\\nUnable to calculate percent change due to zero employee count in historical data.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching employee data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_historical_share_float(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v4/historical/shares_float?symbol={ticker}&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No historical share float data available for {ticker}\"\n",
        "\n",
        "        # Convert dates to datetime objects and sort data\n",
        "        for item in data:\n",
        "            item['date'] = datetime.strptime(item['date'], '%Y-%m-%d')\n",
        "        data.sort(key=lambda x: x['date'])\n",
        "\n",
        "        # Calculate 5-year trend\n",
        "        five_years_ago = datetime.now() - timedelta(days=5*365)\n",
        "        recent_data = [item for item in data if item['date'] >= five_years_ago]\n",
        "\n",
        "        if len(recent_data) < 2:\n",
        "            return f\"Insufficient historical share float data for {ticker} to perform trend analysis\"\n",
        "\n",
        "        first_float = float(recent_data[0]['floatShares'])\n",
        "        last_float = float(recent_data[-1]['floatShares'])\n",
        "\n",
        "        float_change = (last_float - first_float) / first_float * 100\n",
        "\n",
        "        analysis = f\"Historical Share Float Analysis for {ticker}:\\n\\n\"\n",
        "        analysis += f\"Current float: {last_float:,.0f} shares\\n\"\n",
        "        analysis += f\"5-year change: {float_change:.2f}%\\n\\n\"\n",
        "\n",
        "        # Calculate yearly changes\n",
        "        yearly_changes = {}\n",
        "        for item in recent_data:\n",
        "            year = item['date'].year\n",
        "            if year not in yearly_changes:\n",
        "                yearly_changes[year] = []\n",
        "            yearly_changes[year].append(float(item['floatShares']))\n",
        "\n",
        "        analysis += \"Yearly Changes:\\n\"\n",
        "        for year, floats in yearly_changes.items():\n",
        "            year_change = (floats[-1] - floats[0]) / floats[0] * 100\n",
        "            analysis += f\"{year}: {year_change:.2f}%\\n\"\n",
        "\n",
        "        analysis += \"\\nOverall Trend: \"\n",
        "        if float_change > 10:\n",
        "            analysis += \"Significant increase in float over the past 5 years.\\n\"\n",
        "        elif float_change < -10:\n",
        "            analysis += \"Significant decrease in float over the past 5 years.\\n\"\n",
        "        else:\n",
        "            analysis += \"Relatively stable float over the past 5 years.\\n\"\n",
        "\n",
        "        analysis += \"\\nInsights:\\n\"\n",
        "        if float_change > 0:\n",
        "            analysis += \"- Increasing float may indicate share dilution or insider selling.\\n\"\n",
        "            analysis += \"- This could potentially put downward pressure on the stock price.\\n\"\n",
        "        elif float_change < 0:\n",
        "            analysis += \"- Decreasing float may indicate share buybacks or insider buying.\\n\"\n",
        "            analysis += \"- This could potentially support the stock price.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching historical share float data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_company_notes(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v4/company-notes?symbol={ticker}&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No company notes data available for {ticker}\"\n",
        "\n",
        "        analysis = f\"Company Notes Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        # Aggregate data\n",
        "        notes_count = len(data)\n",
        "        exchanges = set()\n",
        "        titles = []\n",
        "\n",
        "        for note in data:\n",
        "            exchanges.add(note['exchange'])\n",
        "            titles.append(note['title'])\n",
        "\n",
        "        # Basic summary\n",
        "        analysis += f\"Total number of notes: {notes_count}\\n\"\n",
        "        analysis += f\"CIK: {data[0]['cik']}\\n\"\n",
        "        analysis += f\"Exchanges: {', '.join(exchanges)}\\n\\n\"\n",
        "\n",
        "        # Analyze titles\n",
        "        analysis += \"Notes Summary:\\n\"\n",
        "        for title in titles:\n",
        "            analysis += f\"- {title}\\n\"\n",
        "\n",
        "        # Analyze interest rates if available\n",
        "        interest_rates = []\n",
        "        for title in titles:\n",
        "            try:\n",
        "                rate = float(title.split('%')[0])\n",
        "                interest_rates.append(rate)\n",
        "            except (ValueError, IndexError):\n",
        "                pass  # Skip titles that don't contain a percentage\n",
        "\n",
        "        if interest_rates:\n",
        "            avg_rate = sum(interest_rates) / len(interest_rates)\n",
        "            min_rate = min(interest_rates)\n",
        "            max_rate = max(interest_rates)\n",
        "            analysis += f\"\\nInterest Rate Analysis:\\n\"\n",
        "            analysis += f\"- Average Rate: {avg_rate:.2f}%\\n\"\n",
        "            analysis += f\"- Minimum Rate: {min_rate:.2f}%\\n\"\n",
        "            analysis += f\"- Maximum Rate: {max_rate:.2f}%\\n\"\n",
        "\n",
        "        # Analyze maturity dates if available\n",
        "        maturity_years = []\n",
        "        for title in titles:\n",
        "            if 'due ' in title.lower():\n",
        "                try:\n",
        "                    year = int(title.lower().split('due ')[-1])\n",
        "                    maturity_years.append(year)\n",
        "                except ValueError:\n",
        "                    pass  # Skip if we can't parse the year\n",
        "\n",
        "        if maturity_years:\n",
        "            current_year = datetime.now().year\n",
        "            years_to_maturity = [year - current_year for year in maturity_years if year > current_year]\n",
        "            if years_to_maturity:\n",
        "                avg_maturity = sum(years_to_maturity) / len(years_to_maturity)\n",
        "                min_maturity = min(years_to_maturity)\n",
        "                max_maturity = max(years_to_maturity)\n",
        "                analysis += f\"\\nMaturity Analysis:\\n\"\n",
        "                analysis += f\"- Average Years to Maturity: {avg_maturity:.1f} years\\n\"\n",
        "                analysis += f\"- Shortest Time to Maturity: {min_maturity} years\\n\"\n",
        "                analysis += f\"- Longest Time to Maturity: {max_maturity} years\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching company notes data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_revenue_by_segment_and_location(ticker, api_key):\n",
        "    # Fetch revenue by product segmentation data\n",
        "    product_url = f\"https://financialmodelingprep.com/api/v4/revenue-product-segmentation?symbol={ticker}&structure=flat&period=annual&apikey={api_key}\"\n",
        "\n",
        "    # Fetch revenue by geographic segmentation data\n",
        "    geo_url = f\"https://financialmodelingprep.com/api/v4/revenue-geographic-segmentation?symbol={ticker}&structure=flat&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        product_response = requests.get(product_url)\n",
        "        product_response.raise_for_status()\n",
        "        product_data = product_response.json()\n",
        "\n",
        "        geo_response = requests.get(geo_url)\n",
        "        geo_response.raise_for_status()\n",
        "        geo_data = geo_response.json()\n",
        "\n",
        "        analysis = f\"Revenue Analysis by Segment and Location for {ticker}:\\n\\n\"\n",
        "\n",
        "        # Analyze product segmentation\n",
        "        if product_data:\n",
        "            if isinstance(product_data, list):\n",
        "                product_data = product_data[0] if product_data else {}\n",
        "\n",
        "            latest_year = max(product_data.keys()) if product_data else None\n",
        "            if latest_year:\n",
        "                analysis += f\"Product Segmentation (Year: {latest_year}):\\n\"\n",
        "                total_revenue = sum(product_data[latest_year].values())\n",
        "\n",
        "                for segment, revenue in product_data[latest_year].items():\n",
        "                    percentage = (revenue / total_revenue) * 100\n",
        "                    analysis += f\"  - {segment}: ${revenue:,.0f} ({percentage:.2f}%)\\n\"\n",
        "\n",
        "                # Calculate year-over-year growth if data for previous year is available\n",
        "                if len(product_data) > 1:\n",
        "                    prev_year = sorted(product_data.keys())[-2]\n",
        "                    analysis += f\"\\nYear-over-Year Growth (from {prev_year} to {latest_year}):\\n\"\n",
        "                    for segment in product_data[latest_year].keys():\n",
        "                        if segment in product_data[prev_year]:\n",
        "                            growth = ((product_data[latest_year][segment] / product_data[prev_year][segment]) - 1) * 100\n",
        "                            analysis += f\"  - {segment}: {growth:.2f}%\\n\"\n",
        "            else:\n",
        "                analysis += \"No product segmentation data available.\\n\"\n",
        "        else:\n",
        "            analysis += \"No product segmentation data available.\\n\"\n",
        "\n",
        "        # Analyze geographic segmentation\n",
        "        if geo_data:\n",
        "            if isinstance(geo_data, list):\n",
        "                geo_data = geo_data[0] if geo_data else {}\n",
        "\n",
        "            latest_year = max(geo_data.keys()) if geo_data else None\n",
        "            if latest_year:\n",
        "                analysis += f\"\\nGeographic Segmentation (Year: {latest_year}):\\n\"\n",
        "                total_revenue = sum(geo_data[latest_year].values())\n",
        "\n",
        "                for location, revenue in geo_data[latest_year].items():\n",
        "                    percentage = (revenue / total_revenue) * 100\n",
        "                    analysis += f\"  - {location}: ${revenue:,.0f} ({percentage:.2f}%)\\n\"\n",
        "\n",
        "                # Calculate year-over-year growth if data for previous year is available\n",
        "                if len(geo_data) > 1:\n",
        "                    prev_year = sorted(geo_data.keys())[-2]\n",
        "                    analysis += f\"\\nYear-over-Year Growth (from {prev_year} to {latest_year}):\\n\"\n",
        "                    for location in geo_data[latest_year].keys():\n",
        "                        if location in geo_data[prev_year]:\n",
        "                            growth = ((geo_data[latest_year][location] / geo_data[prev_year][location]) - 1) * 100\n",
        "                            analysis += f\"  - {location}: {growth:.2f}%\\n\"\n",
        "            else:\n",
        "                analysis += \"\\nNo geographic segmentation data available.\\n\"\n",
        "        else:\n",
        "            analysis += \"\\nNo geographic segmentation data available.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching revenue segmentation data for {ticker}: {str(e)}\"\n",
        "\n",
        "def calculate_financial_ratios(stock_data):\n",
        "    key_stats = stock_data['key_stats']\n",
        "    financials = stock_data['financials']\n",
        "    income_stmt = stock_data['income_stmt']\n",
        "    cash_flow = stock_data['cash_flow']\n",
        "    balance_sheet = stock_data['balance_sheet']\n",
        "\n",
        "    ratios = {\n",
        "        'P/E Ratio': key_stats.get('trailingPE', 'N/A'),\n",
        "        'P/B Ratio': key_stats.get('priceToBook', 'N/A'),\n",
        "        'ROE': key_stats.get('returnOnEquity', 'N/A'),\n",
        "        'Profit Margin': key_stats.get('profitMargins', 'N/A'),\n",
        "        'Debt to Equity': key_stats.get('debtToEquity', 'N/A'),\n",
        "        'Current Ratio': key_stats.get('currentRatio', 'N/A'),\n",
        "        'Quick Ratio': key_stats.get('quickRatio', 'N/A'),\n",
        "        'Dividend Yield': key_stats.get('dividendYield', 'N/A'),\n",
        "    }\n",
        "\n",
        "    def safe_get(df, key, default=0):\n",
        "        if isinstance(df, pd.Series):\n",
        "            return df.get(key, default)\n",
        "        elif isinstance(df, pd.DataFrame):\n",
        "            return df.loc[key].iloc[0] if key in df.index else default\n",
        "        return default\n",
        "\n",
        "    def safe_divide(a, b):\n",
        "        if isinstance(a, str) or isinstance(b, str):\n",
        "            return 0\n",
        "        return a / b if b != 0 else 0\n",
        "\n",
        "    def is_valid_series(s):\n",
        "        return isinstance(s, pd.Series) and not s.empty and s.notna().any()\n",
        "\n",
        "    if not income_stmt.empty and not balance_sheet.empty and not cash_flow.empty:\n",
        "        latest_income = income_stmt.iloc[:, 0] if isinstance(income_stmt, pd.DataFrame) else income_stmt\n",
        "        latest_balance = balance_sheet.iloc[:, 0] if isinstance(balance_sheet, pd.DataFrame) else balance_sheet\n",
        "        latest_cash_flow = cash_flow.iloc[:, 0] if isinstance(cash_flow, pd.DataFrame) else cash_flow\n",
        "\n",
        "        # EV/EBITDA\n",
        "        enterprise_value = key_stats.get('enterpriseValue', 0)\n",
        "        ebitda = safe_get(latest_income, 'EBITDA', 0)\n",
        "        ratios['EV/EBITDA'] = safe_divide(enterprise_value, ebitda)\n",
        "\n",
        "        # Price to Sales (P/S) Ratio\n",
        "        market_cap = key_stats.get('marketCap', 0)\n",
        "        revenue = safe_get(latest_income, 'Total Revenue', 0)\n",
        "        ratios['P/S Ratio'] = safe_divide(market_cap, revenue)\n",
        "\n",
        "        # PEG Ratio calculation\n",
        "        pe_ratio = ratios['P/E Ratio']\n",
        "        earnings_growth = key_stats.get('earningsGrowth', 0)\n",
        "        if isinstance(pe_ratio, (int, float)) and isinstance(earnings_growth, (int, float)):\n",
        "            ratios['PEG Ratio'] = safe_divide(pe_ratio, earnings_growth)\n",
        "        else:\n",
        "            ratios['PEG Ratio'] = 0\n",
        "\n",
        "        # Return on Assets (ROA)\n",
        "        net_income = safe_get(latest_income, 'Net Income', 0)\n",
        "        total_assets = safe_get(latest_balance, 'Total Assets', 0)\n",
        "        ratios['ROA'] = safe_divide(net_income, total_assets)\n",
        "\n",
        "        # Return on Invested Capital (ROIC)\n",
        "        nopat = net_income + safe_get(latest_income, 'Interest Expense', 0) * (1 - key_stats.get('effectiveTaxRate', 0))\n",
        "        invested_capital = safe_get(latest_balance, 'Total Stockholder Equity', 0) + safe_get(latest_balance, 'Total Liabilities', 0) - safe_get(latest_balance, 'Total Current Liabilities', 0)\n",
        "        ratios['ROIC'] = safe_divide(nopat, invested_capital)\n",
        "\n",
        "        # Calculate Gross Margin\n",
        "        gross_profit = safe_get(latest_income, 'Gross Profit')\n",
        "        if not is_valid_series(gross_profit):\n",
        "            total_revenue = safe_get(latest_income, 'Total Revenue')\n",
        "            cost_of_revenue = safe_get(latest_income, 'Cost of Revenue')\n",
        "            if is_valid_series(total_revenue) and is_valid_series(cost_of_revenue):\n",
        "                gross_profit = total_revenue - cost_of_revenue\n",
        "\n",
        "        revenue = safe_get(latest_income, 'Total Revenue')\n",
        "        ratios['Gross Margin'] = safe_divide(gross_profit, revenue)\n",
        "\n",
        "        # Interest Coverage Ratio\n",
        "        ebit = safe_get(latest_income, 'EBIT', 0)\n",
        "        interest_expense = safe_get(latest_income, 'Interest Expense', 0)\n",
        "        ratios['Interest Coverage Ratio'] = safe_divide(ebit, interest_expense)\n",
        "\n",
        "        # Asset Turnover Ratio\n",
        "        ratios['Asset Turnover Ratio'] = safe_divide(revenue, total_assets)\n",
        "\n",
        "        # Inventory Turnover Ratio\n",
        "        cogs = safe_get(latest_income, 'Cost of Revenue', 0)\n",
        "        average_inventory = safe_get(latest_balance, 'Inventory', 0)\n",
        "        ratios['Inventory Turnover Ratio'] = safe_divide(cogs, average_inventory)\n",
        "\n",
        "        # Receivables Turnover Ratio\n",
        "        average_receivables = safe_get(latest_balance, 'Net Receivables', 0)\n",
        "        ratios['Receivables Turnover Ratio'] = safe_divide(revenue, average_receivables)\n",
        "\n",
        "        # Payout Ratio\n",
        "        dividends_paid = abs(safe_get(latest_cash_flow, 'Dividends Paid', 0))\n",
        "        ratios['Payout Ratio'] = safe_divide(dividends_paid, net_income)\n",
        "\n",
        "        # Free Cash Flow Yield\n",
        "        free_cash_flow = safe_get(latest_cash_flow, 'Free Cash Flow', 0)\n",
        "        ratios['Free Cash Flow Yield'] = safe_divide(free_cash_flow, market_cap)\n",
        "\n",
        "        # Price to Cash Flow Ratio\n",
        "        operating_cash_flow = safe_get(latest_cash_flow, 'Operating Cash Flow', 0)\n",
        "        ratios['Price to Cash Flow Ratio'] = safe_divide(market_cap, operating_cash_flow)\n",
        "\n",
        "        # Debt to EBITDA Ratio\n",
        "        total_debt = safe_get(latest_balance, 'Total Debt', 0)\n",
        "        ratios['Debt to EBITDA Ratio'] = safe_divide(total_debt, ebitda)\n",
        "\n",
        "        # Net Debt to EBITDA Ratio\n",
        "        cash_and_equivalents = safe_get(latest_balance, 'Cash and Cash Equivalents', 0)\n",
        "        net_debt = total_debt - cash_and_equivalents\n",
        "        ratios['Net Debt to EBITDA Ratio'] = safe_divide(net_debt, ebitda)\n",
        "\n",
        "        # Operating Margin\n",
        "        operating_income = safe_get(latest_income, 'Operating Income', 0)\n",
        "        ratios['Operating Margin'] = safe_divide(operating_income, revenue)\n",
        "\n",
        "        # Gross Margin and Operating Margin trends\n",
        "        if len(income_stmt.columns) > 1:\n",
        "            total_revenue = safe_get(income_stmt, 'Total Revenue')\n",
        "            gross_profit = safe_get(income_stmt, 'Gross Profit')\n",
        "\n",
        "            if not is_valid_series(gross_profit):\n",
        "                # If 'Gross Profit' is not available, try to calculate it\n",
        "                cost_of_revenue = safe_get(income_stmt, 'Cost of Revenue')\n",
        "                if is_valid_series(total_revenue) and is_valid_series(cost_of_revenue):\n",
        "                    gross_profit = total_revenue - cost_of_revenue\n",
        "\n",
        "            if is_valid_series(total_revenue) and is_valid_series(gross_profit):\n",
        "                gross_margins = gross_profit / total_revenue\n",
        "                ratios['Gross Margin Trend'] = (gross_margins.iloc[0] - gross_margins.iloc[-1]) / len(gross_margins)\n",
        "            else:\n",
        "                ratios['Gross Margin Trend'] = 'N/A'\n",
        "\n",
        "            operating_income = safe_get(income_stmt, 'Operating Income')\n",
        "            if is_valid_series(total_revenue) and is_valid_series(operating_income):\n",
        "                operating_margins = operating_income / total_revenue\n",
        "                ratios['Operating Margin Trend'] = (operating_margins.iloc[0] - operating_margins.iloc[-1]) / len(operating_margins)\n",
        "            else:\n",
        "                ratios['Operating Margin Trend'] = 'N/A'\n",
        "        else:\n",
        "            ratios['Gross Margin Trend'] = 'N/A'\n",
        "            ratios['Operating Margin Trend'] = 'N/A'\n",
        "\n",
        "        # Debt to EBITDA ratio\n",
        "        total_debt = latest_balance.get('Total Debt', 0)\n",
        "        ebitda = latest_income.get('EBITDA', 0)\n",
        "        ratios['Debt to EBITDA Ratio'] = safe_divide(total_debt, ebitda)\n",
        "\n",
        "        # Interest Coverage Ratio\n",
        "        ebit = latest_income.get('EBIT', 0)\n",
        "        interest_expense = latest_income.get('Interest Expense', 0)\n",
        "        ratios['Interest Coverage Ratio'] = safe_divide(ebit, interest_expense)\n",
        "\n",
        "        # Inventory Turnover Ratio\n",
        "        cogs = latest_income.get('Cost of Revenue', 0)\n",
        "        average_inventory = latest_balance.get('Inventory', 0)\n",
        "        ratios['Inventory Turnover Ratio'] = safe_divide(cogs, average_inventory)\n",
        "\n",
        "        # Accounts Receivable Turnover Ratio\n",
        "        revenue = latest_income.get('Total Revenue', 0)\n",
        "        average_receivables = latest_balance.get('Net Receivables', 0)\n",
        "        ratios['Accounts Receivable Turnover Ratio'] = safe_divide(revenue, average_receivables)\n",
        "\n",
        "        # Asset Turnover Ratio\n",
        "        total_assets = latest_balance.get('Total Assets', 0)\n",
        "        ratios['Asset Turnover Ratio'] = safe_divide(revenue, total_assets)\n",
        "\n",
        "        # Net Profit Margin\n",
        "        revenue = latest_income.get('Total Revenue', 0)\n",
        "        net_income = latest_income.get('Net Income', 0)\n",
        "        ratios['Net Profit Margin'] = safe_divide(net_income, revenue)\n",
        "\n",
        "        # Economic Value Added (EVA)\n",
        "        nopat = net_income + latest_income.get('Interest Expense', 0) * (1 - key_stats.get('effectiveTaxRate', 0))\n",
        "        invested_capital = latest_balance.get('Total Stockholder Equity', 0) + latest_balance.get('Total Liabilities', 0) - latest_balance.get('Total Current Liabilities', 0)\n",
        "        wacc = 0.1  # Assume a 10% weighted average cost of capital\n",
        "        ratios['EVA'] = nopat - (wacc * invested_capital)\n",
        "\n",
        "        # Market Value Added (MVA)\n",
        "        market_cap = key_stats.get('marketCap', 0)\n",
        "        ratios['MVA'] = market_cap - invested_capital\n",
        "\n",
        "        # Tobin's Q Ratio\n",
        "        market_cap = key_stats.get('marketCap', 0)\n",
        "        total_assets = safe_get(latest_balance, 'Total Assets')\n",
        "        if total_assets != 0:\n",
        "            ratios['Tobin\\'s Q Ratio'] = (market_cap + safe_get(latest_balance, 'Total Liabilities')) / total_assets\n",
        "        else:\n",
        "            ratios['Tobin\\'s Q Ratio'] = 'N/A'\n",
        "\n",
        "        # Altman Z-Score\n",
        "        working_capital = safe_get(latest_balance, 'Total Current Assets') - safe_get(latest_balance, 'Total Current Liabilities')\n",
        "        retained_earnings = safe_get(latest_balance, 'Retained Earnings')\n",
        "        ebit = safe_get(latest_income, 'EBIT')\n",
        "        book_value_equity = safe_get(latest_balance, 'Total Stockholder Equity')\n",
        "        sales = safe_get(latest_income, 'Total Revenue')\n",
        "\n",
        "        if total_assets != 0:\n",
        "            z_score = (\n",
        "                1.2 * safe_divide(working_capital, total_assets) +\n",
        "                1.4 * safe_divide(retained_earnings, total_assets) +\n",
        "                3.3 * safe_divide(ebit, total_assets) +\n",
        "                0.6 * safe_divide(market_cap, safe_get(latest_balance, 'Total Liabilities')) +\n",
        "                1.0 * safe_divide(sales, total_assets)\n",
        "            )\n",
        "            ratios['Altman Z-Score'] = z_score\n",
        "        else:\n",
        "            ratios['Altman Z-Score'] = 'N/A'\n",
        "\n",
        "        # Piotroski F-Score\n",
        "        f_score = 0\n",
        "        if safe_get(latest_income, 'Net Income') > 0:\n",
        "            f_score += 1\n",
        "        if safe_get(latest_cash_flow, 'Operating Cash Flow') > 0:\n",
        "            f_score += 1\n",
        "        if len(income_stmt.columns) > 1:\n",
        "            prev_income = income_stmt.iloc[:, 1]\n",
        "            if safe_get(latest_income, 'Net Income') > safe_get(prev_income, 'Net Income'):\n",
        "                f_score += 1\n",
        "        if safe_get(latest_cash_flow, 'Operating Cash Flow') > safe_get(latest_income, 'Net Income'):\n",
        "            f_score += 1\n",
        "        if len(balance_sheet.columns) > 1:\n",
        "            prev_balance = balance_sheet.iloc[:, 1]\n",
        "            if safe_get(latest_balance, 'Long Term Debt') <= safe_get(prev_balance, 'Long Term Debt'):\n",
        "                f_score += 1\n",
        "            if safe_divide(safe_get(latest_balance, 'Total Current Assets'), safe_get(latest_balance, 'Total Current Liabilities')) > safe_divide(safe_get(prev_balance, 'Total Current Assets'), safe_get(prev_balance, 'Total Current Liabilities')):\n",
        "                f_score += 1\n",
        "        if len(income_stmt.columns) > 1:\n",
        "            if safe_get(latest_income, 'Shares Outstanding') <= safe_get(prev_income, 'Shares Outstanding'):\n",
        "                f_score += 1\n",
        "            if safe_get(latest_income, 'Gross Margin') > safe_get(prev_income, 'Gross Margin'):\n",
        "                f_score += 1\n",
        "            if safe_divide(safe_get(latest_income, 'Total Revenue'), total_assets) > safe_divide(safe_get(prev_income, 'Total Revenue'), safe_get(prev_balance, 'Total Assets')):\n",
        "                f_score += 1\n",
        "        ratios['Piotroski F-Score'] = f_score\n",
        "\n",
        "        # Beneish M-Score\n",
        "        if len(income_stmt.columns) > 1 and len(balance_sheet.columns) > 1:\n",
        "            prev_income = income_stmt.iloc[:, 1]\n",
        "            prev_balance = balance_sheet.iloc[:, 1]\n",
        "\n",
        "            dsri = safe_divide(safe_divide(safe_get(latest_balance, 'Net Receivables'), safe_get(latest_income, 'Total Revenue')),\n",
        "                               safe_divide(safe_get(prev_balance, 'Net Receivables'), safe_get(prev_income, 'Total Revenue')))\n",
        "            gmi = safe_divide(safe_divide(safe_get(prev_income, 'Gross Profit'), safe_get(prev_income, 'Total Revenue')),\n",
        "                              safe_divide(safe_get(latest_income, 'Gross Profit'), safe_get(latest_income, 'Total Revenue')))\n",
        "            aqi = safe_divide((1 - safe_divide(safe_get(latest_balance, 'Property Plant Equipment') + safe_get(latest_balance, 'Current Assets'), total_assets)),\n",
        "                              (1 - safe_divide(safe_get(prev_balance, 'Property Plant Equipment') + safe_get(prev_balance, 'Current Assets'), safe_get(prev_balance, 'Total Assets'))))\n",
        "            sgi = safe_divide(safe_get(latest_income, 'Total Revenue'), safe_get(prev_income, 'Total Revenue'))\n",
        "            depi = safe_divide(safe_divide(safe_get(prev_balance, 'Accumulated Depreciation'), (safe_get(prev_balance, 'Property Plant Equipment') + safe_get(prev_balance, 'Accumulated Depreciation'))),\n",
        "                               safe_divide(safe_get(latest_balance, 'Accumulated Depreciation'), (safe_get(latest_balance, 'Property Plant Equipment') + safe_get(latest_balance, 'Accumulated Depreciation'))))\n",
        "            sgai = safe_divide(safe_divide(safe_get(latest_income, 'Selling General Administrative'), safe_get(latest_income, 'Total Revenue')),\n",
        "                               safe_divide(safe_get(prev_income, 'Selling General Administrative'), safe_get(prev_income, 'Total Revenue')))\n",
        "            lvgi = safe_divide(safe_divide(safe_get(latest_balance, 'Total Liabilities'), total_assets),\n",
        "                               safe_divide(safe_get(prev_balance, 'Total Liabilities'), safe_get(prev_balance, 'Total Assets')))\n",
        "            tata = safe_divide((safe_get(latest_income, 'Net Income') - safe_get(latest_cash_flow, 'Operating Cash Flow')), total_assets)\n",
        "\n",
        "            m_score = -4.84 + 0.92*dsri + 0.528*gmi + 0.404*aqi + 0.892*sgi + 0.115*depi - 0.172*sgai + 4.679*tata - 0.327*lvgi\n",
        "            ratios['Beneish M-Score'] = m_score\n",
        "        else:\n",
        "            ratios['Beneish M-Score'] = 'N/A'\n",
        "\n",
        "        # Calculate Short Interest Ratio\n",
        "        short_interest = key_stats.get('sharesShort', 0)\n",
        "        avg_daily_volume = key_stats.get('averageVolume', 1)  # Use 1 as default to avoid division by zero\n",
        "        ratios['Short Interest Ratio'] = safe_divide(short_interest, avg_daily_volume)\n",
        "\n",
        "        # Calculate Analyst Revision Ratio\n",
        "        up_revisions = key_stats.get('revenueQuarterlyGrowth', 0)  # Use revenue growth as a proxy for upward revisions\n",
        "        down_revisions = key_stats.get('earningsQuarterlyGrowth', 0)  # Use earnings growth as a proxy for downward revisions\n",
        "        ratios['Analyst Revision Ratio'] = safe_divide(up_revisions, down_revisions)\n",
        "\n",
        "    return ratios\n",
        "\n",
        "def analyze_financial_ratios(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/ratios/{ticker}?period=quarter&limit=24&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No financial ratio data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Financial Ratio Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Current Ratio', 'currentRatio'),\n",
        "            ('Quick Ratio', 'quickRatio'),\n",
        "            ('Cash Ratio', 'cashRatio'),\n",
        "            ('Gross Profit Margin', 'grossProfitMargin'),\n",
        "            ('Operating Profit Margin', 'operatingProfitMargin'),\n",
        "            ('Net Profit Margin', 'netProfitMargin'),\n",
        "            ('Return on Assets', 'returnOnAssets'),\n",
        "            ('Return on Equity', 'returnOnEquity'),\n",
        "            ('Return on Capital Employed', 'returnOnCapitalEmployed'),\n",
        "            ('Debt Ratio', 'debtRatio'),\n",
        "            ('Debt to Equity Ratio', 'debtEquityRatio'),\n",
        "            ('Interest Coverage', 'interestCoverage'),\n",
        "            ('Asset Turnover', 'assetTurnover'),\n",
        "            ('Inventory Turnover', 'inventoryTurnover'),\n",
        "            ('Receivables Turnover', 'receivablesTurnover'),\n",
        "            ('Payables Turnover', 'payablesTurnover'),\n",
        "            ('Price to Earnings Ratio', 'priceEarningsRatio'),\n",
        "            ('Price to Book Ratio', 'priceToBookRatio'),\n",
        "            ('Price to Sales Ratio', 'priceToSalesRatio'),\n",
        "            ('Dividend Yield', 'dividendYield'),\n",
        "            ('Earnings Yield', 'earningsYield'),\n",
        "            ('Free Cash Flow Yield', 'freeCashFlowYield')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value:.4f}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = np.mean(historical_values)\n",
        "                        if avg_historical != 0:\n",
        "                            pct_change = ((value - avg_historical) / avg_historical) * 100\n",
        "                            trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                            analysis += f\"  - Trend: {trend} (24-quarter avg: {avg_historical:.4f})\\n\"\n",
        "                            analysis += f\"  - Change from 24-quarter average: {pct_change:.2f}%\\n\"\n",
        "                        else:\n",
        "                            analysis += f\"  - Trend: Unable to calculate (24-quarter avg is zero)\\n\"\n",
        "\n",
        "                        # Analyze the trend over the past 24 quarters\n",
        "                        if len(historical_values) >= 24:\n",
        "                            trend_slope = np.polyfit(range(24), historical_values[:24], 1)[0]\n",
        "                            if trend_slope > 0:\n",
        "                                analysis += f\"  - Positive trend over past 24 quarters (slope: {trend_slope:.4f})\\n\"\n",
        "                            elif trend_slope < 0:\n",
        "                                analysis += f\"  - Negative trend over past 24 quarters (slope: {trend_slope:.4f})\\n\"\n",
        "                            else:\n",
        "                                analysis += \"  - Stable trend over past 24 quarters\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Ratio interpretation\n",
        "                analysis += interpret_ratio(metric_name, value)\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nOverall Financial Health Assessment:\\n\"\n",
        "        analysis += assess_overall_health(latest_data)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching financial ratio data for {ticker}: {str(e)}\"\n",
        "\n",
        "def interpret_ratio(metric_name, value):\n",
        "    interpretations = {\n",
        "        'Current Ratio': lambda x: \"  - Good liquidity\" if x > 1.5 else \"  - Potential liquidity issues\",\n",
        "        'Quick Ratio': lambda x: \"  - Strong ability to meet short-term obligations\" if x > 1 else \"  - May struggle to meet short-term obligations\",\n",
        "        'Cash Ratio': lambda x: \"  - High cash reserves\" if x > 0.5 else \"  - Low cash reserves\",\n",
        "        'Gross Profit Margin': lambda x: \"  - Strong pricing power\" if x > 0.4 else \"  - Potential pricing pressure\",\n",
        "        'Operating Profit Margin': lambda x: \"  - Efficient operations\" if x > 0.15 else \"  - Room for operational improvement\",\n",
        "        'Net Profit Margin': lambda x: \"  - High profitability\" if x > 0.1 else \"  - Low profitability\",\n",
        "        'Return on Assets': lambda x: \"  - Efficient use of assets\" if x > 0.05 else \"  - Inefficient use of assets\",\n",
        "        'Return on Equity': lambda x: \"  - Good return for shareholders\" if x > 0.15 else \"  - Poor return for shareholders\",\n",
        "        'Return on Capital Employed': lambda x: \"  - Efficient use of capital\" if x > 0.1 else \"  - Inefficient use of capital\",\n",
        "        'Debt Ratio': lambda x: \"  - Low leverage\" if x < 0.5 else \"  - High leverage\",\n",
        "        'Debt to Equity Ratio': lambda x: \"  - Conservative financing\" if x < 1 else \"  - Aggressive financing\",\n",
        "        'Interest Coverage': lambda x: \"  - Strong ability to pay interest\" if x > 3 else \"  - May struggle to pay interest\",\n",
        "        'Asset Turnover': lambda x: \"  - Efficient use of assets\" if x > 1 else \"  - Inefficient use of assets\",\n",
        "        'Inventory Turnover': lambda x: \"  - Efficient inventory management\" if x > 6 else \"  - Potential inventory management issues\",\n",
        "        'Receivables Turnover': lambda x: \"  - Efficient collection of receivables\" if x > 8 else \"  - Slow collection of receivables\",\n",
        "        'Payables Turnover': lambda x: \"  - Timely payment to suppliers\" if x > 8 else \"  - Slow payment to suppliers\",\n",
        "        'Price to Earnings Ratio': lambda x: \"  - Potentially undervalued\" if x < 15 else \"  - Potentially overvalued\",\n",
        "        'Price to Book Ratio': lambda x: \"  - Potentially undervalued\" if x < 1.5 else \"  - Potentially overvalued\",\n",
        "        'Price to Sales Ratio': lambda x: \"  - Potentially undervalued\" if x < 2 else \"  - Potentially overvalued\",\n",
        "        'Dividend Yield': lambda x: \"  - Attractive dividend\" if x > 0.03 else \"  - Low dividend yield\",\n",
        "        'Earnings Yield': lambda x: \"  - Attractive earnings yield\" if x > 0.05 else \"  - Low earnings yield\",\n",
        "        'Free Cash Flow Yield': lambda x: \"  - Strong cash generation\" if x > 0.05 else \"  - Weak cash generation\"\n",
        "    }\n",
        "\n",
        "    return interpretations.get(metric_name, lambda x: \"\")(value) + \"\\n\"\n",
        "\n",
        "def assess_overall_health(data):\n",
        "    assessment = \"\"\n",
        "\n",
        "    # Liquidity assessment\n",
        "    if data.get('currentRatio', 0) > 1.5 and data.get('quickRatio', 0) > 1:\n",
        "        assessment += \"- Strong liquidity position\\n\"\n",
        "    elif data.get('currentRatio', 0) < 1 or data.get('quickRatio', 0) < 0.5:\n",
        "        assessment += \"- Potential liquidity issues\\n\"\n",
        "    else:\n",
        "        assessment += \"- Adequate liquidity\\n\"\n",
        "\n",
        "    # Profitability assessment\n",
        "    if data.get('netProfitMargin', 0) > 0.1 and data.get('returnOnEquity', 0) > 0.15:\n",
        "        assessment += \"- Strong profitability and returns\\n\"\n",
        "    elif data.get('netProfitMargin', 0) < 0.05 or data.get('returnOnEquity', 0) < 0.1:\n",
        "        assessment += \"- Weak profitability and returns\\n\"\n",
        "    else:\n",
        "        assessment += \"- Moderate profitability and returns\\n\"\n",
        "\n",
        "    # Leverage assessment\n",
        "    if data.get('debtEquityRatio', 0) < 0.5 and data.get('interestCoverage', 0) > 5:\n",
        "        assessment += \"- Conservative financial leverage\\n\"\n",
        "    elif data.get('debtEquityRatio', 0) > 2 or data.get('interestCoverage', 0) < 2:\n",
        "        assessment += \"- High financial leverage, potential risk\\n\"\n",
        "    else:\n",
        "        assessment += \"- Moderate financial leverage\\n\"\n",
        "\n",
        "    # Efficiency assessment\n",
        "    if data.get('assetTurnover', 0) > 1 and data.get('inventoryTurnover', 0) > 6:\n",
        "        assessment += \"- Efficient asset and inventory management\\n\"\n",
        "    elif data.get('assetTurnover', 0) < 0.5 or data.get('inventoryTurnover', 0) < 3:\n",
        "        assessment += \"- Inefficient asset and inventory management\\n\"\n",
        "    else:\n",
        "        assessment += \"- Moderate efficiency in asset and inventory management\\n\"\n",
        "\n",
        "    # Valuation assessment\n",
        "    if data.get('priceEarningsRatio', 0) < 15 and data.get('priceToBookRatio', 0) < 1.5:\n",
        "        assessment += \"- Potentially undervalued\\n\"\n",
        "    elif data.get('priceEarningsRatio', 0) > 25 or data.get('priceToBookRatio', 0) > 3:\n",
        "        assessment += \"- Potentially overvalued\\n\"\n",
        "    else:\n",
        "        assessment += \"- Moderate valuation\\n\"\n",
        "\n",
        "    return assessment\n",
        "\n",
        "def analyze_financial_growth(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/financial-growth/{ticker}?period=annual&limit=5&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No financial growth data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Financial Growth Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Revenue Growth', 'revenueGrowth'),\n",
        "            ('Gross Profit Growth', 'grossProfitGrowth'),\n",
        "            ('EBIT Growth', 'ebitgrowth'),\n",
        "            ('Operating Income Growth', 'operatingIncomeGrowth'),\n",
        "            ('Net Income Growth', 'netIncomeGrowth'),\n",
        "            ('EPS Growth', 'epsgrowth'),\n",
        "            ('EPS Diluted Growth', 'epsdilutedGrowth'),\n",
        "            ('Weighted Average Shares Growth', 'weightedAverageSharesGrowth'),\n",
        "            ('Weighted Average Shares Diluted Growth', 'weightedAverageSharesDilutedGrowth'),\n",
        "            ('Dividends per Share Growth', 'dividendsperShareGrowth'),\n",
        "            ('Operating Cash Flow Growth', 'operatingCashFlowGrowth'),\n",
        "            ('Free Cash Flow Growth', 'freeCashFlowGrowth'),\n",
        "            ('10Y Revenue Growth (per Share)', 'tenYRevenueGrowthPerShare'),\n",
        "            ('5Y Revenue Growth (per Share)', 'fiveYRevenueGrowthPerShare'),\n",
        "            ('3Y Revenue Growth (per Share)', 'threeYRevenueGrowthPerShare'),\n",
        "            ('10Y Operating CF Growth (per Share)', 'tenYOperatingCFGrowthPerShare'),\n",
        "            ('5Y Operating CF Growth (per Share)', 'fiveYOperatingCFGrowthPerShare'),\n",
        "            ('3Y Operating CF Growth (per Share)', 'threeYOperatingCFGrowthPerShare'),\n",
        "            ('10Y Net Income Growth (per Share)', 'tenYNetIncomeGrowthPerShare'),\n",
        "            ('5Y Net Income Growth (per Share)', 'fiveYNetIncomeGrowthPerShare'),\n",
        "            ('3Y Net Income Growth (per Share)', 'threeYNetIncomeGrowthPerShare'),\n",
        "            ('10Y Shareholders Equity Growth (per Share)', 'tenYShareholdersEquityGrowthPerShare'),\n",
        "            ('5Y Shareholders Equity Growth (per Share)', 'fiveYShareholdersEquityGrowthPerShare'),\n",
        "            ('3Y Shareholders Equity Growth (per Share)', 'threeYShareholdersEquityGrowthPerShare'),\n",
        "            ('10Y Dividend per Share Growth', 'tenYDividendperShareGrowthPerShare'),\n",
        "            ('5Y Dividend per Share Growth', 'fiveYDividendperShareGrowthPerShare'),\n",
        "            ('3Y Dividend per Share Growth', 'threeYDividendperShareGrowthPerShare'),\n",
        "            ('Receivables Growth', 'receivablesGrowth'),\n",
        "            ('Inventory Growth', 'inventoryGrowth'),\n",
        "            ('Asset Growth', 'assetGrowth'),\n",
        "            ('Book Value per Share Growth', 'bookValueperShareGrowth'),\n",
        "            ('Debt Growth', 'debtGrowth'),\n",
        "            ('R&D Expense Growth', 'rdexpenseGrowth'),\n",
        "            ('SG&A Expenses Growth', 'sgaexpensesGrowth')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value:.2%}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key, 0) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = sum(historical_values) / len(historical_values)\n",
        "                        trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                        analysis += f\"  - Trend: {trend} (5-year avg: {avg_historical:.2%})\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Growth consistency\n",
        "                if historical_data:\n",
        "                    consistency = all(year.get(metric_key, 0) > 0 for year in historical_data if year.get(metric_key) is not None)\n",
        "                    analysis += f\"  - Consistency: {'Consistent' if consistency else 'Inconsistent'} growth\\n\"\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nGrowth Quality Assessment:\\n\"\n",
        "        if latest_data['revenueGrowth'] > 0 and latest_data['operatingIncomeGrowth'] > latest_data['revenueGrowth']:\n",
        "            analysis += \"- High-quality growth: Operating income growing faster than revenue, indicating improving operational efficiency.\\n\"\n",
        "        elif latest_data['revenueGrowth'] > 0 and latest_data['freeCashFlowGrowth'] > latest_data['revenueGrowth']:\n",
        "            analysis += \"- Strong cash generation: Free cash flow growing faster than revenue, indicating efficient capital management.\\n\"\n",
        "\n",
        "        analysis += \"\\nSustainability Analysis:\\n\"\n",
        "        if latest_data['revenueGrowth'] > 0.2:\n",
        "            analysis += \"- Caution: Very high revenue growth rate may be difficult to sustain in the long term.\\n\"\n",
        "        if latest_data['debtGrowth'] > latest_data['revenueGrowth']:\n",
        "            analysis += \"- Warning: Debt growing faster than revenue, which may lead to financial strain if not managed properly.\\n\"\n",
        "\n",
        "        analysis += \"\\nDetailed Interpretations:\\n\"\n",
        "        if latest_data['revenueGrowth'] > 0.1:\n",
        "            analysis += \"- Strong revenue growth (>10%), indicating expanding market share or product demand.\\n\"\n",
        "        elif latest_data['revenueGrowth'] < 0:\n",
        "            analysis += \"- Negative revenue growth, suggesting potential market challenges or declining demand.\\n\"\n",
        "\n",
        "        if latest_data['netIncomeGrowth'] > latest_data['revenueGrowth']:\n",
        "            analysis += \"- Net income growing faster than revenue, indicating improving profitability and efficiency.\\n\"\n",
        "        elif latest_data['netIncomeGrowth'] < latest_data['revenueGrowth']:\n",
        "            analysis += \"- Net income growing slower than revenue, suggesting potential cost management issues.\\n\"\n",
        "\n",
        "        if latest_data['freeCashFlowGrowth'] > 0:\n",
        "            analysis += \"- Positive free cash flow growth, indicating strong financial health and ability to fund growth or return value to shareholders.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Negative free cash flow growth, which may limit the company's financial flexibility.\\n\"\n",
        "\n",
        "        if latest_data['debtGrowth'] > 0:\n",
        "            analysis += \"- Increasing debt levels, which may indicate expansion or potential financial risk.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Decreasing debt levels, suggesting improved financial stability.\\n\"\n",
        "\n",
        "        if latest_data['rdexpenseGrowth'] > latest_data['revenueGrowth']:\n",
        "            analysis += \"- R&D expenses growing faster than revenue, indicating strong focus on innovation and future growth.\\n\"\n",
        "\n",
        "        if latest_data['bookValueperShareGrowth'] > 0:\n",
        "            analysis += \"- Positive book value per share growth, suggesting increasing intrinsic value.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Negative book value per share growth, which may be a concern for long-term value.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching financial growth data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_balance_sheet_growth(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/balance-sheet-statement-growth/{ticker}?period=annual&limit=5&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No balance sheet growth data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Balance Sheet Growth Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Cash and Cash Equivalents Growth', 'growthCashAndCashEquivalents'),\n",
        "            ('Short Term Investments Growth', 'growthShortTermInvestments'),\n",
        "            ('Cash and Short Term Investments Growth', 'growthCashAndShortTermInvestments'),\n",
        "            ('Net Receivables Growth', 'growthNetReceivables'),\n",
        "            ('Inventory Growth', 'growthInventory'),\n",
        "            ('Other Current Assets Growth', 'growthOtherCurrentAssets'),\n",
        "            ('Total Current Assets Growth', 'growthTotalCurrentAssets'),\n",
        "            ('Property Plant Equipment Net Growth', 'growthPropertyPlantEquipmentNet'),\n",
        "            ('Goodwill Growth', 'growthGoodwill'),\n",
        "            ('Intangible Assets Growth', 'growthIntangibleAssets'),\n",
        "            ('Goodwill and Intangible Assets Growth', 'growthGoodwillAndIntangibleAssets'),\n",
        "            ('Long Term Investments Growth', 'growthLongTermInvestments'),\n",
        "            ('Tax Assets Growth', 'growthTaxAssets'),\n",
        "            ('Other Non Current Assets Growth', 'growthOtherNonCurrentAssets'),\n",
        "            ('Total Non Current Assets Growth', 'growthTotalNonCurrentAssets'),\n",
        "            ('Total Assets Growth', 'growthTotalAssets'),\n",
        "            ('Account Payables Growth', 'growthAccountPayables'),\n",
        "            ('Short Term Debt Growth', 'growthShortTermDebt'),\n",
        "            ('Tax Payables Growth', 'growthTaxPayables'),\n",
        "            ('Deferred Revenue Growth', 'growthDeferredRevenue'),\n",
        "            ('Other Current Liabilities Growth', 'growthOtherCurrentLiabilities'),\n",
        "            ('Total Current Liabilities Growth', 'growthTotalCurrentLiabilities'),\n",
        "            ('Long Term Debt Growth', 'growthLongTermDebt'),\n",
        "            ('Deferred Revenue Non Current Growth', 'growthDeferredRevenueNonCurrent'),\n",
        "            ('Deferred Tax Liabilities Non Current Growth', 'growthDeferredTaxLiabilitiesNonCurrent'),\n",
        "            ('Other Non Current Liabilities Growth', 'growthOtherNonCurrentLiabilities'),\n",
        "            ('Total Non Current Liabilities Growth', 'growthTotalNonCurrentLiabilities'),\n",
        "            ('Total Liabilities Growth', 'growthTotalLiabilities'),\n",
        "            ('Common Stock Growth', 'growthCommonStock'),\n",
        "            ('Retained Earnings Growth', 'growthRetainedEarnings'),\n",
        "            ('Accumulated Other Comprehensive Income Loss Growth', 'growthAccumulatedOtherComprehensiveIncomeLoss'),\n",
        "            ('Other Total Stockholders Equity Growth', 'growthOthertotalStockholdersEquity'),\n",
        "            ('Total Stockholders Equity Growth', 'growthTotalStockholdersEquity'),\n",
        "            ('Total Liabilities and Stockholders Equity Growth', 'growthTotalLiabilitiesAndStockholdersEquity'),\n",
        "            ('Total Investments Growth', 'growthTotalInvestments'),\n",
        "            ('Total Debt Growth', 'growthTotalDebt'),\n",
        "            ('Net Debt Growth', 'growthNetDebt')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value:.2%}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = np.mean(historical_values)\n",
        "                        trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                        analysis += f\"  - Trend: {trend} (5-year avg: {avg_historical:.2%})\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Growth consistency\n",
        "                if historical_data:\n",
        "                    non_zero_values = [v for v in historical_values if v != 0]\n",
        "                    if non_zero_values:\n",
        "                        consistency = \"Consistent\" if (all(v > 0 for v in non_zero_values) or all(v < 0 for v in non_zero_values)) else \"Inconsistent\"\n",
        "                    else:\n",
        "                        consistency = \"Consistent\" if value == 0 else \"Inconsistent\"\n",
        "                    analysis += f\"  - Consistency: {consistency} growth\\n\"\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nBalance Sheet Health Assessment:\\n\"\n",
        "        if latest_data['growthTotalAssets'] > latest_data['growthTotalLiabilities']:\n",
        "            analysis += \"- Positive: Assets are growing faster than liabilities, indicating improving financial health.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Liabilities are growing faster than assets, which may indicate increasing financial risk.\\n\"\n",
        "\n",
        "        if latest_data['growthCashAndShortTermInvestments'] > latest_data['growthTotalCurrentLiabilities']:\n",
        "            analysis += \"- Positive: Liquid assets are growing faster than current liabilities, improving short-term solvency.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Current liabilities are growing faster than liquid assets, which may pressure short-term liquidity.\\n\"\n",
        "\n",
        "        if latest_data['growthRetainedEarnings'] > 0:\n",
        "            analysis += \"- Positive: Growing retained earnings indicate consistent profitability and reinvestment in the business.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Declining retained earnings may indicate profitability issues or significant dividend payouts.\\n\"\n",
        "\n",
        "        analysis += \"\\nDetailed Interpretations:\\n\"\n",
        "        if latest_data['growthTotalAssets'] > 0.1:\n",
        "            analysis += \"- Strong asset growth (>10%), indicating expansion of the company's resource base.\\n\"\n",
        "        elif latest_data['growthTotalAssets'] < 0:\n",
        "            analysis += \"- Negative asset growth, suggesting potential contraction or divestment of resources.\\n\"\n",
        "\n",
        "        if latest_data['growthTotalDebt'] > latest_data['growthTotalAssets']:\n",
        "            analysis += \"- Debt is growing faster than assets, which may increase financial leverage and risk.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Assets are growing faster than debt, potentially improving the company's debt-to-asset ratio.\\n\"\n",
        "\n",
        "        if latest_data['growthInventory'] > latest_data['growthTotalAssets']:\n",
        "            analysis += \"- Inventory is growing faster than total assets, which may indicate potential overstock or slow turnover.\\n\"\n",
        "\n",
        "        if latest_data['growthNetReceivables'] > latest_data['growthTotalAssets']:\n",
        "            analysis += \"- Receivables are growing faster than total assets, which may indicate potential collection issues or looser credit policies.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching balance sheet growth data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_income_growth(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/income-statement-growth/{ticker}?period=annual&limit=5&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No income growth data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Income Growth Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Revenue Growth', 'growthRevenue'),\n",
        "            ('Cost of Revenue Growth', 'growthCostOfRevenue'),\n",
        "            ('Gross Profit Growth', 'growthGrossProfit'),\n",
        "            ('Gross Profit Ratio Growth', 'growthGrossProfitRatio'),\n",
        "            ('R&D Expenses Growth', 'growthResearchAndDevelopmentExpenses'),\n",
        "            ('G&A Expenses Growth', 'growthGeneralAndAdministrativeExpenses'),\n",
        "            ('SG&A Expenses Growth', 'growthSellingGeneralAndAdministrativeExpenses'),\n",
        "            ('Selling and Marketing Expenses Growth', 'growthSellingAndMarketingExpenses'),\n",
        "            ('Other Expenses Growth', 'growthOtherExpenses'),\n",
        "            ('Operating Expenses Growth', 'growthOperatingExpenses'),\n",
        "            ('Cost and Expenses Growth', 'growthCostAndExpenses'),\n",
        "            ('Interest Expense Growth', 'growthInterestExpense'),\n",
        "            ('Interest Income Growth', 'growthInterestIncome'),\n",
        "            ('Depreciation and Amortization Growth', 'growthDepreciationAndAmortization'),\n",
        "            ('EBITDA Growth', 'growthEbitda'),\n",
        "            ('EBITDA Ratio Growth', 'growthEbitdaRatio'),\n",
        "            ('Operating Income Growth', 'growthOperatingIncome'),\n",
        "            ('Operating Income Ratio Growth', 'growthOperatingIncomeRatio'),\n",
        "            ('Total Other Income Expenses Net Growth', 'growthTotalOtherIncomeExpensesNet'),\n",
        "            ('Income Before Tax Growth', 'growthIncomeBeforeTax'),\n",
        "            ('Income Before Tax Ratio Growth', 'growthIncomeBeforeTaxRatio'),\n",
        "            ('Income Tax Expense Growth', 'growthIncomeTaxExpense'),\n",
        "            ('Net Income Growth', 'growthNetIncome'),\n",
        "            ('Net Income Ratio Growth', 'growthNetIncomeRatio'),\n",
        "            ('EPS Growth', 'growthEps'),\n",
        "            ('EPS Diluted Growth', 'growthEpsdiluted'),\n",
        "            ('Weighted Average Shares Outstanding Growth', 'growthWeightedAverageShsOut'),\n",
        "            ('Weighted Average Shares Outstanding Diluted Growth', 'growthWeightedAverageShsOutDil')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value:.2%}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = np.mean(historical_values)\n",
        "                        trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                        analysis += f\"  - Trend: {trend} (5-year avg: {avg_historical:.2%})\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Growth consistency\n",
        "                if historical_data:\n",
        "                    non_zero_values = [v for v in historical_values if v != 0]\n",
        "                    if non_zero_values:\n",
        "                        consistency = \"Consistent\" if (all(v > 0 for v in non_zero_values) or all(v < 0 for v in non_zero_values)) else \"Inconsistent\"\n",
        "                    else:\n",
        "                        consistency = \"Consistent\" if value == 0 else \"Inconsistent\"\n",
        "                    analysis += f\"  - Consistency: {consistency} growth\\n\"\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nIncome Statement Health Assessment:\\n\"\n",
        "        if latest_data.get('growthRevenue', 0) > latest_data.get('growthCostOfRevenue', 0):\n",
        "            analysis += \"- Positive: Revenue is growing faster than cost of revenue, indicating improving operational efficiency.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Cost of revenue is growing faster than revenue, which may pressure profit margins.\\n\"\n",
        "\n",
        "        if latest_data.get('growthGrossProfit', 0) > latest_data.get('growthRevenue', 0):\n",
        "            analysis += \"- Positive: Gross profit is growing faster than revenue, suggesting improving pricing power or cost management.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Gross profit growth is lagging revenue growth, which may indicate pricing pressure or rising input costs.\\n\"\n",
        "\n",
        "        if latest_data.get('growthNetIncome', 0) > latest_data.get('growthRevenue', 0):\n",
        "            analysis += \"- Positive: Net income is growing faster than revenue, indicating improving overall profitability.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Net income growth is lagging revenue growth, which may suggest increasing costs or expenses.\\n\"\n",
        "\n",
        "        analysis += \"\\nDetailed Interpretations:\\n\"\n",
        "        if latest_data.get('growthRevenue', 0) > 0.1:\n",
        "            analysis += \"- Strong revenue growth (>10%), indicating expanding market share or product demand.\\n\"\n",
        "        elif latest_data.get('growthRevenue', 0) < 0:\n",
        "            analysis += \"- Negative revenue growth, suggesting potential market challenges or declining demand.\\n\"\n",
        "\n",
        "        if latest_data.get('growthOperatingIncome', 0) > latest_data.get('growthRevenue', 0):\n",
        "            analysis += \"- Operating income growing faster than revenue, indicating improving operational efficiency.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Operating income growth lagging revenue growth, suggesting potential cost management issues.\\n\"\n",
        "\n",
        "        if 'growthEps' in latest_data and 'growthNetIncome' in latest_data:\n",
        "            if latest_data['growthEps'] > latest_data['growthNetIncome']:\n",
        "                analysis += \"- EPS growing faster than net income, potentially due to share buybacks or reduced share count.\\n\"\n",
        "            elif latest_data['growthEps'] < latest_data['growthNetIncome']:\n",
        "                analysis += \"- EPS growth lagging net income growth, potentially due to share dilution.\\n\"\n",
        "\n",
        "        if latest_data.get('growthResearchAndDevelopmentExpenses', 0) > latest_data.get('growthRevenue', 0):\n",
        "            analysis += \"- R&D expenses growing faster than revenue, indicating increased focus on innovation and future growth.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching income growth data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_key_metrics(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/key-metrics/{ticker}?period=annual&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No key metrics data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Key Financial Metrics Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Revenue per Share', 'revenuePerShare'),\n",
        "            ('Net Income per Share', 'netIncomePerShare'),\n",
        "            ('Operating Cash Flow per Share', 'operatingCashFlowPerShare'),\n",
        "            ('Free Cash Flow per Share', 'freeCashFlowPerShare'),\n",
        "            ('Cash per Share', 'cashPerShare'),\n",
        "            ('Book Value per Share', 'bookValuePerShare'),\n",
        "            ('Tangible Book Value per Share', 'tangibleBookValuePerShare'),\n",
        "            ('Shareholders Equity per Share', 'shareholdersEquityPerShare'),\n",
        "            ('Interest Debt per Share', 'interestDebtPerShare'),\n",
        "            ('Market Cap', 'marketCap'),\n",
        "            ('Enterprise Value', 'enterpriseValue'),\n",
        "            ('PE Ratio', 'peRatio'),\n",
        "            ('Price to Sales Ratio', 'priceToSalesRatio'),\n",
        "            ('POCF Ratio', 'pocfratio'),\n",
        "            ('PFCF Ratio', 'pfcfRatio'),\n",
        "            ('PB Ratio', 'pbRatio'),\n",
        "            ('PTB Ratio', 'ptbRatio'),\n",
        "            ('EV to Sales', 'evToSales'),\n",
        "            ('Enterprise Value over EBITDA', 'enterpriseValueOverEBITDA'),\n",
        "            ('EV to Operating Cash Flow', 'evToOperatingCashFlow'),\n",
        "            ('EV to Free Cash Flow', 'evToFreeCashFlow'),\n",
        "            ('Earnings Yield', 'earningsYield'),\n",
        "            ('Free Cash Flow Yield', 'freeCashFlowYield'),\n",
        "            ('Debt to Equity', 'debtToEquity'),\n",
        "            ('Debt to Assets', 'debtToAssets'),\n",
        "            ('Net Debt to EBITDA', 'netDebtToEBITDA'),\n",
        "            ('Current Ratio', 'currentRatio'),\n",
        "            ('Interest Coverage', 'interestCoverage'),\n",
        "            ('Income Quality', 'incomeQuality'),\n",
        "            ('Dividend Yield', 'dividendYield'),\n",
        "            ('Payout Ratio', 'payoutRatio'),\n",
        "            ('SG&A to Revenue', 'salesGeneralAndAdministrativeToRevenue'),\n",
        "            ('R&D to Revenue', 'researchAndDdevelopementToRevenue'),\n",
        "            ('Intangibles to Total Assets', 'intangiblesToTotalAssets'),\n",
        "            ('CAPEX to Operating Cash Flow', 'capexToOperatingCashFlow'),\n",
        "            ('CAPEX to Revenue', 'capexToRevenue'),\n",
        "            ('CAPEX to Depreciation', 'capexToDepreciation'),\n",
        "            ('Stock-Based Compensation to Revenue', 'stockBasedCompensationToRevenue'),\n",
        "            ('Graham Number', 'grahamNumber'),\n",
        "            ('ROIC', 'roic'),\n",
        "            ('Return on Tangible Assets', 'returnOnTangibleAssets'),\n",
        "            ('Graham Net-Net', 'grahamNetNet'),\n",
        "            ('Working Capital', 'workingCapital'),\n",
        "            ('Tangible Asset Value', 'tangibleAssetValue'),\n",
        "            ('Net Current Asset Value', 'netCurrentAssetValue'),\n",
        "            ('Invested Capital', 'investedCapital'),\n",
        "            ('Average Receivables', 'averageReceivables'),\n",
        "            ('Average Payables', 'averagePayables'),\n",
        "            ('Average Inventory', 'averageInventory'),\n",
        "            ('Days Sales Outstanding', 'daysSalesOutstanding'),\n",
        "            ('Days Payables Outstanding', 'daysPayablesOutstanding'),\n",
        "            ('Days of Inventory on Hand', 'daysOfInventoryOnHand'),\n",
        "            ('Receivables Turnover', 'receivablesTurnover'),\n",
        "            ('Payables Turnover', 'payablesTurnover'),\n",
        "            ('Inventory Turnover', 'inventoryTurnover'),\n",
        "            ('ROE', 'roe'),\n",
        "            ('CAPEX per Share', 'capexPerShare')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = sum(historical_values) / len(historical_values)\n",
        "                        trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                        analysis += f\"  - Trend: {trend} (5-year avg: {avg_historical:.2f})\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Metric interpretation\n",
        "                analysis += interpret_metric(metric_name, value, historical_values if historical_data else None)\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching key metrics data for {ticker}: {str(e)}\"\n",
        "\n",
        "def interpret_metric(metric_name, value, historical_values):\n",
        "    interpretation = \"  - Interpretation: \"\n",
        "\n",
        "    if metric_name in ['Revenue per Share', 'Net Income per Share', 'Operating Cash Flow per Share', 'Free Cash Flow per Share']:\n",
        "        if historical_values and value > max(historical_values):\n",
        "            interpretation += \"At an all-time high, indicating strong financial performance.\\n\"\n",
        "        elif historical_values and value < min(historical_values):\n",
        "            interpretation += \"At a low point, potential cause for concern.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Within historical range.\\n\"\n",
        "\n",
        "    elif metric_name in ['PE Ratio', 'Price to Sales Ratio', 'PB Ratio']:\n",
        "        if value < 15:\n",
        "            interpretation += \"Potentially undervalued compared to the market.\\n\"\n",
        "        elif value > 30:\n",
        "            interpretation += \"Potentially overvalued compared to the market.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Within typical market range.\\n\"\n",
        "\n",
        "    elif metric_name == 'Debt to Equity':\n",
        "        if value < 0.5:\n",
        "            interpretation += \"Low leverage, indicating financial stability.\\n\"\n",
        "        elif value > 2:\n",
        "            interpretation += \"High leverage, potential financial risk.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Moderate leverage.\\n\"\n",
        "\n",
        "    elif metric_name == 'Current Ratio':\n",
        "        if value > 2:\n",
        "            interpretation += \"Strong short-term liquidity position.\\n\"\n",
        "        elif value < 1:\n",
        "            interpretation += \"Potential short-term liquidity issues.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Adequate short-term liquidity.\\n\"\n",
        "\n",
        "    elif metric_name == 'Dividend Yield':\n",
        "        if value > 0.04:\n",
        "            interpretation += \"High dividend yield, attractive for income investors.\\n\"\n",
        "        elif value == 0:\n",
        "            interpretation += \"No dividend paid.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Moderate dividend yield.\\n\"\n",
        "\n",
        "    elif metric_name == 'ROIC':\n",
        "        if value > 0.1:\n",
        "            interpretation += \"Strong return on invested capital, efficient use of capital.\\n\"\n",
        "        elif value < 0.05:\n",
        "            interpretation += \"Low return on invested capital, potential inefficiency in capital use.\\n\"\n",
        "        else:\n",
        "            interpretation += \"Moderate return on invested capital.\\n\"\n",
        "\n",
        "    else:\n",
        "        interpretation += \"Assess within industry contexts.\\n\"\n",
        "\n",
        "    return interpretation\n",
        "\n",
        "\n",
        "\n",
        "def get_institutional_ownership_analysis(ticker, api_key, quarters=1):\n",
        "    base_url = \"https://financialmodelingprep.com/api/v4/institutional-ownership/symbol-ownership\"\n",
        "\n",
        "    params = {\n",
        "        \"symbol\": ticker,\n",
        "        \"includeCurrentQuarter\": \"false\",\n",
        "        \"apikey\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No institutional ownership data available for {ticker}\"\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df = df.sort_values('date', ascending=False)\n",
        "        df = df.head(quarters)  # Limit to the specified number of quarters\n",
        "\n",
        "        analysis = f\"Institutional Ownership Analysis for {ticker} (Last {len(df)} quarter{'s' if len(df) > 1 else ''}):\\n\\n\"\n",
        "\n",
        "        # Latest quarter analysis\n",
        "        latest = df.iloc[0]\n",
        "        analysis += f\"Latest Quarter (Q{latest['quarter']} {latest['year']}):\\n\"\n",
        "        analysis += f\"- Ownership Percent: {latest['ownershipPercent']:.2f}%\\n\"\n",
        "        analysis += f\"- Number of Institutional Holders: {latest['investorsHolding']}\\n\"\n",
        "        analysis += f\"- Total Shares Held: {latest['numberOf13Fshares']:,}\\n\"\n",
        "\n",
        "        # Put/Call analysis\n",
        "        if latest['putCallRatio'] > 1:\n",
        "            analysis += f\"- Put/Call Ratio: {latest['putCallRatio']:.2f} (More puts than calls, potentially bearish)\\n\"\n",
        "        else:\n",
        "            analysis += f\"- Put/Call Ratio: {latest['putCallRatio']:.2f} (More calls than puts, potentially bullish)\\n\"\n",
        "\n",
        "        # New positions vs. closed positions\n",
        "        net_position_change = latest['newPositions'] - latest['closedPositions']\n",
        "        analysis += f\"- Net Position Change: {net_position_change:+d} \"\n",
        "        analysis += f\"(New: {latest['newPositions']}, Closed: {latest['closedPositions']})\\n\"\n",
        "\n",
        "        if len(df) > 1:\n",
        "            # Calculate trends\n",
        "            ownership_trend = df['ownershipPercent'].diff(-1).mean()\n",
        "            holders_trend = df['investorsHolding'].diff(-1).mean()\n",
        "\n",
        "            analysis += f\"\\nTrends (over {len(df)} quarters):\\n\"\n",
        "            analysis += f\"- Ownership Trend: {'Increasing' if ownership_trend > 0 else 'Decreasing'} \"\n",
        "            analysis += f\"(Average change: {ownership_trend:.2f}% per quarter)\\n\"\n",
        "            analysis += f\"- Number of Institutional Holders Trend: {'Increasing' if holders_trend > 0 else 'Decreasing'} \"\n",
        "            analysis += f\"(Average change: {holders_trend:.0f} per quarter)\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching institutional ownership data for {ticker}: {str(e)}\"\n",
        "\n",
        "def analyze_insider_trades(symbol, api_key, lookback_years=5):\n",
        "    base_url = \"https://financialmodelingprep.com/api/v4/insider-roaster-statistic\"\n",
        "\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=365 * lookback_years)\n",
        "\n",
        "    all_data = []\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        year = current_date.year\n",
        "        quarter = (current_date.month - 1) // 3 + 1\n",
        "\n",
        "        params = {\n",
        "            \"symbol\": symbol,\n",
        "            \"year\": year,\n",
        "            \"quarter\": quarter,\n",
        "            \"apikey\": api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data:\n",
        "                all_data.extend(data)\n",
        "\n",
        "        current_date += timedelta(days=90)\n",
        "\n",
        "    if not all_data:\n",
        "        return f\"No insider trade data available for {symbol}\"\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df['date'] = pd.to_datetime(df['year'].astype(str) + 'Q' + df['quarter'].astype(str))\n",
        "    df = df.sort_values('date')\n",
        "\n",
        "    total_purchases = df['purchases'].sum()\n",
        "    total_sales = df['sales'].sum()\n",
        "    total_bought = df['totalBought'].sum()\n",
        "    total_sold = df['totalSold'].sum()\n",
        "    avg_buy_sell_ratio = df['buySellRatio'].mean()\n",
        "\n",
        "    # Calculate trends using linear regression\n",
        "    df['quarter_num'] = range(len(df))\n",
        "    purchase_trend = np.polyfit(df['quarter_num'], df['purchases'], 1)[0]\n",
        "    sales_trend = np.polyfit(df['quarter_num'], df['sales'], 1)[0]\n",
        "\n",
        "    # Calculate average transaction sizes\n",
        "    avg_purchase_size = total_bought / total_purchases if total_purchases > 0 else 0\n",
        "    avg_sale_size = total_sold / total_sales if total_sales > 0 else 0\n",
        "\n",
        "    insights = f\"Insider Trade Analysis for {symbol} (Past {lookback_years} years):\\n\"\n",
        "    insights += f\"1. Total Insider Purchases: {total_purchases:,}\\n\"\n",
        "    insights += f\"2. Total Insider Sales: {total_sales:,}\\n\"\n",
        "    insights += f\"3. Total Amount Bought: ${total_bought:,.2f}\\n\"\n",
        "    insights += f\"4. Total Amount Sold: ${total_sold:,.2f}\\n\"\n",
        "    insights += f\"5. Average Buy/Sell Ratio: {avg_buy_sell_ratio:.2f}\\n\"\n",
        "    insights += f\"6. Purchase Trend: {'Increasing' if purchase_trend > 0 else 'Decreasing'} ({purchase_trend:.2f} trades per quarter)\\n\"\n",
        "    insights += f\"7. Sales Trend: {'Increasing' if sales_trend > 0 else 'Decreasing'} ({sales_trend:.2f} trades per quarter)\\n\"\n",
        "    insights += f\"8. Average Purchase Size: ${avg_purchase_size:,.2f}\\n\"\n",
        "    insights += f\"9. Average Sale Size: ${avg_sale_size:,.2f}\\n\"\n",
        "\n",
        "    if total_purchases > total_sales:\n",
        "        insights += \"10. Insiders are net buyers, which might indicate positive sentiment.\\n\"\n",
        "    else:\n",
        "        insights += \"10. Insiders are net sellers, which might indicate caution or profit-taking.\\n\"\n",
        "\n",
        "    if avg_buy_sell_ratio > 1:\n",
        "        insights += \"11. The average buy/sell ratio is greater than 1, suggesting more buying than selling activity.\\n\"\n",
        "    else:\n",
        "        insights += \"11. The average buy/sell ratio is less than 1, suggesting more selling than buying activity.\\n\"\n",
        "\n",
        "    # Add information about the largest transactions\n",
        "    largest_purchase = df['totalBought'].max()\n",
        "    largest_sale = df['totalSold'].max()\n",
        "    insights += f\"12. Largest Single Purchase: ${largest_purchase:,.2f}\\n\"\n",
        "    insights += f\"13. Largest Single Sale: ${largest_sale:,.2f}\\n\"\n",
        "\n",
        "    # Add information about recent activity\n",
        "    recent_quarter = df.iloc[-1]\n",
        "    insights += f\"14. Most Recent Quarter Activity (Q{recent_quarter['quarter']} {recent_quarter['year']}):\\n\"\n",
        "    insights += f\"    - Purchases: {recent_quarter['purchases']}\\n\"\n",
        "    insights += f\"    - Sales: {recent_quarter['sales']}\\n\"\n",
        "    insights += f\"    - Buy/Sell Ratio: {recent_quarter['buySellRatio']:.2f}\\n\"\n",
        "\n",
        "    return insights\n",
        "\n",
        "def analyze_cashflow_growth(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/cash-flow-statement-growth/{ticker}?period=annual&limit=5&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No cash flow growth data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "        historical_data = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        analysis = f\"Cash Flow Growth Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Net Income Growth', 'growthNetIncome'),\n",
        "            ('Depreciation and Amortization Growth', 'growthDepreciationAndAmortization'),\n",
        "            ('Stock Based Compensation Growth', 'growthStockBasedCompensation'),\n",
        "            ('Change in Working Capital Growth', 'growthChangeInWorkingCapital'),\n",
        "            ('Accounts Receivables Growth', 'growthAccountsReceivables'),\n",
        "            ('Inventory Growth', 'growthInventory'),\n",
        "            ('Accounts Payables Growth', 'growthAccountsPayables'),\n",
        "            ('Other Working Capital Growth', 'growthOtherWorkingCapital'),\n",
        "            ('Other Non-Cash Items Growth', 'growthOtherNonCashItems'),\n",
        "            ('Net Cash Provided by Operating Activities Growth', 'growthNetCashProvidedByOperatingActivities'),\n",
        "            ('Investments in Property, Plant and Equipment Growth', 'growthInvestmentsInPropertyPlantAndEquipment'),\n",
        "            ('Acquisitions Net Growth', 'growthAcquisitionsNet'),\n",
        "            ('Purchases of Investments Growth', 'growthPurchasesOfInvestments'),\n",
        "            ('Sales/Maturities of Investments Growth', 'growthSalesMaturitiesOfInvestments'),\n",
        "            ('Net Cash Used for Investing Activities Growth', 'growthNetCashUsedForInvestingActivities'),\n",
        "            ('Debt Repayment Growth', 'growthDebtRepayment'),\n",
        "            ('Common Stock Issued Growth', 'growthCommonStockIssued'),\n",
        "            ('Common Stock Repurchased Growth', 'growthCommonStockRepurchased'),\n",
        "            ('Deferred Income Tax Growth', 'growthDeferredIncomeTax'),\n",
        "            ('Dividends Paid Growth', 'growthDividendsPaid'),\n",
        "            ('Net Cash Used/Provided by Financing Activities Growth', 'growthNetCashUsedProvidedByFinancingActivities'),\n",
        "            ('Effect of Forex Changes on Cash Growth', 'growthEffectOfForexChangesOnCash'),\n",
        "            ('Net Change in Cash Growth', 'growthNetChangeInCash'),\n",
        "            ('Cash at End of Period Growth', 'growthCashAtEndOfPeriod'),\n",
        "            ('Cash at Beginning of Period Growth', 'growthCashAtBeginningOfPeriod'),\n",
        "            ('Operating Cash Flow Growth', 'growthOperatingCashFlow'),\n",
        "            ('Capital Expenditure Growth', 'growthCapitalExpenditure'),\n",
        "            ('Free Cash Flow Growth', 'growthFreeCashFlow'),\n",
        "            ('Other Investing Activities Growth', 'growthOtherInvestingActivites'),\n",
        "            ('Other Financing Activities Growth', 'growthOtherFinancingActivites')\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                analysis += f\"{metric_name}: {value:.2%}\\n\"\n",
        "\n",
        "                # Trend analysis\n",
        "                if historical_data:\n",
        "                    historical_values = [year.get(metric_key) for year in historical_data if year.get(metric_key) is not None]\n",
        "                    if historical_values:\n",
        "                        avg_historical = np.mean(historical_values)\n",
        "                        trend = \"improving\" if value > avg_historical else \"declining\"\n",
        "                        analysis += f\"  - Trend: {trend} (5-year avg: {avg_historical:.2%})\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  - Trend: Not enough historical data\\n\"\n",
        "\n",
        "                # Growth consistency\n",
        "                if historical_data:\n",
        "                    non_zero_values = [v for v in historical_values if v != 0]\n",
        "                    if non_zero_values:\n",
        "                        consistency = \"Consistent\" if (all(v > 0 for v in non_zero_values) or all(v < 0 for v in non_zero_values)) else \"Inconsistent\"\n",
        "                    else:\n",
        "                        consistency = \"Consistent\" if value == 0 else \"Inconsistent\"\n",
        "                    analysis += f\"  - Consistency: {consistency} growth\\n\"\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nCash Flow Health Assessment:\\n\"\n",
        "        if latest_data.get('growthOperatingCashFlow', 0) > latest_data.get('growthNetIncome', 0):\n",
        "            analysis += \"- Positive: Operating cash flow is growing faster than net income, indicating strong cash generation from operations.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Operating cash flow growth is lagging net income growth, which may indicate potential issues with cash conversion.\\n\"\n",
        "\n",
        "        if latest_data.get('growthFreeCashFlow', 0) > latest_data.get('growthOperatingCashFlow', 0):\n",
        "            analysis += \"- Positive: Free cash flow is growing faster than operating cash flow, suggesting efficient capital management.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Caution: Free cash flow growth is lagging operating cash flow growth, which may indicate increased capital expenditures.\\n\"\n",
        "\n",
        "        if latest_data.get('growthCapitalExpenditure', 0) > 0:\n",
        "            analysis += \"- Note: Increasing capital expenditures, which may indicate investments in future growth.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Note: Decreasing capital expenditures, which may indicate reduced investments in future growth.\\n\"\n",
        "\n",
        "        analysis += \"\\nDetailed Interpretations:\\n\"\n",
        "        if latest_data.get('growthOperatingCashFlow', 0) > 0.1:\n",
        "            analysis += \"- Strong operating cash flow growth (>10%), indicating robust cash generation from core business activities.\\n\"\n",
        "        elif latest_data.get('growthOperatingCashFlow', 0) < 0:\n",
        "            analysis += \"- Negative operating cash flow growth, suggesting potential challenges in cash generation from operations.\\n\"\n",
        "\n",
        "        if latest_data.get('growthFreeCashFlow', 0) > 0.1:\n",
        "            analysis += \"- Strong free cash flow growth (>10%), indicating improved ability to generate cash after capital expenditures.\\n\"\n",
        "        elif latest_data.get('growthFreeCashFlow', 0) < 0:\n",
        "            analysis += \"- Negative free cash flow growth, which may limit the company's financial flexibility and ability to fund growth or return value to shareholders.\\n\"\n",
        "\n",
        "        if latest_data.get('growthNetCashUsedForInvestingActivities', 0) > 0:\n",
        "            analysis += \"- Increasing cash used in investing activities, potentially indicating higher investments for future growth.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Decreasing cash used in investing activities, which may suggest reduced investment in growth opportunities.\\n\"\n",
        "\n",
        "        if latest_data.get('growthNetCashUsedProvidedByFinancingActivities', 0) > 0:\n",
        "            analysis += \"- Increasing cash from financing activities, which could indicate increased borrowing or equity issuance.\\n\"\n",
        "        else:\n",
        "            analysis += \"- Decreasing cash from financing activities, potentially due to debt repayment or share buybacks.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching cash flow growth data for {ticker}: {str(e)}\"\n",
        "\n",
        "def perform_dcf_analysis(ticker, api_key):\n",
        "    url = f\"https://financialmodelingprep.com/api/v4/advanced_discounted_cash_flow?symbol={ticker}&apikey={api_key}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            return f\"No DCF analysis data available for {ticker}\"\n",
        "\n",
        "        latest_data = data[0]\n",
        "\n",
        "        analysis = f\"Discounted Cash Flow (DCF) Analysis for {ticker}:\\n\\n\"\n",
        "\n",
        "        metrics = [\n",
        "            ('Current Price', 'price'),\n",
        "            ('Beta', 'beta'),\n",
        "            ('Final Tax Rate', 'finalTaxRate'),\n",
        "            ('Total Debt', 'totalDebt'),\n",
        "            ('Total Equity', 'totalEquity'),\n",
        "            ('Total Capital', 'totalCapital'),\n",
        "            ('Diluted Shares Outstanding', 'dilutedSharesOutstanding'),\n",
        "            ('Debt Weighting', 'debtWeighting'),\n",
        "            ('Equity Weighting', 'equityWeighting'),\n",
        "            ('Revenue', 'revenue'),\n",
        "            ('EBITDA', 'ebitda'),\n",
        "            ('Operating Cash Flow', 'operatingCashFlow'),\n",
        "            ('EBIT', 'ebit'),\n",
        "            ('Net Debt', 'netDebt'),\n",
        "            ('Tax Rate', 'taxRate'),\n",
        "            ('EBITDA Percentage', 'ebitdaPercentage'),\n",
        "            ('EBIT Percentage', 'ebitPercentage'),\n",
        "            ('After-Tax Cost of Debt', 'afterTaxCostOfDebt'),\n",
        "            ('Market Risk Premium', 'marketRiskPremium'),\n",
        "            ('Long-Term Growth Rate', 'longTermGrowthRate'),\n",
        "            ('Cost of Equity', 'costOfEquity'),\n",
        "            ('WACC', 'wacc'),\n",
        "            ('Free Cash Flow', 'ufcf'),\n",
        "            ('Risk-Free Rate', 'riskFreeRate'),\n",
        "            ('Terminal Value', 'terminalValue'),\n",
        "            ('Present Terminal Value', 'presentTerminalValue'),\n",
        "            ('Enterprise Value', 'enterpriseValue'),\n",
        "            ('Equity Value', 'equityValue'),\n",
        "            ('Equity Value per Share', 'equityValuePerShare'),\n",
        "            ('Free Cash Flow (T+1)', 'freeCashFlowT1'),\n",
        "            ('Cost of Debt', 'costofDebt'),\n",
        "        ]\n",
        "\n",
        "        for metric_name, metric_key in metrics:\n",
        "            value = latest_data.get(metric_key)\n",
        "            if value is not None:\n",
        "                if isinstance(value, (int, float)):\n",
        "                    if metric_name in ['Current Price', 'Equity Value per Share']:\n",
        "                        analysis += f\"{metric_name}: ${value:.2f}\\n\"\n",
        "                    elif 'Percentage' in metric_name:\n",
        "                        analysis += f\"{metric_name}: {value:.2%}\\n\"\n",
        "                    elif metric_name in ['Beta', 'WACC', 'Long-Term Growth Rate', 'Cost of Equity', 'After-Tax Cost of Debt', 'Cost of Debt']:\n",
        "                        analysis += f\"{metric_name}: {value:.4f}\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"{metric_name}: {value:,.0f}\\n\"\n",
        "                else:\n",
        "                    analysis += f\"{metric_name}: {value}\\n\"\n",
        "            else:\n",
        "                analysis += f\"{metric_name}: N/A\\n\"\n",
        "\n",
        "        analysis += \"\\nDCF Valuation Analysis:\\n\"\n",
        "\n",
        "        current_price = latest_data.get('price', 0)\n",
        "        dcf_value = latest_data.get('equityValuePerShare', 0)\n",
        "\n",
        "        if current_price > 0 and dcf_value > 0:\n",
        "            upside_potential = (dcf_value / current_price - 1) * 100\n",
        "            analysis += f\"DCF Implied Upside: {upside_potential:.2f}%\\n\"\n",
        "\n",
        "            if upside_potential > 20:\n",
        "                analysis += \"The stock appears significantly undervalued based on DCF analysis.\\n\"\n",
        "            elif upside_potential < -20:\n",
        "                analysis += \"The stock appears significantly overvalued based on DCF analysis.\\n\"\n",
        "            elif -10 <= upside_potential <= 10:\n",
        "                analysis += \"The stock appears fairly valued based on DCF analysis.\\n\"\n",
        "            elif 10 < upside_potential <= 20:\n",
        "                analysis += \"The stock appears slightly undervalued based on DCF analysis.\\n\"\n",
        "            else:\n",
        "                analysis += \"The stock appears slightly overvalued based on DCF analysis.\\n\"\n",
        "\n",
        "        wacc = latest_data.get('wacc')\n",
        "        if wacc is not None:\n",
        "            analysis += f\"\\nWACC Analysis:\\n\"\n",
        "            if wacc < 0.06:\n",
        "                analysis += \"Low WACC indicates the company has a low cost of capital, which is favorable.\\n\"\n",
        "            elif 0.06 <= wacc <= 0.10:\n",
        "                analysis += \"Moderate WACC suggests a balanced cost of capital.\\n\"\n",
        "            else:\n",
        "                analysis += \"High WACC indicates the company has a high cost of capital, which may be concerning.\\n\"\n",
        "\n",
        "        long_term_growth = latest_data.get('longTermGrowthRate')\n",
        "        if long_term_growth is not None:\n",
        "            analysis += f\"\\nLong-Term Growth Rate Analysis:\\n\"\n",
        "            if long_term_growth > 0.04:\n",
        "                analysis += \"High long-term growth rate assumption, which may be optimistic.\\n\"\n",
        "            elif 0.02 <= long_term_growth <= 0.04:\n",
        "                analysis += \"Moderate long-term growth rate assumption, which seems reasonable.\\n\"\n",
        "            else:\n",
        "                analysis += \"Low long-term growth rate assumption, which may be conservative.\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching DCF analysis data for {ticker}: {str(e)}\"\n",
        "\n",
        "def perform_time_series_decomposition(hist_data):\n",
        "    # Ensure the data is sorted by date\n",
        "    hist_data = hist_data.sort_index()\n",
        "\n",
        "    # Handle missing data\n",
        "    close_prices = hist_data['Close'].interpolate()\n",
        "\n",
        "    # Check for stationarity\n",
        "    result = adfuller(close_prices)\n",
        "    if result[1] > 0.05:\n",
        "        # If not stationary, take first difference\n",
        "        close_prices = close_prices.diff().dropna()\n",
        "\n",
        "    # Calculate the actual number of trading days in a year\n",
        "    days_per_year = len(close_prices) / ((close_prices.index[-1] - close_prices.index[0]).days / 252)\n",
        "    period = int(round(days_per_year))\n",
        "\n",
        "    # Perform STL decomposition\n",
        "    stl = STL(close_prices, period=period, robust=True)\n",
        "    result = stl.fit()\n",
        "\n",
        "    # Extract components\n",
        "    trend = result.trend\n",
        "    seasonal = result.seasonal\n",
        "    residual = result.resid\n",
        "\n",
        "    # Validate decomposition\n",
        "    reconstruction_error = np.mean(np.abs(close_prices - (trend + seasonal + residual)))\n",
        "\n",
        "    return {\n",
        "        'trend': trend.tolist(),\n",
        "        'seasonal': seasonal.tolist(),\n",
        "        'residual': residual.tolist(),\n",
        "        'reconstruction_error': reconstruction_error\n",
        "    }\n",
        "\n",
        "def summarize_decomposition(decomposition_results):\n",
        "    trend = decomposition_results['trend']\n",
        "    seasonal = decomposition_results['seasonal']\n",
        "    residual = decomposition_results['residual']\n",
        "    reconstruction_error = decomposition_results['reconstruction_error']\n",
        "\n",
        "    summary = {\n",
        "        'trend': {\n",
        "            'start': trend[0],\n",
        "            'end': trend[-1],\n",
        "            'min': min(trend),\n",
        "            'max': max(trend),\n",
        "            'mean': sum(trend) / len(trend)\n",
        "        },\n",
        "        'seasonal': {\n",
        "            'min': min(seasonal),\n",
        "            'max': max(seasonal),\n",
        "            'mean': sum(seasonal) / len(seasonal)\n",
        "        },\n",
        "        'residual': {\n",
        "            'min': min(residual),\n",
        "            'max': max(residual),\n",
        "            'mean': sum(residual) / len(residual),\n",
        "            'std': (sum((x - (sum(residual) / len(residual))) ** 2 for x in residual) / len(residual)) ** 0.5\n",
        "        },\n",
        "        'reconstruction_error': reconstruction_error\n",
        "    }\n",
        "\n",
        "    summary['interpretation'] = f\"\"\"\n",
        "    Trend: The overall direction of the stock price over time.\n",
        "    Seasonal: Repeating patterns or cycles in the stock price.\n",
        "    Residual: Unexplained variations or noise in the price data.\n",
        "    Reconstruction error: {summary['reconstruction_error']:.4f} (lower is better)\n",
        "\n",
        "    The trend component shows a {'positive' if summary['trend']['end'] > summary['trend']['start'] else 'negative'} overall trend.\n",
        "    The seasonal component has a range of {summary['seasonal']['max'] - summary['seasonal']['min']:.4f}, indicating {'strong' if (summary['seasonal']['max'] - summary['seasonal']['min']) > 0.1 else 'weak'} seasonal effects.\n",
        "    The residual component has a standard deviation of {summary['residual']['std']:.4f}, suggesting {'high' if summary['residual']['std'] > 0.05 else 'low'} unexplained volatility.\n",
        "    \"\"\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "def calculate_string_theory_metrics(hist_data):\n",
        "    close_prices = hist_data['Close'].values\n",
        "    returns = np.diff(np.log(close_prices))\n",
        "\n",
        "    # Calculate string tension (T)\n",
        "    string_tension = np.std(returns) * np.sqrt(252)  # Annualized volatility as a proxy for string tension\n",
        "\n",
        "    # Calculate compactification radius (R)\n",
        "    compactification_radius = 1 / (np.mean(np.abs(returns)) * np.sqrt(252))  # Inverse of average absolute return\n",
        "\n",
        "    # Calculate string coupling constant (g_s)\n",
        "    string_coupling = np.mean(returns) / np.std(returns)  # Sharpe ratio as a proxy for string coupling\n",
        "\n",
        "    # Calculate number of dimensions (D)\n",
        "    dimensions = int(np.round(np.log(len(returns)) / np.log(2)))  # Log base 2 of the number of data points\n",
        "\n",
        "    return {\n",
        "        'string_tension': string_tension,\n",
        "        'compactification_radius': compactification_radius,\n",
        "        'string_coupling': string_coupling,\n",
        "        'dimensions': dimensions,\n",
        "        'explanation': \"\"\"\n",
        "        String tension: Represents the volatility of the stock (higher tension = higher volatility).\n",
        "        Compactification radius: Inversely related to the average absolute return (smaller radius = larger returns).\n",
        "        String coupling: Similar to the Sharpe ratio, measuring risk-adjusted returns.\n",
        "        Dimensions: Estimates the complexity of the price dynamics.\n",
        "        These metrics provide a novel perspective on stock behavior using concepts from string theory.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "def summarize_index_data(stock_data=None, index_data=None):\n",
        "    if stock_data is None and index_data is not None:\n",
        "        # This is the case for summarizing all indices\n",
        "        all_summary = {}\n",
        "        for index, data in index_data.items():\n",
        "            all_summary[index] = summarize_index_data(data, {index: data})\n",
        "        return all_summary\n",
        "\n",
        "    # This is the case for summarizing a single index\n",
        "    stock_returns = stock_data['Close'].pct_change().dropna()\n",
        "    summary = {}\n",
        "\n",
        "    for index, data in index_data.items():\n",
        "        index_returns = data['Close'].pct_change().dropna()\n",
        "\n",
        "        # Align stock and index returns\n",
        "        aligned_returns = pd.concat([stock_returns, index_returns], axis=1).dropna()\n",
        "        stock_aligned = aligned_returns.iloc[:, 0]\n",
        "        index_aligned = aligned_returns.iloc[:, 1]\n",
        "\n",
        "        # Calculate max drawdown\n",
        "        peak = data['Close'].cummax()\n",
        "        drawdown = (peak - data['Close']) / peak\n",
        "        max_drawdown = drawdown.max() * 100\n",
        "\n",
        "        # Calculate Sharpe ratio\n",
        "        risk_free_rate = 0.02\n",
        "        excess_returns = index_returns - risk_free_rate / 252\n",
        "        sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std() if excess_returns.std() != 0 else 0\n",
        "\n",
        "        # Calculate beta\n",
        "        beta = np.cov(index_aligned, stock_aligned)[0][1] / np.var(index_aligned) if np.var(index_aligned) != 0 else 0\n",
        "\n",
        "        # Calculate alpha\n",
        "        alpha = (stock_aligned.mean() - risk_free_rate/252) - beta * (index_aligned.mean() - risk_free_rate/252)\n",
        "\n",
        "        # Calculate information ratio\n",
        "        active_returns = stock_aligned - index_aligned\n",
        "        information_ratio = active_returns.mean() / active_returns.std() if active_returns.std() != 0 else 0\n",
        "\n",
        "        # Calculate rolling correlation\n",
        "        rolling_correlation = index_aligned.rolling(60).corr(stock_aligned)\n",
        "\n",
        "        # Calculate relative strength\n",
        "        relative_strength = (stock_data['Close'].pct_change(252) - data['Close'].pct_change(252)) * 100\n",
        "\n",
        "        # Calculate momentum\n",
        "        momentum = (data['Close'].iloc[-1] / data['Close'].iloc[-90] - 1) * 100 if len(data) >= 90 else 0\n",
        "\n",
        "        # Calculate trend strength\n",
        "        short_ma = data['Close'].rolling(50).mean()\n",
        "        long_ma = data['Close'].rolling(200).mean()\n",
        "        trend_strength = ((short_ma - long_ma) / long_ma).iloc[-1] * 100 if len(data) >= 200 else 0\n",
        "\n",
        "        summary[index] = {\n",
        "            'overall_performance': (data['Close'].iloc[-1] / data['Close'].iloc[0] - 1) * 100,\n",
        "            'volatility': index_returns.std() * np.sqrt(252) * 100,  # Annualized volatility\n",
        "            'correlation': stock_aligned.corr(index_aligned),\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'best_day': index_returns.max() * 100,\n",
        "            'worst_day': index_returns.min() * 100,\n",
        "            'sharpe_ratio': sharpe_ratio,\n",
        "            'beta': beta,\n",
        "            'alpha': alpha,\n",
        "            'information_ratio': information_ratio,\n",
        "            'rolling_correlation': rolling_correlation.iloc[-1] if not rolling_correlation.empty else 0,\n",
        "            'relative_strength': relative_strength.iloc[-1] if not relative_strength.empty else 0,\n",
        "            'momentum': momentum,\n",
        "            'trend_strength': trend_strength,\n",
        "        }\n",
        "\n",
        "    return summary\n",
        "\n",
        "def calculate_index_correlations(stock_data, index_data):\n",
        "    stock_returns = stock_data['Close'].pct_change().dropna()\n",
        "    index_correlations = {}\n",
        "    for index, data in index_data.items():\n",
        "        index_returns = data['Close'].pct_change().dropna()\n",
        "        aligned_returns = pd.concat([stock_returns, index_returns], axis=1).dropna()\n",
        "        if not aligned_returns.empty:\n",
        "            index_correlations[index] = aligned_returns.iloc[:, 0].corr(aligned_returns.iloc[:, 1])\n",
        "        else:\n",
        "            index_correlations[index] = None\n",
        "    return index_correlations\n",
        "\n",
        "\n",
        "def fit_arima_garch(returns):\n",
        "    # Fit ARIMA model\n",
        "    arima_model = ARIMA(returns, order=(1, 0, 1)).fit()\n",
        "\n",
        "    # Fit GARCH model to the residuals\n",
        "    garch_model = arch_model(arima_model.resid, vol='GARCH', p=1, q=1)\n",
        "    garch_results = garch_model.fit()\n",
        "\n",
        "    return arima_model, garch_results\n",
        "\n",
        "def forecast_arima_garch(arima_model, garch_results, steps=252):\n",
        "    # Forecast ARIMA\n",
        "    arima_forecast = arima_model.forecast(steps=steps)\n",
        "\n",
        "    # Forecast GARCH\n",
        "    garch_forecast = garch_results.forecast(horizon=steps)\n",
        "\n",
        "    return arima_forecast, garch_forecast\n",
        "\n",
        "def fit_var(data, max_lags=5):\n",
        "    model = VAR(data)\n",
        "    results = model.fit(maxlags=max_lags, ic='aic')\n",
        "    return results\n",
        "\n",
        "def forecast_var(var_results, steps=252):\n",
        "    # Get the last known values\n",
        "    last_known_values = var_results.model.endog[-var_results.k_ar:]\n",
        "    return var_results.forecast(last_known_values, steps=steps)\n",
        "\n",
        "def perform_wavelet_analysis(data, wavelet='db1', level=5):\n",
        "    # Perform wavelet decomposition\n",
        "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
        "\n",
        "    # Calculate wavelet variances\n",
        "    variances = [np.var(coeff) for coeff in coeffs]\n",
        "\n",
        "    # Calculate cumulative variance contribution\n",
        "    total_variance = sum(variances)\n",
        "    cumulative_variance = np.cumsum(variances) / total_variance\n",
        "\n",
        "    return coeffs, variances, cumulative_variance\n",
        "\n",
        "def perform_advanced_analysis(hist_data):\n",
        "    close_prices = hist_data['Close'].values\n",
        "    returns = np.diff(np.log(close_prices))\n",
        "\n",
        "    # Nonlinear analysis\n",
        "    forecast = nonlinear_forecast(returns)\n",
        "    surrogate_test = surrogate_data_test(returns)\n",
        "    mfdfa_results = perform_mfdfa(returns)\n",
        "\n",
        "    # Dynamical systems analysis\n",
        "    bifurcation_points = detect_bifurcations(close_prices)\n",
        "    trend_stability = assess_trend_stability(close_prices)\n",
        "    market_regimes = identify_market_regimes(close_prices)\n",
        "    sp500 = yf.Ticker('^GSPC').history(start=hist_data.index[0], end=hist_data.index[-1])['Close'].values\n",
        "    synchronization = calculate_synchronization(close_prices, sp500)\n",
        "\n",
        "    # Chaos analysis\n",
        "    fractal_dim = calculate_fractal_dimension(returns)\n",
        "    lyapunov_exp = estimate_lyapunov_exponent(returns)\n",
        "    correlation_dim = calculate_correlation_dimension(returns.reshape(-1, 1))\n",
        "    information_dim = calculate_information_dimension(returns.reshape(-1, 1))\n",
        "    ks_entropy = calculate_kolmogorov_sinai_entropy(returns)\n",
        "\n",
        "    # Information theory metrics\n",
        "    te = calculate_transfer_entropy(returns)\n",
        "    mi = calculate_mutual_information(close_prices)\n",
        "    pe = calculate_permutation_entropy(returns)\n",
        "    pe_ce, ce = calculate_complexity_entropy(returns)\n",
        "\n",
        "    # ARIMA-GARCH analysis\n",
        "    arima_garch_analysis = {}\n",
        "    try:\n",
        "        # Use price levels instead of returns\n",
        "        prices = pd.Series(close_prices)\n",
        "\n",
        "        # Determine if differencing is needed\n",
        "        if adfuller(prices)[1] > 0.05:  # If p-value > 0.05, the series is non-stationary\n",
        "            prices = prices.diff().dropna()  # Take first difference\n",
        "\n",
        "        # Fit ARIMA model\n",
        "        arima_model = auto_arima(prices, start_p=1, start_q=1, max_p=5, max_q=5,\n",
        "                                 d=None, max_d=2, seasonal=False, trace=False,\n",
        "                                 error_action='ignore', suppress_warnings=True,\n",
        "                                 stepwise=True, random_state=20, n_fits=50)\n",
        "\n",
        "        # Forecast prices\n",
        "        forecast_steps = 252 * 5  # 5 years of daily data\n",
        "        arima_forecast = arima_model.predict(n_periods=forecast_steps)\n",
        "\n",
        "        # Fit GARCH model to the residuals\n",
        "        residuals = arima_model.resid()\n",
        "\n",
        "        # Rescale residuals to address the warning\n",
        "        scaled_residuals = residuals * 10\n",
        "\n",
        "        # Suppress the specific warning\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=DataScaleWarning)\n",
        "\n",
        "            garch_model = arch_model(scaled_residuals, vol='GARCH', p=1, q=1, rescale=False)\n",
        "            garch_results = garch_model.fit(disp='off')\n",
        "\n",
        "        # Forecast volatility\n",
        "        garch_forecast = garch_results.forecast(horizon=forecast_steps)\n",
        "\n",
        "        # Extract mean and variance forecasts\n",
        "        if isinstance(garch_forecast.mean, pd.DataFrame):\n",
        "            garch_forecast_mean = garch_forecast.mean.iloc[-1].values\n",
        "        elif isinstance(garch_forecast.mean, pd.Series):\n",
        "            garch_forecast_mean = garch_forecast.mean.values\n",
        "        else:\n",
        "            garch_forecast_mean = garch_forecast.mean\n",
        "\n",
        "        if isinstance(garch_forecast.variance, pd.DataFrame):\n",
        "            garch_forecast_variance = garch_forecast.variance.iloc[-1].values\n",
        "        elif isinstance(garch_forecast.variance, pd.Series):\n",
        "            garch_forecast_variance = garch_forecast.variance.values\n",
        "        else:\n",
        "            garch_forecast_variance = garch_forecast.variance\n",
        "\n",
        "        # Ensure forecasts are the correct length\n",
        "        garch_forecast_mean = garch_forecast_mean[:forecast_steps]\n",
        "        garch_forecast_variance = garch_forecast_variance[:forecast_steps]\n",
        "\n",
        "        # Rescale the forecasts back to the original scale\n",
        "        garch_forecast_mean /= 10\n",
        "        garch_forecast_variance /= 100  # Variance scales with the square of the scaling factor\n",
        "\n",
        "        arima_garch_analysis = {\n",
        "            'arima_summary': str(arima_model.summary()),\n",
        "            'garch_summary': str(garch_results.summary()),\n",
        "            'arima_forecast': arima_forecast.tolist(),\n",
        "            'garch_forecast_mean': garch_forecast_mean.tolist(),\n",
        "            'garch_forecast_variance': garch_forecast_variance.tolist(),\n",
        "            'forecast_horizon': f\"{forecast_steps} days ({forecast_steps/252:.1f} years)\",\n",
        "            'last_price': close_prices[-1],\n",
        "            'forecast_price_range': (\n",
        "                float(arima_forecast.min()),\n",
        "                float(arima_forecast.max())\n",
        "            ),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in ARIMA-GARCH analysis: {str(e)}\")\n",
        "        arima_garch_analysis = {'error': str(e)}\n",
        "\n",
        "    # VAR analysis\n",
        "    var_data = pd.DataFrame({'returns': returns, 'volume': hist_data['Volume'].pct_change().dropna()})\n",
        "    var_results = fit_var(var_data)\n",
        "    var_forecast = forecast_var(var_results)\n",
        "    var_analysis = {\n",
        "        'var_forecast': var_forecast.tolist(),\n",
        "        'var_summary': str(var_results.summary())\n",
        "    }\n",
        "\n",
        "    # Wavelet analysis\n",
        "    wavelet_coeffs, wavelet_variances, cumulative_variance = perform_wavelet_analysis(returns)\n",
        "    wavelet_analysis = {\n",
        "        'wavelet_coefficients': [coeff.tolist() if hasattr(coeff, 'tolist') else coeff for coeff in wavelet_coeffs],\n",
        "        'wavelet_variances': wavelet_variances if isinstance(wavelet_variances, list) else wavelet_variances.tolist(),\n",
        "        'cumulative_variance': cumulative_variance if isinstance(cumulative_variance, list) else cumulative_variance.tolist()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'nonlinear_analysis': {\n",
        "            'nonlinear_forecast': float(forecast),\n",
        "            'surrogate_test': float(surrogate_test),\n",
        "            'mfdfa_results': {\n",
        "                'hurst_exponents': [float(h) for h in mfdfa_results['hurst_exponents']],\n",
        "                'multifractal_spectrum': [\n",
        "                    [float(a) for a in mfdfa_results['multifractal_spectrum'][0]],\n",
        "                    [float(f) for f in mfdfa_results['multifractal_spectrum'][1]]\n",
        "                ],\n",
        "                'fluctuations': [[float(f) for f in row] for row in mfdfa_results['fluctuations']],\n",
        "                'scales': [int(s) for s in mfdfa_results['scales']],\n",
        "                'q_values': [float(q) for q in mfdfa_results['q_values']]\n",
        "            }\n",
        "        },\n",
        "        'dynamical_systems_analysis': {\n",
        "            'bifurcation_points': bifurcation_points,\n",
        "            'trend_stability': trend_stability,\n",
        "            'market_regimes': market_regimes,\n",
        "            'synchronization': synchronization\n",
        "        },\n",
        "        'chaos_analysis': {\n",
        "            'fractal_dimension': fractal_dim,\n",
        "            'lyapunov_exponent': lyapunov_exp,\n",
        "            'correlation_dimension': correlation_dim,\n",
        "            'information_dimension': information_dim,\n",
        "            'kolmogorov_sinai_entropy': ks_entropy\n",
        "        },\n",
        "        'information_theory_metrics': {\n",
        "            'transfer_entropy': te,\n",
        "            'mutual_information': mi,\n",
        "            'permutation_entropy': pe,\n",
        "            'complexity_entropy': ce\n",
        "        },\n",
        "        'arima_garch_analysis': arima_garch_analysis,\n",
        "        'var_analysis': var_analysis,\n",
        "        'wavelet_analysis': wavelet_analysis\n",
        "    }\n",
        "\n",
        "def discretize_data(data, n_bins=10):\n",
        "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform', subsample=200000)\n",
        "    return discretizer.fit_transform(data.reshape(-1, 1)).flatten()\n",
        "\n",
        "def nonlinear_forecast(data, embedding_dim=3, future_steps=1):\n",
        "    # Simple nonlinear forecasting using nearest neighbors\n",
        "    X = np.array([data[i:i+embedding_dim] for i in range(len(data)-embedding_dim-future_steps+1)])\n",
        "    y = data[embedding_dim+future_steps-1:]\n",
        "\n",
        "    model = NearestNeighbors(n_neighbors=embedding_dim+1, metric='euclidean')\n",
        "    model.fit(X)\n",
        "\n",
        "    last_known = data[-embedding_dim:]\n",
        "    _, indices = model.kneighbors([last_known])\n",
        "\n",
        "    return np.mean(y[indices[0]])\n",
        "\n",
        "def surrogate_data_test(data, num_surrogates=100):\n",
        "    original_stat = np.std(data)\n",
        "    surrogate_stats = []\n",
        "\n",
        "    for _ in range(num_surrogates):\n",
        "        surrogate = np.random.permutation(data)\n",
        "        surrogate_stats.append(np.std(surrogate))\n",
        "\n",
        "    p_value = np.sum(surrogate_stats >= original_stat) / num_surrogates\n",
        "    return p_value\n",
        "\n",
        "def perform_mfdfa(data, q_range=(-5, 5), scale_range=(16, 1024), num_scales=50):\n",
        "    # Ensure data is a numpy array\n",
        "    data = np.array(data)\n",
        "\n",
        "    # Calculate the profile (cumulative sum)\n",
        "    profile = np.cumsum(data - np.mean(data))\n",
        "\n",
        "    # Generate logarithmically spaced scales\n",
        "    scales = np.logspace(np.log10(scale_range[0]), np.log10(scale_range[1]), num=num_scales).astype(int)\n",
        "\n",
        "    # Generate q values\n",
        "    q_values = np.linspace(q_range[0], q_range[1], num=20)\n",
        "\n",
        "    # Initialize fluctuation function\n",
        "    fluctuations = np.zeros((len(scales), len(q_values)))\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        # Calculate the number of segments\n",
        "        n_segments = len(profile) // scale\n",
        "\n",
        "        # Check if n_segments is zero\n",
        "        if n_segments == 0:\n",
        "            fluctuations[i, :] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Reshape the profile into segments\n",
        "        segments = profile[:n_segments*scale].reshape((n_segments, scale))\n",
        "\n",
        "        # Calculate local trends using polynomial fitting\n",
        "        x = np.arange(scale)\n",
        "        local_trends = np.array([np.polyval(np.polyfit(x, segment, deg=1), x) for segment in segments])\n",
        "\n",
        "        # Calculate local fluctuations\n",
        "        local_fluct = np.sqrt(np.mean((segments - local_trends)**2, axis=1))\n",
        "\n",
        "        # Calculate q-order fluctuation function\n",
        "        for j, q in enumerate(q_values):\n",
        "            if q == 0:\n",
        "                fluctuations[i, j] = np.exp(0.5 * np.mean(np.log(local_fluct**2)))\n",
        "            else:\n",
        "                fluctuations[i, j] = np.mean(local_fluct**q)**(1/q)\n",
        "\n",
        "    # Remove any rows with NaN values\n",
        "    valid_rows = ~np.isnan(fluctuations).any(axis=1)\n",
        "    fluctuations = fluctuations[valid_rows]\n",
        "    scales = scales[valid_rows]\n",
        "\n",
        "    # Calculate Hurst exponents and multifractal spectrum\n",
        "    hurst_exponents = np.zeros(len(q_values))\n",
        "    for j in range(len(q_values)):\n",
        "        hurst_exponents[j] = np.polyfit(np.log(scales), np.log(fluctuations[:, j]), 1)[0]\n",
        "\n",
        "    # Calculate multifractal spectrum\n",
        "    tau = q_values * hurst_exponents - 1\n",
        "    alpha = np.diff(tau) / np.diff(q_values)\n",
        "    f_alpha = q_values[:-1] * alpha - tau[:-1]\n",
        "\n",
        "    return {\n",
        "        'hurst_exponents': hurst_exponents,\n",
        "        'multifractal_spectrum': (alpha, f_alpha),\n",
        "        'fluctuations': fluctuations,\n",
        "        'scales': scales,\n",
        "        'q_values': q_values\n",
        "    }\n",
        "\n",
        "def detect_bifurcations(price_data, window_size=50):\n",
        "    \"\"\"\n",
        "    Detect potential bifurcations or regime shifts in price dynamics.\n",
        "    \"\"\"\n",
        "    returns = np.diff(np.log(price_data))\n",
        "    volatility = np.array([np.std(returns[i:i+window_size]) for i in range(len(returns)-window_size+1)])\n",
        "\n",
        "    # Detect significant changes in volatility\n",
        "    peaks, _ = find_peaks(volatility, height=np.mean(volatility) + 2*np.std(volatility))\n",
        "\n",
        "    bifurcation_points = [window_size + peak for peak in peaks]\n",
        "    return bifurcation_points\n",
        "\n",
        "def assess_trend_stability(price_data, window_size=50):\n",
        "    \"\"\"\n",
        "    Assess the stability of price trends using rolling ADF tests.\n",
        "    \"\"\"\n",
        "    adf_stats = []\n",
        "    for i in range(len(price_data) - window_size + 1):\n",
        "        window = price_data[i:i+window_size]\n",
        "        adf_result = adfuller(window)[0]\n",
        "        adf_stats.append(adf_result)\n",
        "\n",
        "    stability_score = np.mean(adf_stats)\n",
        "    return stability_score\n",
        "\n",
        "def calculate_synchronization(price_data1, price_data2):\n",
        "    \"\"\"\n",
        "    Calculate synchronization between two price series using Pearson correlation.\n",
        "    \"\"\"\n",
        "    # Ensure both price series have the same length\n",
        "    min_length = min(len(price_data1), len(price_data2))\n",
        "    price_data1 = price_data1[-min_length:]\n",
        "    price_data2 = price_data2[-min_length:]\n",
        "\n",
        "    returns1 = np.diff(np.log(price_data1))\n",
        "    returns2 = np.diff(np.log(price_data2))\n",
        "\n",
        "    correlation, _ = pearsonr(returns1, returns2)\n",
        "    return correlation\n",
        "\n",
        "def identify_market_regimes(price_data, n_regimes=3):\n",
        "    \"\"\"\n",
        "    Identify market regimes using K-means clustering on returns and volatility.\n",
        "    \"\"\"\n",
        "    returns = np.diff(np.log(price_data))\n",
        "    volatility = np.array([np.std(returns[i:i+20]) for i in range(len(returns)-19)])\n",
        "\n",
        "    features = np.column_stack((returns[19:], volatility))\n",
        "    kmeans = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)  # Explicitly set n_init\n",
        "    regimes = kmeans.fit_predict(features)\n",
        "\n",
        "    return regimes\n",
        "\n",
        "def calculate_transfer_entropy(data, lag=1, k=1, n_bins=10):\n",
        "    if len(data) <= lag + k:\n",
        "        return 0  # Not enough data points\n",
        "\n",
        "    # Ensure data is numeric and finite\n",
        "    data = np.array(data)\n",
        "    if not np.isfinite(data).all():\n",
        "        return 0  # Return 0 if data contains non-finite values\n",
        "\n",
        "    # Discretize the data\n",
        "    bins = np.linspace(data.min(), data.max(), n_bins + 1)\n",
        "    symbolic_data = np.digitize(data, bins) - 1\n",
        "\n",
        "    # Prepare lagged data\n",
        "    x = symbolic_data[k:len(data)-lag]\n",
        "    y = symbolic_data[k+lag:]\n",
        "    z = symbolic_data[:len(data)-lag-k]\n",
        "\n",
        "    # Calculate joint probabilities\n",
        "    p_xyz, _ = np.histogramdd(np.column_stack((x, y, z)), bins=n_bins, range=[[0, n_bins-1]]*3)\n",
        "    p_xz, _ = np.histogramdd(np.column_stack((x, z)), bins=n_bins, range=[[0, n_bins-1]]*2)\n",
        "    p_yz, _ = np.histogramdd(np.column_stack((y, z)), bins=n_bins, range=[[0, n_bins-1]]*2)\n",
        "    p_z, _ = np.histogram(z, bins=n_bins, range=[0, n_bins-1])\n",
        "\n",
        "    # Normalize probabilities\n",
        "    p_xyz = p_xyz / np.sum(p_xyz)\n",
        "    p_xz = p_xz / np.sum(p_xz)\n",
        "    p_yz = p_yz / np.sum(p_yz)\n",
        "    p_z = p_z / np.sum(p_z)\n",
        "\n",
        "    # Calculate transfer entropy\n",
        "    te = 0\n",
        "    for i in range(n_bins):\n",
        "        for j in range(n_bins):\n",
        "            for k in range(n_bins):\n",
        "                if p_xyz[i,j,k] > 0 and p_z[k] > 0 and p_yz[j,k] > 0 and p_xz[i,k] > 0:\n",
        "                    te += p_xyz[i,j,k] * np.log2(p_xyz[i,j,k] * p_z[k] / (p_yz[j,k] * p_xz[i,k]))\n",
        "\n",
        "    return max(0, te)  # Ensure non-negative value\n",
        "\n",
        "def calculate_mutual_information(data, lag=1, n_bins=10):\n",
        "    if len(data) <= lag:\n",
        "        return 0  # Not enough data points\n",
        "\n",
        "    # Discretize the data\n",
        "    bins = np.linspace(data.min(), data.max(), n_bins + 1)\n",
        "    symbolic_data = np.digitize(data, bins) - 1\n",
        "\n",
        "    # Prepare lagged data\n",
        "    x = symbolic_data[:-lag]\n",
        "    y = symbolic_data[lag:]\n",
        "\n",
        "    # Calculate probabilities\n",
        "    p_xy = np.histogram2d(x, y, bins=n_bins, range=[[0, n_bins-1], [0, n_bins-1]])[0]\n",
        "    p_x = p_xy.sum(axis=1)\n",
        "    p_y = p_xy.sum(axis=0)\n",
        "\n",
        "    # Add small constant to avoid divide by zero\n",
        "    epsilon = 1e-10\n",
        "    p_xy = (p_xy + epsilon) / (p_xy.sum() + epsilon * p_xy.size)\n",
        "    p_x = (p_x + epsilon) / (p_x.sum() + epsilon * p_x.size)\n",
        "    p_y = (p_y + epsilon) / (p_y.sum() + epsilon * p_y.size)\n",
        "\n",
        "    # Calculate mutual information\n",
        "    mi = np.sum(p_xy * np.log2(p_xy / (p_x[:, np.newaxis] * p_y[np.newaxis, :])))\n",
        "\n",
        "    return max(0, mi)  # Ensure non-negative value\n",
        "\n",
        "def calculate_permutation_entropy(data, order=3, delay=1):\n",
        "    n = len(data)\n",
        "    permutations = np.array(list(itertools.permutations(range(order))))\n",
        "    c = [0] * len(permutations)\n",
        "\n",
        "    for i in range(n - delay * (order - 1)):\n",
        "        # Extract a sub-sequence\n",
        "        sub_sequence = data[i:i + delay * order:delay]\n",
        "        # Find the permutation of the sub-sequence\n",
        "        sort_index = np.argsort(sub_sequence)\n",
        "        permutation = permutations[np.all(permutations == sort_index, axis=1)][0]\n",
        "        c[np.where((permutations == permutation).all(axis=1))[0][0]] += 1\n",
        "\n",
        "    c = np.array(c) / float(sum(c))\n",
        "    return -sum(p * np.log2(p) for p in c if p != 0)\n",
        "\n",
        "def calculate_complexity_entropy(data, order=3, delay=1):\n",
        "    pe = calculate_permutation_entropy(data, order, delay)\n",
        "\n",
        "    # Calculate the disequilibrium\n",
        "    n = len(data)\n",
        "    permutations = np.array(list(itertools.permutations(range(order))))\n",
        "    c = [0] * len(permutations)\n",
        "\n",
        "    for i in range(n - delay * (order - 1)):\n",
        "        sub_sequence = data[i:i + delay * order:delay]\n",
        "        sort_index = np.argsort(sub_sequence)\n",
        "        permutation = permutations[np.all(permutations == sort_index, axis=1)][0]\n",
        "        c[np.where((permutations == permutation).all(axis=1))[0][0]] += 1\n",
        "\n",
        "    c = np.array(c) / float(sum(c))\n",
        "    q = np.sum((c - 1/len(c))**2)\n",
        "\n",
        "    # Calculate the statistical complexity\n",
        "    c = q * pe\n",
        "\n",
        "    return pe, c\n",
        "\n",
        "def calculate_fractal_dimension(time_series, max_lag=100):\n",
        "    \"\"\"\n",
        "    Calculate the fractal dimension of a time series using the Higuchi method.\n",
        "    \"\"\"\n",
        "    N = len(time_series)\n",
        "    L = np.zeros((max_lag,))\n",
        "    x = np.arange(1, max_lag + 1)\n",
        "\n",
        "    for k in range(1, max_lag + 1):\n",
        "        Lk = np.zeros((k,))\n",
        "        for m in range(k):\n",
        "            indices = np.arange(1, int((N-m)/k))\n",
        "            Lmk = np.sum(np.abs(time_series[m+indices*k] - time_series[m+(indices-1)*k]))\n",
        "            Lmk = (Lmk * (N - 1) / (((N - m) / k) * k)) / k\n",
        "            Lk[m] = Lmk\n",
        "        L[k-1] = np.log(np.mean(Lk))\n",
        "\n",
        "    slope, _, _, _, _ = linregress(np.log(x), L)\n",
        "    return -slope\n",
        "\n",
        "def estimate_lyapunov_exponent(time_series, embedding_dimension=3, lag=1, iterations=1000):\n",
        "    \"\"\"\n",
        "    Estimate the largest Lyapunov exponent of a time series.\n",
        "    \"\"\"\n",
        "    N = len(time_series)\n",
        "    M = N - (embedding_dimension - 1) * lag\n",
        "\n",
        "    Y = np.zeros((M, embedding_dimension))\n",
        "    for i in range(embedding_dimension):\n",
        "        Y[:, i] = time_series[i*lag:i*lag+M]\n",
        "\n",
        "    distances = np.zeros((M, M))\n",
        "    for i in range(M):\n",
        "        for j in range(i+1, M):\n",
        "            distances[i, j] = np.linalg.norm(Y[i] - Y[j])\n",
        "            distances[j, i] = distances[i, j]\n",
        "\n",
        "    lyap = 0\n",
        "    for i in range(iterations):\n",
        "        if i + 1 >= M:\n",
        "            break\n",
        "        neighbor = np.argmin(distances[i, i+1:]) + i + 1\n",
        "        if neighbor + 1 >= M:\n",
        "            break\n",
        "        d0 = distances[i, neighbor]\n",
        "        d1 = np.linalg.norm(Y[i+1] - Y[neighbor+1])\n",
        "        if d0 != 0:\n",
        "            lyap += np.log(d1 / d0)\n",
        "\n",
        "    if iterations > 0:\n",
        "        lyap /= iterations\n",
        "    if len(time_series) > 1:\n",
        "        lyap /= (time_series[1] - time_series[0])\n",
        "\n",
        "    return lyap\n",
        "\n",
        "def calculate_correlation_dimension(data, max_dim=10):\n",
        "    distances = pdist(data)\n",
        "    dist_matrix = squareform(distances)\n",
        "    r_values = np.logspace(-3, 1, 20)\n",
        "    corr_int = np.zeros(len(r_values))\n",
        "\n",
        "    for i, r in enumerate(r_values):\n",
        "        corr_int[i] = np.sum(distances < r) / (len(data) * (len(data) - 1))\n",
        "\n",
        "    log_r = np.log(r_values)\n",
        "    log_c = np.log(corr_int)\n",
        "\n",
        "    slope, _ = np.polyfit(log_r, log_c, 1)\n",
        "    return slope\n",
        "\n",
        "def calculate_information_dimension(data, k=2):\n",
        "    nbrs = NearestNeighbors(n_neighbors=k).fit(data)\n",
        "    distances, _ = nbrs.kneighbors(data)\n",
        "    # Add a small epsilon to avoid log(0)\n",
        "    epsilon = 1e-10\n",
        "    return -np.mean(np.log(distances[:, 1] + epsilon))\n",
        "\n",
        "def calculate_kolmogorov_sinai_entropy(data, tau=1, dim=3):\n",
        "    N = len(data)\n",
        "    Y = np.zeros((N - (dim - 1) * tau, dim))\n",
        "    for i in range(dim):\n",
        "        Y[:, i] = data[i * tau: i * tau + N - (dim - 1) * tau]\n",
        "\n",
        "    distances = pdist(Y)\n",
        "    epsilon = np.median(distances)\n",
        "\n",
        "    count = np.sum(squareform(distances) < epsilon, axis=1)\n",
        "    prob = count / (N - (dim - 1) * tau)\n",
        "    entropy = -np.sum(prob * np.log2(prob + 1e-10)) / (N - (dim - 1) * tau)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def perform_chaos_analysis(hist_data):\n",
        "    close_prices = hist_data['Close'].values\n",
        "    returns = np.diff(np.log(close_prices))\n",
        "\n",
        "    fractal_dim = calculate_fractal_dimension(returns)\n",
        "    lyapunov_exp = estimate_lyapunov_exponent(returns)\n",
        "    correlation_dim = calculate_correlation_dimension(returns.reshape(-1, 1))\n",
        "    information_dim = calculate_information_dimension(returns.reshape(-1, 1))\n",
        "    ks_entropy = calculate_kolmogorov_sinai_entropy(returns)\n",
        "\n",
        "    return {\n",
        "        'fractal_dimension': fractal_dim,\n",
        "        'lyapunov_exponent': lyapunov_exp,\n",
        "        'correlation_dimension': correlation_dim,\n",
        "        'information_dimension': information_dim,\n",
        "        'kolmogorov_sinai_entropy': ks_entropy\n",
        "    }\n",
        "\n",
        "def summarize_advanced_analysis(advanced_analysis_results):\n",
        "    summary = {}\n",
        "\n",
        "    # Summarize nonlinear analysis\n",
        "    nonlinear = advanced_analysis_results['nonlinear_analysis']\n",
        "    summary['nonlinear_analysis'] = {\n",
        "        'nonlinear_forecast': nonlinear['nonlinear_forecast'],\n",
        "        'surrogate_test': nonlinear['surrogate_test'],\n",
        "        'hurst_exponent_range': (min(nonlinear['mfdfa_results']['hurst_exponents']),\n",
        "                                 max(nonlinear['mfdfa_results']['hurst_exponents'])),\n",
        "        'explanation': \"\"\"\n",
        "        Nonlinear forecast: Predicts future values based on nonlinear patterns in the data.\n",
        "        Surrogate test: Measures the likelihood that the observed nonlinearity is due to chance.\n",
        "        Hurst exponent: Indicates the long-term memory of the time series (0.5 = random walk, >0.5 = trend-following, <0.5 = mean-reverting).\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize dynamical systems analysis\n",
        "    dynamical = advanced_analysis_results['dynamical_systems_analysis']\n",
        "    summary['dynamical_systems_analysis'] = {\n",
        "        'bifurcation_points': len(dynamical['bifurcation_points']),\n",
        "        'trend_stability': dynamical['trend_stability'],\n",
        "        'market_regimes': len(set(dynamical['market_regimes'])),\n",
        "        'synchronization': dynamical['synchronization'],\n",
        "        'explanation': \"\"\"\n",
        "        Bifurcation points: Indicate potential regime shifts in the price dynamics.\n",
        "        Trend stability: Measures the consistency of price trends over time.\n",
        "        Market regimes: Identifies distinct states or behaviors in the market.\n",
        "        Synchronization: Measures the correlation between the stock and the broader market.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize chaos analysis\n",
        "    chaos = advanced_analysis_results['chaos_analysis']\n",
        "    summary['chaos_analysis'] = {\n",
        "        'fractal_dimension': chaos['fractal_dimension'],\n",
        "        'lyapunov_exponent': chaos['lyapunov_exponent'],\n",
        "        'correlation_dimension': chaos['correlation_dimension'],\n",
        "        'explanation': \"\"\"\n",
        "        Fractal dimension: Measures the complexity of the price time series.\n",
        "        Lyapunov exponent: Quantifies the sensitivity to initial conditions (chaos).\n",
        "        Correlation dimension: Estimates the underlying dimensionality of the system.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize information theory metrics\n",
        "    info_theory = advanced_analysis_results['information_theory_metrics']\n",
        "    summary['information_theory_metrics'] = {\n",
        "        'transfer_entropy': info_theory['transfer_entropy'],\n",
        "        'mutual_information': info_theory['mutual_information'],\n",
        "        'permutation_entropy': info_theory['permutation_entropy'],\n",
        "        'explanation': \"\"\"\n",
        "        Transfer entropy: Measures the directed transfer of information between time series.\n",
        "        Mutual information: Quantifies the mutual dependence between variables.\n",
        "        Permutation entropy: Measures the complexity of the time series.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize ARIMA-GARCH analysis\n",
        "    arima_garch = advanced_analysis_results['arima_garch_analysis']\n",
        "    summary['arima_garch_analysis'] = {\n",
        "        'arima_forecast_mean': np.mean(arima_garch.get('arima_forecast', [])) if 'arima_forecast' in arima_garch else 'N/A',\n",
        "        'garch_forecast_mean': np.mean(arima_garch.get('garch_forecast_mean', [])) if 'garch_forecast_mean' in arima_garch else 'N/A',\n",
        "        'garch_forecast_variance_mean': np.mean(arima_garch.get('garch_forecast_variance', [])) if 'garch_forecast_variance' in arima_garch else 'N/A',\n",
        "        'explanation': \"\"\"\n",
        "        ARIMA: Forecasts future values based on past observations and errors.\n",
        "        GARCH: Models volatility clustering in financial time series.\n",
        "        Forecast mean: Average predicted value for future periods.\n",
        "        Forecast variance: Expected variability of future values.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize VAR analysis\n",
        "    var = advanced_analysis_results['var_analysis']\n",
        "    summary['var_analysis'] = {\n",
        "        'var_forecast_mean': np.mean(var['var_forecast']),\n",
        "        'explanation': \"\"\"\n",
        "        VAR (Vector Autoregression): Models the dynamic relationships between multiple time series.\n",
        "        Forecast mean: Average predicted values for future periods across all variables.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Summarize wavelet analysis\n",
        "    wavelet = advanced_analysis_results['wavelet_analysis']\n",
        "    summary['wavelet_analysis'] = {\n",
        "        'wavelet_variances': wavelet['wavelet_variances'],\n",
        "        'cumulative_variance': wavelet['cumulative_variance'][-1],\n",
        "        'explanation': \"\"\"\n",
        "        Wavelet analysis decomposes the time series into different frequency components.\n",
        "        Variances: Measure the contribution of each frequency component to overall variability.\n",
        "        Cumulative variance: Proportion of total variance explained by all components.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    return summary\n",
        "\n",
        "import ssl\n",
        "import certifi\n",
        "\n",
        "def get_earning_call_transcripts(ticker, api_key, months=24):\n",
        "    base_url = \"https://financialmodelingprep.com/api/v4/batch_earning_call_transcript/\"\n",
        "\n",
        "    # Calculate the start date (1 year ago from today)\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=months * 30)  # Approximate months to days\n",
        "\n",
        "    all_transcripts = []\n",
        "\n",
        "    # Create a custom SSL context\n",
        "    ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
        "\n",
        "    # We'll fetch transcripts for the current year and the previous year to ensure we cover the full 12-month period\n",
        "    for year in range(start_date.year, end_date.year + 1):\n",
        "        url = f\"{base_url}{ticker}?year={year}&apikey={api_key}\"\n",
        "\n",
        "        try:\n",
        "            with urlopen(url, context=ssl_context) as response:\n",
        "                data = response.read().decode(\"utf-8\")\n",
        "            year_transcripts = json.loads(data)\n",
        "\n",
        "            if isinstance(year_transcripts, list) and year_transcripts:\n",
        "                # Filter transcripts to include only those within the last 12 months\n",
        "                filtered_transcripts = []\n",
        "                for t in year_transcripts:\n",
        "                    try:\n",
        "                        # Try parsing the date, ignoring the time part if present\n",
        "                        transcript_date = datetime.strptime(t['date'].split()[0], '%Y-%m-%d')\n",
        "                        if transcript_date >= start_date:\n",
        "                            filtered_transcripts.append(t)\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error parsing date for transcript: {t['date']}. Error: {str(e)}\")\n",
        "\n",
        "                all_transcripts.extend(filtered_transcripts)\n",
        "            else:\n",
        "                print(f\"No transcripts found for {ticker} in {year}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch transcripts for {ticker} in {year}. Error: {str(e)}\")\n",
        "\n",
        "    # Sort transcripts by date (newest first)\n",
        "    all_transcripts.sort(key=lambda x: x['date'], reverse=True)\n",
        "\n",
        "    return all_transcripts\n",
        "\n",
        "def analyze_transcripts_with_claude(transcripts, ticker):\n",
        "    system_prompt = f\"\"\"You are a seasoned financial analyst with expertise in interpreting earnings call transcripts. Your task is to analyze the provided earnings call transcripts for {ticker} and extract key insights. Please provide specific details from the calls. Focus on the following aspects:\n",
        "\n",
        "    1. Overall sentiment and tone of the calls\n",
        "    2. Key financial metrics and their trends\n",
        "    3. Management's outlook and guidance\n",
        "    4. Major challenges and opportunities discussed\n",
        "    5. Any significant changes in strategy or operations\n",
        "    6. Notable questions from analysts and management's responses\n",
        "    7. Comparison of recent calls to older ones to identify trends or shifts\n",
        "    8. Cite exact quotes from the transcripts where relevant and the date of the call.\n",
        "\n",
        "    Provide a concise summary of your findings for each transcript with specific details, highlighting the most important points that could impact the company's future performance and stock price.\"\"\"\n",
        "\n",
        "    analyses = []\n",
        "\n",
        "    for transcript in transcripts:\n",
        "        transcript_text = f\"Date: {transcript['date']}\\n\\nContent: {transcript['content']}\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Analyze the following earnings call transcript for {ticker}:\\n\\n{transcript_text}\"}\n",
        "        ]\n",
        "\n",
        "        headers = {\n",
        "            \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "            \"anthropic-version\": \"2023-06-01\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"model\": 'claude-3-haiku-20240307',\n",
        "            \"max_tokens\": 4000,\n",
        "            \"temperature\": 0.1,\n",
        "            \"system\": system_prompt,\n",
        "            \"messages\": messages,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json()\n",
        "\n",
        "            if 'content' in response_json:\n",
        "                analysis = response_json['content'][0]['text']\n",
        "                analyses.append(f\"Transcript Analysis for {transcript['date']}:\\n{analysis}\")\n",
        "            else:\n",
        "                print(f\"Unexpected API response format for {ticker}. Full response: {response_json}\")\n",
        "                analyses.append(f\"Error: Unable to parse API response for transcript dated {transcript['date']}.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error making API request for {ticker}: {str(e)}\")\n",
        "            analyses.append(f\"Error: Unable to perform transcript analysis for {transcript['date']}. Details: {str(e)}\")\n",
        "\n",
        "    return \"\\n\\n\".join(analyses)\n",
        "\n",
        "def get_trend_analysis(ticker, api_key):\n",
        "    base_url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
        "    params = {\n",
        "        \"tickers\": ticker,\n",
        "        \"limit\": 500,\n",
        "        \"apikey\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        news_data = response.json()\n",
        "\n",
        "        if not news_data:\n",
        "            return \"No news data available for trend analysis.\"\n",
        "\n",
        "        news_text = \"\"\n",
        "        for article in news_data:\n",
        "            timestamp = article.get('publishedDate', 'N/A')\n",
        "            news_text += f\"\\n\\n---\\n\\nDate: {timestamp}\\nTitle: {article.get('title', 'N/A')}\\nText: {article.get('text', 'N/A')}\"\n",
        "\n",
        "        system_prompt = f\"\"\"As a seasoned hedge fund manager with a proven track record of generating high returns with low risk, conduct a comprehensive trend analysis for {ticker} based on the provided news articles. Your analysis should focus on the following key areas:\n",
        "\n",
        "        1. Sentiment Trends:\n",
        "           - Analyze the overall sentiment trajectory (positive, negative, or neutral) over time.\n",
        "           - Identify any significant shifts in sentiment and their potential triggers.\n",
        "           - Quantify the sentiment if possible (e.g., percentage of positive vs. negative articles).\n",
        "\n",
        "        2. Key Drivers and Themes:\n",
        "           - Identify the most frequently mentioned topics or themes related to {ticker}.\n",
        "           - Analyze how these themes have evolved over time and their potential impact on the stock.\n",
        "           - Highlight any emerging trends or new focus areas for the company.\n",
        "\n",
        "        3. Company-Specific Developments:\n",
        "           - Identify major company announcements, product launches, or strategic initiatives.\n",
        "           - Analyze the market's reaction to these developments.\n",
        "           - Assess the potential long-term impact of these developments on the company's growth and market position.\n",
        "\n",
        "        4. Industry and Competitive Landscape:\n",
        "           - Analyze mentions of competitors and industry-wide trends.\n",
        "           - Assess {ticker}'s position relative to its competitors based on the news coverage.\n",
        "           - Identify any industry disruptions or regulatory changes that could impact {ticker}.\n",
        "\n",
        "        5. Financial Performance and Projections:\n",
        "           - Analyze mentions of financial results, earnings forecasts, and analyst expectations.\n",
        "           - Identify any trends in revenue growth, profitability, or other key financial metrics.\n",
        "           - Assess the market's reaction to financial news and its alignment with actual performance.\n",
        "\n",
        "        6. Risk Factors and Challenges:\n",
        "           - Identify potential risks or challenges mentioned in the news articles.\n",
        "           - Analyze how these risks have evolved over time and their potential impact on {ticker}.\n",
        "           - Assess the company's response to these challenges, if mentioned.\n",
        "\n",
        "        7. Investor and Analyst Sentiment:\n",
        "           - Analyze mentions of institutional investors, major shareholders, or analyst recommendations.\n",
        "           - Identify any trends in analyst ratings or price targets.\n",
        "           - Assess the overall investor sentiment based on the news coverage.\n",
        "\n",
        "        8. Correlation with Stock Performance:\n",
        "           - Analyze how news sentiment and key events correlate with {ticker}'s stock price movements.\n",
        "           - Identify any patterns or lag effects between news and stock price reactions.\n",
        "\n",
        "        9. Future Outlook and Catalysts:\n",
        "           - Based on the news analysis, provide insights on potential future catalysts for {ticker}.\n",
        "           - Identify any upcoming events or developments that could significantly impact the stock.\n",
        "           - Assess the overall future outlook for {ticker} based on the trends observed in the news.\n",
        "\n",
        "        10. Quantitative Summary:\n",
        "            - Provide a quantitative summary of key metrics, such as sentiment distribution, mention frequency of key themes, and correlation coefficients where applicable.\n",
        "\n",
        "        Synthesize all these elements to provide a comprehensive trend analysis that offers actionable insights for investment decision-making. Your analysis should be data-driven, citing specific examples from the news articles to support your conclusions.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"News articles for {ticker}:\\n{news_text}\\n\\n----\\n\\n Based on this comprehensive analysis from the system prompt, provide insights on how the changes could potentially impact the stock's performance and investor behavior. Highlight any notable trends, shifts, or change over time, and discuss their implications for your investment decisions. identify key sentiment drivers and their potential impact on stock performance. Also, list the most significant catalysts with their potential impact and timing.\"},\n",
        "        ]\n",
        "\n",
        "        headers = {\n",
        "            \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "            \"anthropic-version\": \"2023-06-01\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"model\": 'claude-3-haiku-20240307',\n",
        "            \"max_tokens\": 4000,\n",
        "            \"temperature\": 0.1,\n",
        "            \"system\": system_prompt,\n",
        "            \"messages\": messages,\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "        response_json = response.json()\n",
        "\n",
        "        if 'content' in response_json:\n",
        "            response_text = response_json['content'][0]['text']\n",
        "        else:\n",
        "            print(f\"Unexpected API response format for {ticker}. Full response: {response_json}\")\n",
        "            response_text = \"Error: Unable to parse API response.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request for {ticker}: {str(e)}\")\n",
        "        response_text = \"Error: Unable to perform sentiment analysis.\"\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "def get_analyst_ratings(ticker, stock_data):\n",
        "    api_key = \"pCSuMKRuyV5IXI8Utfl5i7F7bq4TpRCY\"  # Model Prep API key\n",
        "    key_stats = stock_data.get('key_stats', {})\n",
        "\n",
        "    analysis = f\"Analyst Ratings and Recommendations for {ticker}:\\n\\n\"\n",
        "\n",
        "    # Analyst Overview\n",
        "    analysis += \"Analyst Overview:\\n\"\n",
        "    num_analysts = key_stats.get('numberOfAnalystOpinions', 'N/A')\n",
        "    analysis += f\"Number of Analysts: {num_analysts}\\n\"\n",
        "    rec_mean = key_stats.get('recommendationMean', 'N/A')\n",
        "    analysis += f\"Recommendation Mean: {rec_mean} (1=Strong Buy, 5=Strong Sell)\\n\"\n",
        "    rec_key = key_stats.get('recommendationKey', 'N/A')\n",
        "    analysis += f\"Recommendation Key: {rec_key}\\n\\n\"\n",
        "\n",
        "    # Price target analysis from Financial Modeling Prep\n",
        "    url = f\"https://financialmodelingprep.com/api/v4/price-target-consensus?symbol={ticker}&apikey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        fmp_data = response.json()\n",
        "        if fmp_data:\n",
        "            latest_data = fmp_data[0]\n",
        "            target_high = latest_data.get('targetHigh', 'N/A')\n",
        "            target_low = latest_data.get('targetLow', 'N/A')\n",
        "            target_consensus = latest_data.get('targetConsensus', 'N/A')\n",
        "            target_median = latest_data.get('targetMedian', 'N/A')\n",
        "\n",
        "            analysis += f\"Price Targets:\\n\"\n",
        "            analysis += f\"High: ${target_high}\\n\"\n",
        "            analysis += f\"Low: ${target_low}\\n\"\n",
        "            analysis += f\"Consensus: ${target_consensus}\\n\"\n",
        "            analysis += f\"Median: ${target_median}\\n\"\n",
        "\n",
        "            current_price = stock_data.get('hist_data', pd.DataFrame()).get('Close', pd.Series()).iloc[-1] if not stock_data.get('hist_data', pd.DataFrame()).empty else 'N/A'\n",
        "\n",
        "            if isinstance(target_consensus, (int, float)) and isinstance(current_price, (int, float)) and current_price != 0:\n",
        "                implied_return = (target_consensus / current_price - 1) * 100\n",
        "                analysis += f\"Implied Return (based on consensus target): {implied_return:.2f}%\\n\"\n",
        "\n",
        "                if implied_return > 20:\n",
        "                    analysis += \"Analysts expect significant upside potential.\\n\"\n",
        "                elif implied_return < -20:\n",
        "                    analysis += \"Analysts expect significant downside risk.\\n\"\n",
        "                elif -5 <= implied_return <= 5:\n",
        "                    analysis += \"Analysts expect the stock to trade near its current price.\\n\"\n",
        "            else:\n",
        "                analysis += \"Implied Return: Not available\\n\"\n",
        "\n",
        "        else:\n",
        "            analysis += \"No price target data available from Financial Modeling Prep.\\n\"\n",
        "    else:\n",
        "        analysis += f\"Error fetching price target data from Financial Modeling Prep: {response.status_code}\\n\"\n",
        "\n",
        "    analysis += \"\\n\"\n",
        "\n",
        "    # FMP analyst recommendations\n",
        "    url = f\"https://financialmodelingprep.com/api/v3/analyst-stock-recommendations/{ticker}?apikey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        fmp_recommendations = response.json()\n",
        "        if fmp_recommendations:\n",
        "            analysis += \"FMP Analyst Recommendations Summary:\\n\"\n",
        "\n",
        "            # Most recent recommendation\n",
        "            latest_rec = fmp_recommendations[0]\n",
        "            analysis += f\"Latest Recommendation ({latest_rec['date']}):\\n\"\n",
        "            analysis += f\"Strong Buy: {latest_rec['analystRatingsStrongBuy']}, \"\n",
        "            analysis += f\"Buy: {latest_rec['analystRatingsbuy']}, \"\n",
        "            analysis += f\"Hold: {latest_rec['analystRatingsHold']}, \"\n",
        "            analysis += f\"Sell: {latest_rec['analystRatingsSell']}, \"\n",
        "            analysis += f\"Strong Sell: {latest_rec['analystRatingsStrongSell']}\\n\\n\"\n",
        "\n",
        "            # Long-term trend analysis\n",
        "            def get_rec_at_date(date):\n",
        "                return next((rec for rec in fmp_recommendations if rec['date'] <= date), None)\n",
        "\n",
        "            current_date = datetime.now()\n",
        "            one_year_ago = (current_date - pd.DateOffset(years=1)).strftime('%Y-%m-%d')\n",
        "            three_years_ago = (current_date - pd.DateOffset(years=3)).strftime('%Y-%m-%d')\n",
        "            five_years_ago = (current_date - pd.DateOffset(years=5)).strftime('%Y-%m-%d')\n",
        "\n",
        "            rec_1y = get_rec_at_date(one_year_ago)\n",
        "            rec_3y = get_rec_at_date(three_years_ago)\n",
        "            rec_5y = get_rec_at_date(five_years_ago)\n",
        "\n",
        "            analysis += \"Long-term Recommendation Trends:\\n\"\n",
        "\n",
        "            def calculate_buy_percentage(rec):\n",
        "                if rec:\n",
        "                    total = sum(rec[f'analystRatings{rating}'] for rating in ['StrongBuy', 'buy', 'Hold', 'Sell', 'StrongSell'])\n",
        "                    return (rec['analystRatingsStrongBuy'] + rec['analystRatingsbuy']) / total * 100 if total > 0 else 0\n",
        "                return None\n",
        "\n",
        "            current_buy_pct = calculate_buy_percentage(latest_rec)\n",
        "            buy_pct_1y = calculate_buy_percentage(rec_1y)\n",
        "            buy_pct_3y = calculate_buy_percentage(rec_3y)\n",
        "            buy_pct_5y = calculate_buy_percentage(rec_5y)\n",
        "\n",
        "            analysis += f\"Current Buy %: {current_buy_pct:.1f}%\\n\"\n",
        "            if buy_pct_1y is not None:\n",
        "                analysis += f\"1-year change: {current_buy_pct - buy_pct_1y:.1f}%\\n\"\n",
        "            if buy_pct_3y is not None:\n",
        "                analysis += f\"3-year change: {current_buy_pct - buy_pct_3y:.1f}%\\n\"\n",
        "            if buy_pct_5y is not None:\n",
        "                analysis += f\"5-year change: {current_buy_pct - buy_pct_5y:.1f}%\\n\"\n",
        "\n",
        "            analysis += \"\\n\"\n",
        "\n",
        "            # Calculate overall sentiment\n",
        "            total_ratings = sum(latest_rec[f'analystRatings{rating}'] for rating in ['StrongBuy', 'buy', 'Hold', 'Sell', 'StrongSell'])\n",
        "            buy_percentage = (latest_rec['analystRatingsStrongBuy'] + latest_rec['analystRatingsbuy']) / total_ratings * 100 if total_ratings > 0 else 0\n",
        "            hold_percentage = latest_rec['analystRatingsHold'] / total_ratings * 100 if total_ratings > 0 else 0\n",
        "            sell_percentage = (latest_rec['analystRatingsSell'] + latest_rec['analystRatingsStrongSell']) / total_ratings * 100 if total_ratings > 0 else 0\n",
        "\n",
        "            analysis += f\"Current Recommendation Distribution:\\n\"\n",
        "            analysis += f\"Buy: {buy_percentage:.1f}%\\n\"\n",
        "            analysis += f\"Hold: {hold_percentage:.1f}%\\n\"\n",
        "            analysis += f\"Sell: {sell_percentage:.1f}%\\n\"\n",
        "\n",
        "            if buy_percentage > 60:\n",
        "                analysis += \"Overall Sentiment: Strongly Bullish\\n\"\n",
        "            elif buy_percentage > 40:\n",
        "                analysis += \"Overall Sentiment: Bullish\\n\"\n",
        "            elif sell_percentage > 60:\n",
        "                analysis += \"Overall Sentiment: Strongly Bearish\\n\"\n",
        "            elif sell_percentage > 40:\n",
        "                analysis += \"Overall Sentiment: Bearish\\n\"\n",
        "            else:\n",
        "                analysis += \"Overall Sentiment: Neutral\\n\"\n",
        "\n",
        "            # Analyst coverage trend\n",
        "            first_rec = fmp_recommendations[-1]\n",
        "            first_total = sum(first_rec[f'analystRatings{rating}'] for rating in ['StrongBuy', 'buy', 'Hold', 'Sell', 'StrongSell'])\n",
        "            coverage_change = total_ratings - first_total\n",
        "            if coverage_change > 0:\n",
        "                analysis += f\"\\nAnalyst coverage has increased by {coverage_change} since the oldest recommendation.\\n\"\n",
        "            elif coverage_change < 0:\n",
        "                analysis += f\"\\nAnalyst coverage has decreased by {abs(coverage_change)} since the oldest recommendation.\\n\"\n",
        "            else:\n",
        "                analysis += \"\\nAnalyst coverage has remained stable.\\n\"\n",
        "\n",
        "            # Confidence metric\n",
        "            ratings = [latest_rec['analystRatingsStrongBuy'], latest_rec['analystRatingsbuy'],\n",
        "                      latest_rec['analystRatingsHold'], latest_rec['analystRatingsSell'],\n",
        "                      latest_rec['analystRatingsStrongSell']]\n",
        "            total_ratings = sum(ratings)\n",
        "            if total_ratings > 0:\n",
        "                proportions = [r / total_ratings for r in ratings]\n",
        "                max_proportion = max(proportions)\n",
        "                agreement = max_proportion * 100\n",
        "            else:\n",
        "                agreement = 0\n",
        "\n",
        "            analysis += f\"\\nAnalyst Agreement: {agreement:.1f}% (higher percentage indicates stronger consensus)\\n\"\n",
        "\n",
        "        else:\n",
        "            analysis += \"No FMP analyst recommendations available.\\n\"\n",
        "    else:\n",
        "        analysis += f\"Error fetching FMP analyst recommendations: {response.status_code}\\n\"\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def perform_tda(hist_data):\n",
        "    # Prepare the data\n",
        "    price_data = hist_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "    # Compute persistent homology\n",
        "    diagrams = ripser(price_data)['dgms']\n",
        "\n",
        "    # Calculate persistence features\n",
        "    persistence_0 = np.sum(diagrams[0][:, 1] - diagrams[0][:, 0])\n",
        "    if len(diagrams) > 1:\n",
        "        persistence_1 = np.sum(diagrams[1][:, 1] - diagrams[1][:, 0])\n",
        "    else:\n",
        "        persistence_1 = 0\n",
        "\n",
        "    # Calculate topological complexity\n",
        "    complexity = len(diagrams[0]) + (len(diagrams[1]) if len(diagrams) > 1 else 0)\n",
        "\n",
        "    # Calculate Betti numbers\n",
        "    betti_0 = len(diagrams[0])\n",
        "    betti_1 = len(diagrams[1]) if len(diagrams) > 1 else 0\n",
        "\n",
        "    return {\n",
        "        'persistence_0': persistence_0,\n",
        "        'persistence_1': persistence_1,\n",
        "        'topological_complexity': complexity,\n",
        "        'betti_0': betti_0,\n",
        "        'betti_1': betti_1,\n",
        "        'explanation': \"\"\"\n",
        "        Persistence: Measures the lifespan of topological features across different scales.\n",
        "        Topological complexity: Indicates the overall intricacy of the price dynamics.\n",
        "        Betti numbers: Count the number of connected components (0) and loops (1) in the data.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "# calculate Technical Indicators\n",
        "def calculate_fibonacci_retracements(data):\n",
        "    high = data['High'].max()\n",
        "    low = data['Low'].min()\n",
        "    diff = high - low\n",
        "    levels = [0, 0.236, 0.382, 0.5, 0.618, 0.786, 1]\n",
        "    retracements = [(high - (diff * level)) for level in levels]\n",
        "\n",
        "    # Create a DataFrame with the same index as the input data\n",
        "    fib_df = pd.DataFrame(index=data.index)\n",
        "\n",
        "    # Add each Fibonacci level as a column\n",
        "    for level, value in zip(levels, retracements):\n",
        "        fib_df[f'Fib_{level}'] = value\n",
        "\n",
        "    return fib_df\n",
        "\n",
        "def identify_elliott_wave_patterns(data):\n",
        "    close = data['Close']\n",
        "    diff = close.diff()\n",
        "    waves = [0]  # Start with 0 for the first row\n",
        "    current_wave = 0\n",
        "    for i in range(1, len(diff)):\n",
        "        if diff.iloc[i] > 0 and diff.iloc[i-1] <= 0:\n",
        "            current_wave += 1\n",
        "        elif diff.iloc[i] < 0 and diff.iloc[i-1] >= 0:\n",
        "            current_wave += 1\n",
        "        waves.append(current_wave)\n",
        "    return pd.DataFrame({'Elliott_Wave': waves}, index=data.index)\n",
        "\n",
        "def detect_macd_divergence(data):\n",
        "    close = data['Close']\n",
        "    macd_line, signal_line, _ = calculate_macd(close)\n",
        "\n",
        "    divergence = [0] * 14  # Start with 0 for the first 14 rows\n",
        "    for i in range(14, len(close)):\n",
        "        price_trend = close.iloc[i] - close.iloc[i-14]\n",
        "        macd_trend = macd_line.iloc[i] - macd_line.iloc[i-14]\n",
        "        if price_trend > 0 and macd_trend < 0:\n",
        "            divergence.append(-1)  # Bearish divergence\n",
        "        elif price_trend < 0 and macd_trend > 0:\n",
        "            divergence.append(1)  # Bullish divergence\n",
        "        else:\n",
        "            divergence.append(0)  # No divergence\n",
        "\n",
        "    return pd.DataFrame({'MACD_Divergence': divergence}, index=data.index)\n",
        "\n",
        "def calculate_ad_line(data):\n",
        "    clv = ((data['Close'] - data['Low']) - (data['High'] - data['Close'])) / (data['High'] - data['Low'])\n",
        "    ad = clv * data['Volume']\n",
        "    return ad.cumsum()\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    exp1 = prices.ewm(span=fast, adjust=False).mean()\n",
        "    exp2 = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = exp1 - exp2\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_gann_fan(data):\n",
        "    high = data['High'].max()\n",
        "    low = data['Low'].min()\n",
        "    time_range = len(data)\n",
        "\n",
        "    gann_angles = [1, 2, 3, 4, 6, 8]\n",
        "    gann_levels = {}\n",
        "\n",
        "    for angle in gann_angles:\n",
        "        price_change = (high - low) * (angle / 8)\n",
        "        time_change = time_range * (angle / 8)\n",
        "\n",
        "        gann_levels[f'1x{angle}'] = [(low + price_change * (i / time_change)) for i in range(time_range)]\n",
        "        gann_levels[f'-1x{angle}'] = [(high - price_change * (i / time_change)) for i in range(time_range)]\n",
        "\n",
        "    return pd.DataFrame(gann_levels, index=data.index)\n",
        "\n",
        "def calculate_hurst_exponent(time_series, max_lag=100, min_lag=2):\n",
        "    \"\"\"\n",
        "    Calculate the Hurst Exponent using R/S analysis with improved accuracy and robustness.\n",
        "\n",
        "    Parameters:\n",
        "    time_series (array-like): Input time series data\n",
        "    max_lag (int): Maximum lag for R/S analysis\n",
        "    min_lag (int): Minimum lag for R/S analysis\n",
        "\n",
        "    Returns:\n",
        "    float: Hurst exponent\n",
        "    \"\"\"\n",
        "    # Convert to numpy array and ensure sufficient data\n",
        "    time_series = np.array(time_series)\n",
        "    if len(time_series) < max_lag:\n",
        "        max_lag = len(time_series) // 2\n",
        "\n",
        "    # Calculate range of lags\n",
        "    lags = np.floor(np.logspace(np.log10(min_lag), np.log10(max_lag), 20)).astype(int)\n",
        "    lags = np.unique(lags)  # Remove duplicates\n",
        "\n",
        "    # Calculate R/S values for each lag\n",
        "    rs_values = []\n",
        "    for lag in lags:\n",
        "        # Calculate R/S for current lag\n",
        "        rs = []\n",
        "        for start in range(0, len(time_series) - lag, lag):\n",
        "            segment = time_series[start:start + lag]\n",
        "            mean = np.mean(segment)\n",
        "            std = np.std(segment)\n",
        "            if std == 0:  # Avoid division by zero\n",
        "                continue\n",
        "\n",
        "            # Calculate cumulative deviations\n",
        "            cumdev = np.cumsum(segment - mean)\n",
        "\n",
        "            # Calculate R/S ratio\n",
        "            r = np.max(cumdev) - np.min(cumdev)  # Range\n",
        "            s = std if std != 0 else 1e-10  # Scale\n",
        "            rs.append(r/s)\n",
        "\n",
        "        if rs:  # Only append if we have valid R/S values\n",
        "            rs_values.append(np.mean(rs))\n",
        "\n",
        "    # Clean data: remove any invalid values\n",
        "    valid_data = np.logical_and(~np.isnan(rs_values), ~np.isinf(rs_values))\n",
        "    lags = lags[valid_data]\n",
        "    rs_values = np.array(rs_values)[valid_data]\n",
        "\n",
        "    if len(lags) < 2:  # Need at least 2 points for regression\n",
        "        print(\"Warning: Insufficient valid data points for Hurst calculation\")\n",
        "        return np.nan\n",
        "\n",
        "    # Perform linear regression on log-log plot\n",
        "    log_lags = np.log10(lags)\n",
        "    log_rs = np.log10(rs_values)\n",
        "    poly = np.polyfit(log_lags, log_rs, 1)\n",
        "    hurst = poly[0]\n",
        "\n",
        "    return hurst\n",
        "\n",
        "#Peform Technical Analysis\n",
        "def perform_technical_analysis(hist_data):\n",
        "    if hist_data.empty:\n",
        "        print(\"Warning: Empty historical data. Skipping technical analysis.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "    new_columns = []\n",
        "\n",
        "    # Calculate basic technical indicators\n",
        "    new_columns.append(pd.Series(hist_data['Close'].rolling(window=20).mean(), name='SMA_20'))\n",
        "    new_columns.append(pd.Series(hist_data['Close'].rolling(window=50).mean(), name='SMA_50'))\n",
        "    new_columns.append(pd.Series(hist_data['Close'].rolling(window=200).mean(), name='SMA_200'))\n",
        "    new_columns.append(pd.Series(hist_data['Close'].rolling(window=300).mean(), name='SMA_300'))\n",
        "    new_columns.append(pd.Series(hist_data['Close'].rolling(window=500).mean(), name='SMA_500'))\n",
        "\n",
        "    # Calculate MACD with different periods\n",
        "    macd_periods = [\n",
        "        ('MACD', 12, 26, 9),\n",
        "        ('MACD_Long', 26, 52, 18),\n",
        "        ('MACD_VLong', 52, 104, 36),\n",
        "        ('MACD_XVLong', 104, 208, 72)\n",
        "    ]\n",
        "\n",
        "    for name, fast, slow, signal in macd_periods:\n",
        "        macd_indicator = MACD(close=hist_data['Close'], window_slow=slow, window_fast=fast, window_sign=signal)\n",
        "        new_columns.append(pd.Series(macd_indicator.macd(), name=f'{name}'))\n",
        "        new_columns.append(pd.Series(macd_indicator.macd_signal(), name=f'{name}_Signal'))\n",
        "        new_columns.append(pd.Series(macd_indicator.macd_diff(), name=f'{name}_Hist'))\n",
        "\n",
        "    # Add all technical analysis features\n",
        "    ta_features = add_all_ta_features(hist_data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
        "    new_columns.extend([ta_features[col] for col in ta_features.columns if col not in hist_data.columns])\n",
        "\n",
        "    # Calculate Gann Fan levels\n",
        "    gann_fan = calculate_gann_fan(hist_data)\n",
        "    new_columns.extend([gann_fan[col] for col in gann_fan.columns])\n",
        "\n",
        "    # Calculate RSI\n",
        "    rsi_indicator = RSIIndicator(close=hist_data['Close'], window=14)\n",
        "    new_columns.append(pd.Series(rsi_indicator.rsi(), name='RSI'))\n",
        "\n",
        "    # Calculate Hurst Exponent\n",
        "    close_prices = hist_data['Close'].values\n",
        "    hurst_exponent = calculate_hurst_exponent(close_prices, max_lag=len(close_prices)//2)\n",
        "    new_columns.append(pd.Series([hurst_exponent] * len(hist_data), index=hist_data.index, name='Hurst_Exponent'))\n",
        "\n",
        "    # Calculate additional indicators\n",
        "    new_columns.append(pd.Series(calculate_ad_line(hist_data), name='Acc_Dist_Line'))\n",
        "    new_columns.extend([calculate_fibonacci_retracements(hist_data)[col] for col in calculate_fibonacci_retracements(hist_data).columns])\n",
        "    new_columns.extend([identify_elliott_wave_patterns(hist_data)[col] for col in identify_elliott_wave_patterns(hist_data).columns])\n",
        "    new_columns.extend([detect_macd_divergence(hist_data)[col] for col in detect_macd_divergence(hist_data).columns])\n",
        "\n",
        "    # Calculate Ichimoku Cloud\n",
        "    ichimoku = IchimokuIndicator(high=hist_data['High'], low=hist_data['Low'], window1=9, window2=26, window3=52)\n",
        "    new_columns.append(pd.Series(ichimoku.ichimoku_conversion_line(), name='Ichimoku_Conversion_Line'))\n",
        "    new_columns.append(pd.Series(ichimoku.ichimoku_base_line(), name='Ichimoku_Base_Line'))\n",
        "    new_columns.append(pd.Series(ichimoku.ichimoku_a(), name='Ichimoku_Span_A'))\n",
        "    new_columns.append(pd.Series(ichimoku.ichimoku_b(), name='Ichimoku_Span_B'))\n",
        "\n",
        "    # Calculate other indicators\n",
        "    cmf_indicator = ChaikinMoneyFlowIndicator(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], volume=hist_data['Volume'], window=20)\n",
        "    new_columns.append(pd.Series(cmf_indicator.chaikin_money_flow(), name='CMF'))\n",
        "\n",
        "    force_index = ForceIndexIndicator(close=hist_data['Close'], volume=hist_data['Volume'], window=13)\n",
        "    new_columns.append(pd.Series(force_index.force_index(), name='Force_Index'))\n",
        "\n",
        "    mfi_indicator = MFIIndicator(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], volume=hist_data['Volume'], window=14)\n",
        "    new_columns.append(pd.Series(mfi_indicator.money_flow_index(), name='MFI'))\n",
        "\n",
        "    mass_index = MassIndex(high=hist_data['High'], low=hist_data['Low'], window_fast=9, window_slow=25)\n",
        "    new_columns.append(pd.Series(mass_index.mass_index(), name='Mass_Index'))\n",
        "\n",
        "    obv_indicator = OnBalanceVolumeIndicator(close=hist_data['Close'], volume=hist_data['Volume'])\n",
        "    new_columns.append(pd.Series(obv_indicator.on_balance_volume(), name='OBV'))\n",
        "\n",
        "    adx_indicator = ADXIndicator(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], window=14)\n",
        "    new_columns.append(pd.Series(adx_indicator.adx(), name='ADX'))\n",
        "\n",
        "    stoch_indicator = StochasticOscillator(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], window=14, smooth_window=3)\n",
        "    new_columns.append(pd.Series(stoch_indicator.stoch(), name='Stoch_K'))\n",
        "    new_columns.append(pd.Series(stoch_indicator.stoch_signal(), name='Stoch_D'))\n",
        "\n",
        "    williams_r = WilliamsRIndicator(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], lbp=14)\n",
        "    new_columns.append(pd.Series(williams_r.williams_r(), name='Williams_%R'))\n",
        "\n",
        "    bollinger = BollingerBands(close=hist_data['Close'], window=20, window_dev=2)\n",
        "    new_columns.append(pd.Series(bollinger.bollinger_hband(), name='BB_Upper'))\n",
        "    new_columns.append(pd.Series(bollinger.bollinger_lband(), name='BB_Lower'))\n",
        "    new_columns.append(pd.Series(bollinger.bollinger_mavg(), name='BB_MA'))\n",
        "\n",
        "    # Calculate ATR\n",
        "    atr_indicator = AverageTrueRange(high=hist_data['High'], low=hist_data['Low'], close=hist_data['Close'], window=14)\n",
        "    new_columns.append(pd.Series(atr_indicator.average_true_range(), name='ATR'))\n",
        "\n",
        "    # Combine all new features with the original data\n",
        "    result = pd.concat([hist_data] + new_columns, axis=1)\n",
        "\n",
        "    return result\n",
        "\n",
        "def analyze_technical_indicators(hist_data):\n",
        "    if hist_data.empty:\n",
        "        print(\"Warning: Empty historical data. Skipping technical indicator analysis.\")\n",
        "        return \"Insufficient data for technical analysis.\"\n",
        "\n",
        "    latest_data = hist_data.iloc[-1]\n",
        "    prev_data = hist_data.iloc[-2] if len(hist_data) > 1 else latest_data\n",
        "\n",
        "    analysis = \"Technical Analysis:\\n\"\n",
        "\n",
        "    # Analyze Moving Averages\n",
        "    if latest_data['Close'] > latest_data['SMA_500'] > latest_data['SMA_300'] > latest_data['SMA_200'] > latest_data['SMA_50'] > latest_data['SMA_20']:\n",
        "        analysis += \"- Very strong bullish trend: Price above 20-day, 50-day, 200-day, 300-day, and 500-day SMAs\\n\"\n",
        "    elif latest_data['Close'] < latest_data['SMA_500'] < latest_data['SMA_300'] < latest_data['SMA_200'] < latest_data['SMA_50'] < latest_data['SMA_20']:\n",
        "        analysis += \"- Very strong bearish trend: Price below 20-day, 50-day, 200-day, 300-day, and 500-day SMAs\\n\"\n",
        "    elif latest_data['Close'] > latest_data['SMA_200']:\n",
        "        analysis += \"- Long-term bullish trend: Price above 200-day SMA\\n\"\n",
        "    elif latest_data['Close'] < latest_data['SMA_200']:\n",
        "        analysis += \"- Long-term bearish trend: Price below 200-day SMA\\n\"\n",
        "    else:\n",
        "        analysis += \"- Mixed signals from moving averages\\n\"\n",
        "\n",
        "    # Additional analysis for 300-day and 500-day SMAs\n",
        "    if latest_data['Close'] > latest_data['SMA_300']:\n",
        "        analysis += \"- Price above 300-day SMA, indicating strong long-term bullish trend\\n\"\n",
        "    elif latest_data['Close'] < latest_data['SMA_300']:\n",
        "        analysis += \"- Price below 300-day SMA, indicating strong long-term bearish trend\\n\"\n",
        "\n",
        "    if latest_data['Close'] > latest_data['SMA_500']:\n",
        "        analysis += \"- Price above 500-day SMA, indicating very strong long-term bullish trend\\n\"\n",
        "    elif latest_data['Close'] < latest_data['SMA_500']:\n",
        "        analysis += \"- Price below 500-day SMA, indicating very strong long-term bearish trend\\n\"\n",
        "\n",
        "    # Analyze Force Index\n",
        "    latest_force_index = hist_data['Force_Index'].iloc[-1]\n",
        "    prev_force_index = hist_data['Force_Index'].iloc[-2]\n",
        "\n",
        "    if latest_force_index > 0 and prev_force_index <= 0:\n",
        "        analysis += \"- Force Index turned positive: Potential bullish signal\\n\"\n",
        "    elif latest_force_index < 0 and prev_force_index >= 0:\n",
        "        analysis += \"- Force Index turned negative: Potential bearish signal\\n\"\n",
        "    elif latest_force_index > prev_force_index:\n",
        "        analysis += \"- Force Index is increasing: Bullish momentum\\n\"\n",
        "    elif latest_force_index < prev_force_index:\n",
        "        analysis += \"- Force Index is decreasing: Bearish momentum\\n\"\n",
        "    else:\n",
        "        analysis += \"- Force Index is stable\\n\"\n",
        "\n",
        "    # Analyze Hurst Exponent\n",
        "    hurst_exponent = hist_data['Hurst_Exponent'].iloc[-1]\n",
        "    analysis += f\"\\nHurst Exponent: {hurst_exponent:.2f}\\n\"\n",
        "    if hurst_exponent > 0.5:\n",
        "        analysis += \"- Hurst Exponent indicates a trend-following market (persistent behavior)\\n\"\n",
        "    elif hurst_exponent < 0.5:\n",
        "        analysis += \"- Hurst Exponent indicates a mean-reverting market (anti-persistent behavior)\\n\"\n",
        "    else:\n",
        "        analysis += \"- Hurst Exponent indicates a random walk (no clear trend)\\n\"\n",
        "\n",
        "    # Analyze Accumulation/Distribution Line\n",
        "    latest_ad = hist_data['Acc_Dist_Line'].iloc[-1]\n",
        "    prev_ad = hist_data['Acc_Dist_Line'].iloc[-2]\n",
        "    if latest_ad > prev_ad:\n",
        "        analysis += \"- Accumulation/Distribution Line is increasing, suggesting buying pressure.\\n\"\n",
        "    elif latest_ad < prev_ad:\n",
        "        analysis += \"- Accumulation/Distribution Line is decreasing, suggesting selling pressure.\\n\"\n",
        "    else:\n",
        "        analysis += \"- Accumulation/Distribution Line is stable.\\n\"\n",
        "\n",
        "    # Analyze Ichimoku Cloud\n",
        "    latest_close = latest_data['Close']\n",
        "    conversion_line = latest_data['Ichimoku_Conversion_Line']\n",
        "    base_line = latest_data['Ichimoku_Base_Line']\n",
        "    span_a = latest_data['Ichimoku_Span_A']\n",
        "    span_b = latest_data['Ichimoku_Span_B']\n",
        "\n",
        "    if latest_close > span_a and latest_close > span_b:\n",
        "        analysis += \"- Price is above the Ichimoku Cloud, indicating a bullish trend\\n\"\n",
        "    elif latest_close < span_a and latest_close < span_b:\n",
        "        analysis += \"- Price is below the Ichimoku Cloud, indicating a bearish trend\\n\"\n",
        "    else:\n",
        "        analysis += \"- Price is within the Ichimoku Cloud, indicating a neutral trend\\n\"\n",
        "\n",
        "    if conversion_line > base_line:\n",
        "        analysis += \"- Conversion line above base line, suggesting bullish momentum\\n\"\n",
        "    elif conversion_line < base_line:\n",
        "        analysis += \"- Conversion line below base line, suggesting bearish momentum\\n\"\n",
        "\n",
        "    # Analyze Money Flow Index (MFI)\n",
        "    if 'MFI' in latest_data:\n",
        "        mfi = latest_data['MFI']\n",
        "        if mfi > 80:\n",
        "            analysis += \"- MFI indicates overbought conditions\\n\"\n",
        "        elif mfi < 20:\n",
        "            analysis += \"- MFI indicates oversold conditions\\n\"\n",
        "        else:\n",
        "            analysis += \"- MFI is in neutral territory\\n\"\n",
        "\n",
        "    # Analyze Chaikin Money Flow\n",
        "    latest_cmf = hist_data['CMF'].iloc[-1]\n",
        "    prev_cmf = hist_data['CMF'].iloc[-2]\n",
        "\n",
        "    if latest_cmf > 0:\n",
        "        analysis += \"- Chaikin Money Flow is positive, indicating buying pressure\\n\"\n",
        "    else:\n",
        "        analysis += \"- Chaikin Money Flow is negative, indicating selling pressure\\n\"\n",
        "\n",
        "    if latest_cmf > prev_cmf:\n",
        "        analysis += \"- Chaikin Money Flow is increasing, suggesting growing buying pressure\\n\"\n",
        "    elif latest_cmf < prev_cmf:\n",
        "        analysis += \"- Chaikin Money Flow is decreasing, suggesting growing selling pressure\\n\"\n",
        "\n",
        "    # Analyze Mass Index\n",
        "    latest_mass_index = hist_data['Mass_Index'].iloc[-1]\n",
        "    prev_mass_index = hist_data['Mass_Index'].iloc[-2]\n",
        "\n",
        "    if latest_mass_index > 27 and prev_mass_index <= 27:\n",
        "        analysis += \"- Mass Index crossed above 27: Potential trend reversal signal\\n\"\n",
        "    elif latest_mass_index < 26.5 and prev_mass_index >= 26.5:\n",
        "        analysis += \"- Mass Index crossed below 26.5: Trend may be stabilizing\\n\"\n",
        "    elif latest_mass_index > 27:\n",
        "        analysis += \"- Mass Index is above 27: Increased likelihood of a trend reversal\\n\"\n",
        "    else:\n",
        "        analysis += \"- Mass Index is below 27: No significant trend reversal signal\\n\"\n",
        "\n",
        "    # Analyze RSI\n",
        "    if latest_data['RSI'] > 70:\n",
        "        analysis += \"- RSI indicates overbought conditions\\n\"\n",
        "    elif latest_data['RSI'] < 30:\n",
        "        analysis += \"- RSI indicates oversold conditions\\n\"\n",
        "    else:\n",
        "        analysis += \"- RSI is neutral\\n\"\n",
        "\n",
        "    # Analyze new MACD indicators\n",
        "    for macd_type in ['MACD_Long', 'MACD_VLong', 'MACD_XVLong']:\n",
        "        if latest_data[macd_type] > latest_data[f'{macd_type}_Signal'] and prev_data[macd_type] <= prev_data[f'{macd_type}_Signal']:\n",
        "            analysis += f\"- {macd_type} bullish crossover: Potential buy signal\\n\"\n",
        "        elif latest_data[macd_type] < latest_data[f'{macd_type}_Signal'] and prev_data[macd_type] >= prev_data[f'{macd_type}_Signal']:\n",
        "            analysis += f\"- {macd_type} bearish crossover: Potential sell signal\\n\"\n",
        "        else:\n",
        "            analysis += f\"- No significant {macd_type} signal\\n\"\n",
        "\n",
        "    # Analyze Bollinger Bands\n",
        "    if latest_data['Close'] > latest_data['volatility_bbh']:\n",
        "        analysis += \"- Price above upper Bollinger Band: Potentially overbought\\n\"\n",
        "    elif latest_data['Close'] < latest_data['volatility_bbl']:\n",
        "        analysis += \"- Price below lower Bollinger Band: Potentially oversold\\n\"\n",
        "    else:\n",
        "        analysis += \"- Price within Bollinger Bands\\n\"\n",
        "\n",
        "    # Analyze Gann Fan\n",
        "    latest_price = hist_data['Close'].iloc[-1]\n",
        "    gann_fan = calculate_gann_fan(hist_data)\n",
        "\n",
        "    analysis += \"\\nGann Fan Analysis:\\n\"\n",
        "    for col in gann_fan.columns:\n",
        "        latest_gann_level = gann_fan[col].iloc[-1]\n",
        "        if abs(latest_price - latest_gann_level) / latest_price < 0.01:  # Within 1% of a Gann level\n",
        "            analysis += f\"- Price is near the {col} Gann Fan level\\n\"\n",
        "\n",
        "    closest_gann_level = min(gann_fan.iloc[-1], key=lambda x: abs(x - latest_price))\n",
        "    closest_gann_angle = gann_fan.columns[gann_fan.iloc[-1].tolist().index(closest_gann_level)]\n",
        "    analysis += f\"- Closest Gann Fan level: {closest_gann_angle} at {closest_gann_level:.2f}\\n\"\n",
        "\n",
        "    # Gann Fan Time Projections\n",
        "    time_range = len(hist_data)\n",
        "    current_time = hist_data.index[-1]\n",
        "    for angle in [1, 2, 3, 4, 6, 8]:\n",
        "        time_projection = current_time + pd.Timedelta(days=int(time_range * (angle / 8)))\n",
        "        analysis += f\"- Gann {angle}/8 time projection: {time_projection.date()}\\n\"\n",
        "\n",
        "    # Analyze Volume\n",
        "    if latest_data['Volume'] > hist_data['Volume'].mean() * 1.5:\n",
        "        analysis += \"- Unusually high trading volume\\n\"\n",
        "    elif latest_data['Volume'] < hist_data['Volume'].mean() * 0.5:\n",
        "        analysis += \"- Unusually low trading volume\\n\"\n",
        "    # Analyze Fibonacci retracements\n",
        "    current_price = latest_data['Close']\n",
        "    for level in [0.236, 0.382, 0.5, 0.618, 0.786]:\n",
        "        fib_level = latest_data[f'Fib_{level}']\n",
        "        if abs(current_price - fib_level) / current_price < 0.01:  # Within 1% of a Fibonacci level\n",
        "            analysis += f\"- Price is near the {level} Fibonacci retracement level\\n\"\n",
        "\n",
        "    # Analyze Elliott Wave patterns\n",
        "    current_wave = latest_data['Elliott_Wave']\n",
        "    analysis += f\"- Current Elliott Wave count: {current_wave}\\n\"\n",
        "\n",
        "    # Analyze MACD divergence\n",
        "    macd_divergence = latest_data['MACD_Divergence']\n",
        "    if macd_divergence == 1:\n",
        "        analysis += \"- Bullish MACD divergence detected\\n\"\n",
        "    elif macd_divergence == -1:\n",
        "        analysis += \"- Bearish MACD divergence detected\\n\"\n",
        "    # Analyze OBV\n",
        "    if latest_data['OBV'] > prev_data['OBV']:\n",
        "        analysis += \"- OBV is increasing, suggesting buying pressure\\n\"\n",
        "    else:\n",
        "        analysis += \"- OBV is decreasing, suggesting selling pressure\\n\"\n",
        "\n",
        "    # Analyze ADX\n",
        "    if latest_data['ADX'] > 25:\n",
        "        analysis += \"- ADX indicates a strong trend\\n\"\n",
        "    elif latest_data['ADX'] < 20:\n",
        "        analysis += \"- ADX indicates a weak trend\\n\"\n",
        "    else:\n",
        "        analysis += \"- ADX indicates a moderate trend\\n\"\n",
        "\n",
        "    # Analyze Williams %R\n",
        "    latest_williams_r = latest_data['Williams_%R']\n",
        "    if latest_williams_r < -80:\n",
        "        analysis += \"- Williams %R indicates oversold conditions\\n\"\n",
        "    elif latest_williams_r > -20:\n",
        "        analysis += \"- Williams %R indicates overbought conditions\\n\"\n",
        "    else:\n",
        "        analysis += \"- Williams %R is in neutral territory\\n\"\n",
        "\n",
        "    # Analyze ATR\n",
        "    latest_atr = hist_data['ATR'].iloc[-1]\n",
        "    prev_atr = hist_data['ATR'].iloc[-2]\n",
        "    atr_change = (latest_atr - prev_atr) / prev_atr * 100\n",
        "\n",
        "    analysis += f\"\\nAverage True Range (ATR) Analysis:\\n\"\n",
        "    analysis += f\"- Current ATR: {latest_atr:.4f}\\n\"\n",
        "    analysis += f\"- ATR change: {atr_change:.2f}%\\n\"\n",
        "\n",
        "    if atr_change > 10:\n",
        "        analysis += \"- ATR is increasing significantly, indicating higher volatility\\n\"\n",
        "    elif atr_change < -10:\n",
        "        analysis += \"- ATR is decreasing significantly, indicating lower volatility\\n\"\n",
        "    else:\n",
        "        analysis += \"- ATR is relatively stable\\n\"\n",
        "\n",
        "    if latest_atr > hist_data['ATR'].mean():\n",
        "        analysis += \"- Current ATR is above its historical average, suggesting higher than usual volatility\\n\"\n",
        "    else:\n",
        "        analysis += \"- Current ATR is below its historical average, suggesting lower than usual volatility\\n\"\n",
        "\n",
        "    # Analyze Stochastic Oscillator\n",
        "    if latest_data['Stoch_K'] > 80 and latest_data['Stoch_D'] > 80:\n",
        "        analysis += \"- Stochastic Oscillator indicates overbought conditions\\n\"\n",
        "    elif latest_data['Stoch_K'] < 20 and latest_data['Stoch_D'] < 20:\n",
        "        analysis += \"- Stochastic Oscillator indicates oversold conditions\\n\"\n",
        "    else:\n",
        "        analysis += \"- Stochastic Oscillator is in neutral territory\\n\"\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def calculate_risk_metrics(hist_data, ticker, risk_free_rate=0.02):\n",
        "    if hist_data is None or hist_data.empty or len(hist_data) < 2:\n",
        "        print(f\"Insufficient data to calculate risk metrics for {ticker}\")\n",
        "        return {metric: 0 for metric in [\n",
        "            'Beta', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown',\n",
        "            'VaR (95%)', 'VaR (99%)', 'CVaR (95%)', 'Downside Deviation',\n",
        "            'Kurtosis', 'Skewness', 'Treynor Ratio', 'Information Ratio',\n",
        "            'Omega Ratio', 'Calmar Ratio', 'Tail Ratio', 'ETL (95%)',\n",
        "            'ETL (99%)', 'EVT VaR (95%)', 'EVT VaR (99%)'\n",
        "        ]}\n",
        "\n",
        "    returns = hist_data['Close'].pct_change().dropna()\n",
        "\n",
        "    if len(returns) < 2:\n",
        "        print(f\"Insufficient return data to calculate risk metrics for {ticker}\")\n",
        "        return {metric: 0 for metric in [\n",
        "            'Beta', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown',\n",
        "            'VaR (95%)', 'VaR (99%)', 'CVaR (95%)', 'Downside Deviation',\n",
        "            'Kurtosis', 'Skewness', 'Treynor Ratio', 'Information Ratio',\n",
        "            'Omega Ratio', 'Calmar Ratio', 'Tail Ratio', 'ETL (95%)',\n",
        "            'ETL (99%)', 'EVT VaR (95%)', 'EVT VaR (99%)'\n",
        "        ]}\n",
        "\n",
        "    metrics = {}\n",
        "    epsilon = 1e-8  # Small value to prevent division by zero\n",
        "\n",
        "    try:\n",
        "        metrics['Beta'] = get_beta(ticker, start_date=hist_data.index[0], end_date=hist_data.index[-1])\n",
        "    except Exception:\n",
        "        metrics['Beta'] = 0\n",
        "\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming daily returns\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    cum_returns = (1 + returns).cumprod()\n",
        "\n",
        "    metrics['Sharpe Ratio'] = calculate_ratio(excess_returns, np.std(excess_returns), epsilon)\n",
        "    metrics['Sortino Ratio'] = calculate_ratio(excess_returns, np.std(downside_returns), epsilon)\n",
        "    metrics['Maximum Drawdown'] = (cum_returns / cum_returns.cummax() - 1).min()\n",
        "    metrics['VaR (95%)'] = np.percentile(returns, 5)\n",
        "    metrics['VaR (99%)'] = np.percentile(returns, 1)\n",
        "    metrics['CVaR (95%)'] = calculate_cvar(returns, 0.95)\n",
        "    metrics['Downside Deviation'] = np.sqrt(np.mean(np.minimum(returns - risk_free_rate / 252, 0)**2))\n",
        "    metrics['Kurtosis'] = stats.kurtosis(returns)\n",
        "    metrics['Skewness'] = stats.skew(returns)\n",
        "    metrics['Treynor Ratio'] = (returns.mean() - risk_free_rate / 252) / metrics['Beta'] if metrics['Beta'] != 0 else 0\n",
        "    metrics['Information Ratio'] = calculate_information_ratio(returns, hist_data)\n",
        "    metrics['Omega Ratio'] = calculate_omega_ratio(returns, risk_free_rate)\n",
        "    metrics['Calmar Ratio'] = returns.mean() * 252 / abs(metrics['Maximum Drawdown']) if metrics['Maximum Drawdown'] != 0 else 0\n",
        "    metrics['Tail Ratio'] = abs(np.percentile(returns, 95)) / abs(np.percentile(returns, 5)) if np.percentile(returns, 5) != 0 else 0\n",
        "    metrics['ETL (95%)'] = calculate_cvar(returns, 0.95)\n",
        "    metrics['ETL (99%)'] = calculate_cvar(returns, 0.99)\n",
        "    metrics['EVT VaR (95%)'] = calculate_evt_var(returns, 0.95)\n",
        "    metrics['EVT VaR (99%)'] = calculate_evt_var(returns, 0.99)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_ratio(excess_returns, denominator, epsilon):\n",
        "    return np.sqrt(252) * safe_mean(excess_returns) / (denominator + epsilon)\n",
        "\n",
        "def calculate_cvar(returns, confidence_level):\n",
        "    var = np.percentile(returns, (1 - confidence_level) * 100)\n",
        "    return -np.mean(returns[returns <= var])\n",
        "\n",
        "def calculate_evt_var(returns, confidence_level):\n",
        "    threshold = np.percentile(returns, 5)  # Use 5% as threshold\n",
        "    exceedances = -returns[returns <= threshold]\n",
        "\n",
        "    if len(exceedances) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    params = genpareto.fit(exceedances)\n",
        "    q = genpareto.ppf(confidence_level, *params)\n",
        "    return -threshold - q\n",
        "\n",
        "def calculate_information_ratio(returns, hist_data):\n",
        "    benchmark_returns = yf.Ticker('^GSPC').history(start=hist_data.index[0], end=hist_data.index[-1])['Close'].pct_change().dropna()\n",
        "    excess_returns_vs_benchmark = returns - benchmark_returns\n",
        "    if len(excess_returns_vs_benchmark) == 0:\n",
        "        return 0\n",
        "    return safe_mean(excess_returns_vs_benchmark) / (np.std(excess_returns_vs_benchmark) + 1e-8)\n",
        "\n",
        "def calculate_omega_ratio(returns, risk_free_rate):\n",
        "    threshold = risk_free_rate / 252\n",
        "    return (returns[returns > threshold].sum() - threshold * (returns > threshold).sum()) / (threshold * (returns <= threshold).sum() - returns[returns <= threshold].sum())\n",
        "\n",
        "def safe_mean(arr):\n",
        "    return np.mean(arr) if len(arr) > 0 else 0\n",
        "\n",
        "\n",
        "def analyze_economic_indicators(economic_data):\n",
        "    analysis = \"Economic Indicators Analysis:\\n\"\n",
        "\n",
        "    for indicator, data in economic_data.items():\n",
        "        if not data.empty:\n",
        "            current_value = data.iloc[-1, 0]\n",
        "            avg_value = data.mean().iloc[0]\n",
        "            pct_change = data.pct_change(fill_method=None).iloc[-1, 0] * 100\n",
        "            years_of_data = (data.index[-1] - data.index[0]).days / 365.25\n",
        "\n",
        "            analysis += f\"\\n{indicator}:\\n\"\n",
        "            analysis += f\"  Current value: {current_value:.2f}\\n\"\n",
        "            analysis += f\"  Average value (20 years): {avg_value:.2f}\\n\"\n",
        "            analysis += f\"  Recent change: {pct_change:.2f}%\\n\"\n",
        "\n",
        "            if indicator == 'GDP':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  The economy is showing strong growth.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  The economy is contracting.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The economy is showing moderate growth.\\n\"\n",
        "            elif indicator == 'Inflation':\n",
        "                if current_value > 3:\n",
        "                    analysis += \"  Inflation is above target, which might lead to tighter monetary policy.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Inflation is within acceptable range.\\n\"\n",
        "            elif indicator == 'Unemployment':\n",
        "                if current_value < 5:\n",
        "                    analysis += \"  The job market is strong.\\n\"\n",
        "                elif current_value > 7:\n",
        "                    analysis += \"  The job market is weak.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The job market is stable.\\n\"\n",
        "            elif indicator == 'Interest Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Interest rates are rising, which might affect borrowing and investment.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  Interest rates are falling, which might stimulate borrowing and investment.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Interest rates are stable.\\n\"\n",
        "            elif indicator == 'Consumer Confidence':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Consumer confidence is improving significantly.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Consumer confidence is declining significantly.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Consumer confidence is relatively stable.\\n\"\n",
        "            elif indicator in ['Housing Starts', 'Building Permits']:\n",
        "                if pct_change > 5:\n",
        "                    analysis += f\"  {indicator} are increasing, indicating strength in the housing market.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += f\"  {indicator} are decreasing, indicating weakness in the housing market.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} are stable.\\n\"\n",
        "            elif indicator == 'Retail Sales':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Consumer spending is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Consumer spending is decreasing.\\n\"\n",
        "            elif indicator == 'Industrial Production':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Industrial production is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Industrial production is decreasing.\\n\"\n",
        "            elif indicator == 'Durable Goods Orders':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Business investment in durable goods is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Business investment in durable goods is decreasing.\\n\"\n",
        "            elif indicator == 'Trade Balance':\n",
        "                if current_value > 0:\n",
        "                    analysis += \"  The country has a trade surplus.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The country has a trade deficit.\\n\"\n",
        "            elif indicator == 'PPI':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  Producer prices are rising significantly, which may lead to higher consumer prices.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  Producer prices are falling, which may lead to lower consumer prices.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Producer prices are stable.\\n\"\n",
        "            elif indicator == 'Capacity Utilization':\n",
        "                if current_value > 80:\n",
        "                    analysis += \"  High capacity utilization may lead to increased business investment.\\n\"\n",
        "                elif current_value < 70:\n",
        "                    analysis += \"  Low capacity utilization indicates economic slack.\\n\"\n",
        "            elif indicator == 'JOLTS':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Job openings are increasing significantly.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Job openings are decreasing significantly.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Job openings are relatively stable.\\n\"\n",
        "            elif indicator in ['Personal Income', 'Personal Spending']:\n",
        "                if pct_change > 0:\n",
        "                    analysis += f\"  {indicator} is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} is decreasing.\\n\"\n",
        "            elif indicator == 'Business Inventories':\n",
        "                if pct_change > 1:\n",
        "                    analysis += \"  Business inventories are increasing, which may indicate expectations of higher future sales.\\n\"\n",
        "                elif pct_change < -1:\n",
        "                    analysis += \"  Business inventories are decreasing, which may indicate expectations of lower future sales.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Business inventories are stable.\\n\"\n",
        "            elif indicator == 'Leading Economic Index':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  The leading economic index is increasing, suggesting potential economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The leading economic index is decreasing, suggesting potential economic contraction.\\n\"\n",
        "            elif indicator == 'Yield Curve':\n",
        "                if current_value > 0:\n",
        "                    analysis += \"  The yield curve is positive, suggesting economic expansion.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The yield curve is inverted, which may signal a potential recession.\\n\"\n",
        "            elif indicator == 'Non-Farm Payrolls':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Job creation is positive, indicating economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Job creation is negative, indicating economic contraction.\\n\"\n",
        "            elif indicator == 'Core PCE Price Index':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  Core inflation is above the Federal Reserve's target.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Core inflation is within the Federal Reserve's target range.\\n\"\n",
        "            elif indicator == 'Initial Jobless Claims':\n",
        "                if pct_change < 0:\n",
        "                    analysis += \"  Initial jobless claims are decreasing, indicating an improving job market.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Initial jobless claims are increasing, indicating potential job market weakness.\\n\"\n",
        "            elif indicator in ['Existing Home Sales', 'New Home Sales']:\n",
        "                if pct_change > 0:\n",
        "                    analysis += f\"  {indicator} are increasing, indicating strength in the housing market.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} are decreasing, indicating weakness in the housing market.\\n\"\n",
        "            elif indicator == 'Balance of Trade':\n",
        "                if current_value > 0:\n",
        "                    analysis += \"  The country has a trade surplus.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The country has a trade deficit.\\n\"\n",
        "            elif indicator == 'Government Debt to GDP':\n",
        "                if current_value > 100:\n",
        "                    analysis += \"  Government debt is higher than GDP, which may be a concern.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Government debt is lower than GDP.\\n\"\n",
        "            elif indicator == 'Manufacturing Production':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Manufacturing production is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Manufacturing production is decreasing.\\n\"\n",
        "            elif indicator == 'Energy Production':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Energy production is increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Energy production is decreasing.\\n\"\n",
        "            elif indicator == 'Real Estate Sales':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Real estate sales are increasing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Real estate sales are decreasing.\\n\"\n",
        "            elif indicator == 'M2':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  M2 money supply is expanding rapidly, which could lead to inflationary pressures.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  M2 money supply is contracting, which could lead to deflationary pressures.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  M2 money supply is growing at a moderate pace.\\n\"\n",
        "            elif indicator == 'Labor Force Participation Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Labor force participation is increasing, indicating more people are entering the job market.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Labor force participation is decreasing, indicating fewer people are in the job market.\\n\"\n",
        "            elif indicator == 'S&P 500 Index':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  The S&P 500 index is rising, indicating positive market sentiment.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The S&P 500 index is falling, indicating negative market sentiment.\\n\"\n",
        "            elif indicator == 'Average Hourly Earnings':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Average hourly earnings are increasing, which could boost consumer spending.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Average hourly earnings are decreasing, which could reduce consumer spending.\\n\"\n",
        "            elif indicator == 'Productivity':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Productivity is increasing, which is positive for economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Productivity is decreasing, which could slow economic growth.\\n\"\n",
        "            elif indicator == 'Unit Labor Costs':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Unit labor costs are increasing, which could pressure corporate profits.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Unit labor costs are decreasing, which could boost corporate profits.\\n\"\n",
        "            elif indicator == 'Corporate Profits':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Corporate profits are increasing, which is positive for the stock market.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Corporate profits are decreasing, which could negatively impact the stock market.\\n\"\n",
        "            elif indicator == 'Federal Debt':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  Federal debt is increasing rapidly, which could lead to long-term fiscal challenges.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Federal debt growth is moderate or decreasing.\\n\"\n",
        "            elif indicator == 'Consumer Credit':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Consumer credit is expanding, indicating increased consumer borrowing and spending.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Consumer credit is contracting, indicating decreased consumer borrowing and spending.\\n\"\n",
        "            elif indicator == 'Household Debt to GDP':\n",
        "                if current_value > 100:\n",
        "                    analysis += \"  Household debt to GDP ratio is high, which could limit future consumer spending growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Household debt to GDP ratio is at a manageable level.\\n\"\n",
        "            elif indicator == 'Velocity of M2 Money Stock':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Velocity of M2 money stock is increasing, indicating more economic activity per unit of money supply.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Velocity of M2 money stock is decreasing, indicating less economic activity per unit of money supply.\\n\"\n",
        "            elif indicator == 'Median Sales Price of Houses':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Median sales price of houses is increasing, which could impact housing affordability.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Median sales price of houses is decreasing, which could improve housing affordability.\\n\"\n",
        "            elif indicator == 'Homeownership Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Homeownership rate is increasing, indicating more people are buying homes.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Homeownership rate is decreasing, indicating fewer people are buying homes.\\n\"\n",
        "            elif indicator == 'Mortgage Delinquency Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Mortgage delinquency rate is increasing, which could signal stress in the housing market.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Mortgage delinquency rate is decreasing, indicating improved health in the housing market.\\n\"\n",
        "            elif indicator == 'Commercial and Industrial Loans':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Commercial and industrial loans are increasing, indicating business expansion.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Commercial and industrial loans are decreasing, which could signal a slowdown in business activity.\\n\"\n",
        "            elif indicator == 'Bank Prime Loan Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Bank prime loan rate is increasing, which could make borrowing more expensive.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Bank prime loan rate is decreasing, which could make borrowing more affordable.\\n\"\n",
        "            elif indicator == 'Total Vehicle Sales':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Total vehicle sales are increasing, indicating strong consumer demand.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Total vehicle sales are decreasing, which could signal weakening consumer demand.\\n\"\n",
        "            elif indicator == 'E-Commerce Retail Sales':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  E-commerce retail sales are increasing, showing growth in online shopping.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  E-commerce retail sales are decreasing, which could indicate a shift in consumer behavior.\\n\"\n",
        "            elif indicator == 'GDP Growth Rate':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  GDP growth rate is strong, indicating robust economic expansion.\\n\"\n",
        "                elif pct_change > 0:\n",
        "                    analysis += \"  GDP growth rate is positive, indicating moderate economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  GDP growth rate is negative, indicating economic contraction.\\n\"\n",
        "            elif indicator == 'Core Inflation Rate':\n",
        "                if current_value > 2:\n",
        "                    analysis += \"  Core inflation rate is above the Federal Reserve's target, which might lead to tighter monetary policy.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Core inflation rate is within the Federal Reserve's target range.\\n\"\n",
        "            elif indicator == 'Consumer Credit Change':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Consumer credit is expanding, indicating increased consumer borrowing.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Consumer credit is contracting, indicating decreased consumer borrowing.\\n\"\n",
        "            elif indicator in ['Retail Inventories', 'Wholesale Inventories']:\n",
        "                if pct_change > 1:\n",
        "                    analysis += f\"  {indicator} are increasing, which may indicate expectations of higher future sales.\\n\"\n",
        "                elif pct_change < -1:\n",
        "                    analysis += f\"  {indicator} are decreasing, which may indicate expectations of lower future sales.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} are stable.\\n\"\n",
        "            elif indicator == 'Factory Orders':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Factory orders are increasing, indicating growth in manufacturing activity.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Factory orders are decreasing, indicating a slowdown in manufacturing activity.\\n\"\n",
        "            elif indicator == 'Construction Spending':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Construction spending is increasing, indicating growth in the construction sector.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Construction spending is decreasing, indicating a slowdown in the construction sector.\\n\"\n",
        "            elif indicator == 'Private Sector Credit':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Private sector credit is expanding, indicating increased borrowing and potential economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Private sector credit is contracting, which could signal economic slowdown.\\n\"\n",
        "            elif indicator in ['Export Prices', 'Import Prices']:\n",
        "                if pct_change > 0:\n",
        "                    analysis += f\"  {indicator} are increasing, which could affect trade balance and inflation.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} are decreasing, which could affect trade balance and inflation.\\n\"\n",
        "            elif indicator == 'Producer Price Index':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  Producer Price Index is rising significantly, which may lead to higher consumer prices.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  Producer Price Index is falling, which may lead to lower consumer prices.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Producer Price Index is stable.\\n\"\n",
        "            elif indicator in ['Philadelphia Fed Manufacturing Index', 'Empire State Manufacturing Index', 'Kansas City Fed Manufacturing Index']:\n",
        "                if current_value > 0:\n",
        "                    analysis += f\"  {indicator} is positive, indicating expansion in manufacturing activity.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} is negative, indicating contraction in manufacturing activity.\\n\"\n",
        "            elif indicator == 'Chicago Fed National Activity Index':\n",
        "                if current_value > 0:\n",
        "                    analysis += \"  Chicago Fed National Activity Index is positive, indicating above-trend economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Chicago Fed National Activity Index is negative, indicating below-trend economic growth.\\n\"\n",
        "            elif indicator == 'NFIB Small Business Optimism Index':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  Small business optimism is increasing, which could lead to more hiring and investment.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Small business optimism is decreasing, which could lead to less hiring and investment.\\n\"\n",
        "            elif indicator == 'US Dollar Index':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  US Dollar is strengthening, which could affect exports and multinational companies' profits.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  US Dollar is weakening, which could boost exports and multinational companies' profits.\\n\"\n",
        "            elif indicator == 'VIX Volatility Index':\n",
        "                if current_value > 20:\n",
        "                    analysis += \"  VIX is elevated, indicating higher market uncertainty and potential volatility.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  VIX is low, indicating lower market uncertainty and potential complacency.\\n\"\n",
        "            elif indicator == 'Global Crude Oil Prices':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Oil prices are rising significantly, which could impact inflation and consumer spending.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Oil prices are falling significantly, which could boost consumer spending but impact energy sector.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Oil prices are relatively stable.\\n\"\n",
        "            elif indicator == 'Henry Hub Natural Gas Spot Price':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Natural gas prices are rising, which could impact energy costs and inflation.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Natural gas prices are falling, which could reduce energy costs.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Natural gas prices are relatively stable.\\n\"\n",
        "            elif indicator in ['Copper Price', 'Global price of Aluminum', 'Global price of Iron Ore']:\n",
        "                if pct_change > 5:\n",
        "                    analysis += f\"  {indicator} is rising significantly, which could indicate increasing industrial demand.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += f\"  {indicator} is falling significantly, which could indicate decreasing industrial demand.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} is relatively stable.\\n\"\n",
        "            elif indicator in ['Corn Price', 'Soybean Price', 'Wheat Price']:\n",
        "                if pct_change > 5:\n",
        "                    analysis += f\"  {indicator} is rising significantly, which could impact food prices and inflation.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += f\"  {indicator} is falling significantly, which could reduce food prices.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} is relatively stable.\\n\"\n",
        "            elif indicator in ['Global price of Cotton', 'Global price of Rubber']:\n",
        "                if pct_change > 5:\n",
        "                    analysis += f\"  {indicator} is rising significantly, which could impact manufacturing costs.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += f\"  {indicator} is falling significantly, which could reduce manufacturing costs.\\n\"\n",
        "                else:\n",
        "                    analysis += f\"  {indicator} is relatively stable.\\n\"\n",
        "            elif indicator == 'Inflation, consumer prices for the World':\n",
        "                if current_value > 3:\n",
        "                    analysis += \"  Global inflation is above average, which could lead to tighter monetary policies worldwide.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global inflation is moderate.\\n\"\n",
        "            elif indicator == 'Global Price Index of All Commodities':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Global commodity prices are rising significantly, which could impact inflation and production costs.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Global commodity prices are falling significantly, which could reduce inflation and production costs.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global commodity prices are relatively stable.\\n\"\n",
        "            elif indicator == 'Global price of Food and beverage index':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Global food and beverage prices are rising significantly, which could impact inflation and consumer spending.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Global food and beverage prices are falling significantly, which could boost consumer spending.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global food and beverage prices are relatively stable.\\n\"\n",
        "            elif indicator == 'Global price of Metal index':\n",
        "                if pct_change > 5:\n",
        "                    analysis += \"  Global metal prices are rising significantly, which could indicate increasing industrial demand.\\n\"\n",
        "                elif pct_change < -5:\n",
        "                    analysis += \"  Global metal prices are falling significantly, which could indicate decreasing industrial demand.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global metal prices are relatively stable.\\n\"\n",
        "            elif indicator == 'Equity Market-related Economic Uncertainty Index':\n",
        "                if current_value > avg_value:\n",
        "                    analysis += \"  Economic uncertainty in equity markets is above average, which may lead to increased market volatility.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Economic uncertainty in equity markets is below average, suggesting relatively stable market conditions.\\n\"\n",
        "\n",
        "            elif indicator == 'Economic Policy Uncertainty Index':\n",
        "                if current_value > avg_value:\n",
        "                    analysis += \"  Economic policy uncertainty is above average, which may impact business decisions and investments.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Economic policy uncertainty is below average, suggesting a more stable policy environment.\\n\"\n",
        "\n",
        "            elif indicator == 'Total Business Inventories to Sales Ratio':\n",
        "                if current_value > avg_value:\n",
        "                    analysis += \"  Business inventories relative to sales are higher than average, which may indicate slower sales or overproduction.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Business inventories relative to sales are lower than average, suggesting efficient inventory management or strong sales.\\n\"\n",
        "\n",
        "            elif indicator == 'ISM Manufacturing PMI':\n",
        "                if current_value > 50:\n",
        "                    analysis += \"  Manufacturing sector is expanding, indicating economic growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Manufacturing sector is contracting, which may signal economic slowdown.\\n\"\n",
        "\n",
        "            elif indicator == 'Capacity Utilization: Manufacturing SIC':\n",
        "                if current_value > 80:\n",
        "                    analysis += \"  Manufacturing capacity utilization is high, suggesting strong demand and potential for increased business investment.\\n\"\n",
        "                elif current_value < 70:\n",
        "                    analysis += \"  Manufacturing capacity utilization is low, indicating economic slack in the manufacturing sector.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Manufacturing capacity utilization is at moderate levels.\\n\"\n",
        "\n",
        "            if indicator == 'China GDP Growth Rate':\n",
        "                if pct_change > 6:\n",
        "                    analysis += \"  China's economy is growing rapidly.\\n\"\n",
        "                elif pct_change > 0:\n",
        "                    analysis += \"  China's economy is growing moderately.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  China's economy is contracting, which is unusual and concerning.\\n\"\n",
        "\n",
        "            elif indicator == 'China Inflation Rate':\n",
        "                if current_value > 3:\n",
        "                    analysis += \"  Inflation in China is above target, which might lead to tighter monetary policy.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Inflation in China is within an acceptable range.\\n\"\n",
        "\n",
        "            elif indicator == 'China Unemployment Rate':\n",
        "                if current_value > 5:\n",
        "                    analysis += \"  Unemployment in China is relatively high, which could impact consumer spending.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Unemployment in China is at a manageable level.\\n\"\n",
        "\n",
        "            elif indicator == 'Japan GDP Growth Rate':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  Japan's economy is showing strong growth.\\n\"\n",
        "                elif pct_change > 0:\n",
        "                    analysis += \"  Japan's economy is showing moderate growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Japan's economy is contracting.\\n\"\n",
        "\n",
        "            elif indicator == 'Japan Inflation Rate':\n",
        "                if current_value > 2:\n",
        "                    analysis += \"  Inflation in Japan is above the Bank of Japan's target, which is unusual.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Inflation in Japan remains low, consistent with recent trends.\\n\"\n",
        "\n",
        "            elif indicator == 'Japan Unemployment Rate':\n",
        "                if current_value > 4:\n",
        "                    analysis += \"  Unemployment in Japan is relatively high by its standards.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Unemployment in Japan remains low, indicating a tight labor market.\\n\"\n",
        "\n",
        "            elif indicator == 'Eurozone GDP Growth Rate':\n",
        "                if pct_change > 2:\n",
        "                    analysis += \"  The Eurozone economy is showing strong growth.\\n\"\n",
        "                elif pct_change > 0:\n",
        "                    analysis += \"  The Eurozone economy is showing moderate growth.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The Eurozone economy is contracting.\\n\"\n",
        "\n",
        "            elif indicator == 'Eurozone Inflation Rate':\n",
        "                if current_value > 2:\n",
        "                    analysis += \"  Inflation in the Eurozone is above the ECB's target, which might lead to tighter monetary policy.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Inflation in the Eurozone is within the ECB's target range.\\n\"\n",
        "\n",
        "            elif indicator == 'Eurozone Unemployment Rate':\n",
        "                if current_value > 8:\n",
        "                    analysis += \"  Unemployment in the Eurozone is relatively high, which could impact consumer spending.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Unemployment in the Eurozone is at a manageable level.\\n\"\n",
        "\n",
        "            elif indicator == 'ECB Interest Rate':\n",
        "                if pct_change > 0:\n",
        "                    analysis += \"  The ECB is raising interest rates, which could affect borrowing and investment in the Eurozone.\\n\"\n",
        "                elif pct_change < 0:\n",
        "                    analysis += \"  The ECB is lowering interest rates, which could stimulate borrowing and investment in the Eurozone.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  The ECB interest rate remains unchanged.\\n\"\n",
        "\n",
        "            elif indicator == 'World GDP Growth':\n",
        "                if pct_change > 3:\n",
        "                    analysis += \"  Global economic growth is strong.\\n\"\n",
        "                elif pct_change > 0:\n",
        "                    analysis += \"  Global economic growth is moderate.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global economic growth is negative, indicating a worldwide recession.\\n\"\n",
        "\n",
        "            elif indicator == 'Global Inflation':\n",
        "                if current_value > 4:\n",
        "                    analysis += \"  Global inflation is high, which could lead to tightening monetary policies worldwide.\\n\"\n",
        "                elif current_value > 2:\n",
        "                    analysis += \"  Global inflation is moderate.\\n\"\n",
        "                else:\n",
        "                    analysis += \"  Global inflation is low, which might concern some central banks.\\n\"\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def get_beta(ticker, market_ticker='^GSPC', start_date=None, end_date=None):\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now() - timedelta(days=252*5)  # 5 years of data\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now()\n",
        "\n",
        "    stock = yf.Ticker(ticker)\n",
        "    market = yf.Ticker(market_ticker)\n",
        "\n",
        "    stock_data = stock.history(start=start_date, end=end_date)['Close']\n",
        "    market_data = market.history(start=start_date, end=end_date)['Close']\n",
        "\n",
        "    stock_returns = stock_data.pct_change().dropna()\n",
        "    market_returns = market_data.pct_change().dropna()\n",
        "\n",
        "    if len(stock_returns) < 2 or len(market_returns) < 2:\n",
        "        return 0\n",
        "\n",
        "    # Ensure the data is aligned\n",
        "    aligned_data = pd.concat([stock_returns, market_returns], axis=1).dropna()\n",
        "    if aligned_data.empty:\n",
        "        return 0\n",
        "\n",
        "    stock_returns = aligned_data.iloc[:, 0]\n",
        "    market_returns = aligned_data.iloc[:, 1]\n",
        "\n",
        "    epsilon = 1e-8  # Small value to prevent division by zero\n",
        "\n",
        "    covariance = np.cov(stock_returns, market_returns)[0][1]\n",
        "    market_variance = np.var(market_returns)\n",
        "\n",
        "    if market_variance == 0:\n",
        "        market_variance = epsilon  # Use epsilon instead of zero\n",
        "\n",
        "    beta = covariance / market_variance\n",
        "\n",
        "    return beta\n",
        "\n",
        "def calculate_option_greeks(S, K, T, r, sigma, option_type):\n",
        "    epsilon = 1e-8  # Small value to prevent division by zero\n",
        "\n",
        "    # Ensure inputs are valid and non-zero\n",
        "    S = max(S, epsilon)\n",
        "    K = max(K, epsilon)\n",
        "    T = max(T, epsilon)\n",
        "    sigma = max(sigma, epsilon)\n",
        "\n",
        "    try:\n",
        "        d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "        d2 = d1 - sigma * np.sqrt(T)\n",
        "\n",
        "        if option_type == 'call':\n",
        "            delta = norm.cdf(d1)\n",
        "            gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))\n",
        "            theta = -S * norm.pdf(d1) * sigma / (2 * np.sqrt(T)) - r * K * np.exp(-r * T) * norm.cdf(d2)\n",
        "            vega = S * norm.pdf(d1) * np.sqrt(T)\n",
        "        else:  # put\n",
        "            delta = -norm.cdf(-d1)\n",
        "            gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))\n",
        "            theta = -S * norm.pdf(d1) * sigma / (2 * np.sqrt(T)) + r * K * np.exp(-r * T) * norm.cdf(-d2)\n",
        "            vega = S * norm.pdf(d1) * np.sqrt(T)\n",
        "\n",
        "        return delta, gamma, theta, vega\n",
        "    except (ValueError, ZeroDivisionError):\n",
        "        # Return default values if calculation fails\n",
        "        return 0, 0, 0, 0\n",
        "\n",
        "def analyze_options_data(options_data, current_price):\n",
        "    if not options_data:\n",
        "        return \"Options data not available.\"\n",
        "\n",
        "    try:\n",
        "        analysis = \"Options Analysis:\\n\"\n",
        "        risk_free_rate = 0.02  # Assume a 2% risk-free rate\n",
        "\n",
        "        for date, data in options_data.items():\n",
        "            calls = data.get('calls')\n",
        "            puts = data.get('puts')\n",
        "\n",
        "            if not isinstance(calls, pd.DataFrame) or not isinstance(puts, pd.DataFrame):\n",
        "                continue\n",
        "\n",
        "            # Calculate put-call ratio\n",
        "            put_call_ratio = len(puts) / len(calls) if len(calls) > 0 else float('inf')\n",
        "\n",
        "            # Calculate implied volatility for ATM options\n",
        "            atm_call = calls.iloc[(calls['strike'] - current_price).abs().argsort()[:1]]\n",
        "            atm_put = puts.iloc[(puts['strike'] - current_price).abs().argsort()[:1]]\n",
        "\n",
        "            atm_call_iv = atm_call['impliedVolatility'].values[0] if not atm_call.empty and 'impliedVolatility' in atm_call.columns else 'N/A'\n",
        "            atm_put_iv = atm_put['impliedVolatility'].values[0] if not atm_put.empty and 'impliedVolatility' in atm_put.columns else 'N/A'\n",
        "\n",
        "            # Calculate average implied volatility\n",
        "            avg_call_iv = calls['impliedVolatility'].mean() if 'impliedVolatility' in calls.columns else 'N/A'\n",
        "            avg_put_iv = puts['impliedVolatility'].mean() if 'impliedVolatility' in puts.columns else 'N/A'\n",
        "\n",
        "            # Calculate open interest put-call ratio\n",
        "            oi_put_call_ratio = puts['openInterest'].sum() / calls['openInterest'].sum() if calls['openInterest'].sum() > 0 else float('inf')\n",
        "\n",
        "            # Identify strikes with highest open interest\n",
        "            try:\n",
        "                if not calls.empty and 'openInterest' in calls.columns and not calls['openInterest'].isna().all():\n",
        "                    max_oi_index = calls['openInterest'].fillna(0).idxmax()\n",
        "                    max_oi_call_strike = calls.loc[max_oi_index, 'strike']\n",
        "                else:\n",
        "                    max_oi_call_strike = 'N/A'\n",
        "            except:\n",
        "                max_oi_call_strike = 'N/A'\n",
        "\n",
        "            try:\n",
        "                if not puts.empty and 'openInterest' in puts.columns and not puts['openInterest'].isna().all():\n",
        "                    max_oi_index = puts['openInterest'].fillna(0).idxmax()\n",
        "                    max_oi_put_strike = puts.loc[max_oi_index, 'strike']\n",
        "                else:\n",
        "                    max_oi_put_strike = 'N/A'\n",
        "            except:\n",
        "                max_oi_put_strike = 'N/A'\n",
        "\n",
        "            # Identify options with unusual volume\n",
        "            try:\n",
        "                unusual_volume_threshold = 3  # 3 times average volume\n",
        "                unusual_calls = calls[calls['volume'] > calls['volume'].mean() * unusual_volume_threshold] if 'volume' in calls.columns else pd.DataFrame()\n",
        "                unusual_puts = puts[puts['volume'] > puts['volume'].mean() * unusual_volume_threshold] if 'volume' in puts.columns else pd.DataFrame()\n",
        "            except:\n",
        "                unusual_calls = pd.DataFrame()\n",
        "                unusual_puts = pd.DataFrame()\n",
        "\n",
        "            # Calculate Greeks for ATM options\n",
        "            T = (pd.to_datetime(date) - pd.Timestamp.now()).days / 252\n",
        "\n",
        "            if not atm_call.empty and 'impliedVolatility' in atm_call.columns:\n",
        "                call_delta, call_gamma, call_theta, call_vega = calculate_option_greeks(\n",
        "                    current_price, atm_call['strike'].values[0], T, risk_free_rate,\n",
        "                    atm_call['impliedVolatility'].values[0], 'call'\n",
        "                )\n",
        "            else:\n",
        "                call_delta, call_gamma, call_theta, call_vega = 'N/A', 'N/A', 'N/A', 'N/A'\n",
        "\n",
        "            if not atm_put.empty and 'impliedVolatility' in atm_put.columns:\n",
        "                put_delta, put_gamma, put_theta, put_vega = calculate_option_greeks(\n",
        "                    current_price, atm_put['strike'].values[0], T, risk_free_rate,\n",
        "                    atm_put['impliedVolatility'].values[0], 'put'\n",
        "                )\n",
        "            else:\n",
        "                put_delta, put_gamma, put_theta, put_vega = 'N/A', 'N/A', 'N/A', 'N/A'\n",
        "\n",
        "            analysis += f\"\"\"\n",
        "            Expiration Date: {date}\n",
        "            - Put-Call Ratio: {put_call_ratio:.2f}\n",
        "            - Open Interest Put-Call Ratio: {oi_put_call_ratio:.2f}\n",
        "            - ATM Call Implied Volatility: {atm_call_iv}\n",
        "            - ATM Put Implied Volatility: {atm_put_iv}\n",
        "            - Average Call Implied Volatility: {avg_call_iv}\n",
        "            - Average Put Implied Volatility: {avg_put_iv}\n",
        "            - Number of Calls with Unusual Volume: {len(unusual_calls)}\n",
        "            - Number of Puts with Unusual Volume: {len(unusual_puts)}\n",
        "            - Strike with Highest Call Open Interest: {max_oi_call_strike}\n",
        "            - Strike with Highest Put Open Interest: {max_oi_put_strike}\n",
        "\n",
        "            Greeks for ATM options:\n",
        "            Call: Delta: {call_delta}, Gamma: {call_gamma}, Theta: {call_theta}, Vega: {call_vega}\n",
        "            Put: Delta: {put_delta}, Gamma: {put_gamma}, Theta: {put_theta}, Vega: {put_vega}\n",
        "            \"\"\"\n",
        "\n",
        "            # Analyze implied volatility skew\n",
        "            try:\n",
        "                if 'impliedVolatility' in calls.columns and 'strike' in calls.columns:\n",
        "                    call_iv_skew = calls.groupby('strike')['impliedVolatility'].mean()\n",
        "                else:\n",
        "                    call_iv_skew = pd.Series()\n",
        "\n",
        "                if 'impliedVolatility' in puts.columns and 'strike' in puts.columns:\n",
        "                    put_iv_skew = puts.groupby('strike')['impliedVolatility'].mean()\n",
        "                else:\n",
        "                    put_iv_skew = pd.Series()\n",
        "\n",
        "                analysis += f\"\"\"\n",
        "                Implied Volatility Skew:\n",
        "                Calls:\n",
        "                {call_iv_skew.to_string() if not call_iv_skew.empty else 'N/A'}\n",
        "\n",
        "                Puts:\n",
        "                {put_iv_skew.to_string() if not put_iv_skew.empty else 'N/A'}\n",
        "                \"\"\"\n",
        "            except Exception as e:\n",
        "                analysis += \"\\nError calculating volatility skew\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in options analysis: {str(e)}\")\n",
        "        return \"Error analyzing options data\"\n",
        "\n",
        "def calculate_options_probability(options_data, current_price, target_price, days_to_expiration):\n",
        "    if not options_data or not isinstance(options_data, dict):\n",
        "        return 0  # Return 0 probability if no valid options data\n",
        "\n",
        "    probabilities = []\n",
        "\n",
        "    epsilon = 1e-8  # Small value to prevent division by zero\n",
        "\n",
        "    for date, expiration_data in options_data.items():\n",
        "        calls = expiration_data.get('calls')\n",
        "        puts = expiration_data.get('puts')\n",
        "\n",
        "        if not isinstance(calls, pd.DataFrame) or not isinstance(puts, pd.DataFrame):\n",
        "            continue  # Skip this expiration date if data is not in expected format\n",
        "\n",
        "        # Calculate implied volatility\n",
        "        avg_call_iv = calls['impliedVolatility'].mean() if 'impliedVolatility' in calls.columns else 0\n",
        "        avg_put_iv = puts['impliedVolatility'].mean() if 'impliedVolatility' in puts.columns else 0\n",
        "        avg_iv = (avg_call_iv + avg_put_iv) / 2\n",
        "\n",
        "        if avg_iv == 0:\n",
        "            avg_iv = epsilon\n",
        "\n",
        "        # Calculate the probability using the Black-Scholes model\n",
        "        time_to_expiration = days_to_expiration / 252\n",
        "        log_return = np.log(target_price / current_price)\n",
        "        d1 = (log_return + (avg_iv ** 2 / 2) * time_to_expiration) / (avg_iv * np.sqrt(time_to_expiration))\n",
        "\n",
        "        probability = norm.cdf(d1)\n",
        "        probabilities.append(probability)\n",
        "\n",
        "    return np.mean(probabilities) if probabilities else 0\n",
        "\n",
        "def analyze_financial_data(stock_data):\n",
        "    quarterly_financials = stock_data.get('quarterly_financials')\n",
        "    institutional_holders = stock_data.get('institutional_holders')\n",
        "    mutual_fund_holders = stock_data.get('mutual_fund_holders')\n",
        "    shares_outstanding = stock_data.get('key_stats', {}).get('sharesOutstanding', 0)\n",
        "\n",
        "    analysis = \"Financial Data Analysis:\\n\\n\"\n",
        "\n",
        "    # Quarterly financials analysis\n",
        "    if quarterly_financials is not None and not quarterly_financials.empty:\n",
        "        key_metrics = ['Total Revenue', 'Operating Income', 'Net Income']\n",
        "        growth_rates = {}\n",
        "        yoy_growth_rates = {}\n",
        "\n",
        "        for metric in key_metrics:\n",
        "            if metric in quarterly_financials.index:\n",
        "                values = quarterly_financials.loc[metric]\n",
        "                if len(values) > 1:\n",
        "                    # Calculate growth rates, handling division by zero\n",
        "                    growth_rates[metric] = values.pct_change().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                if len(values) >= 4:\n",
        "                    yoy_growth_rates[metric] = values.pct_change(periods=4).replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        analysis += \"Quarterly Financial Analysis:\\n\\n\"\n",
        "\n",
        "        for metric, rates in growth_rates.items():\n",
        "            analysis += f\"{metric} Quarter-over-Quarter Growth Rates:\\n\"\n",
        "            for date, rate in rates.items():\n",
        "                analysis += f\"  {date.strftime('%Y-Q%q')}: {rate:.2%}\\n\"\n",
        "            analysis += \"\\n\"\n",
        "\n",
        "        for metric, rates in yoy_growth_rates.items():\n",
        "            analysis += f\"{metric} Year-over-Year Growth Rates:\\n\"\n",
        "            for date, rate in rates.items():\n",
        "                analysis += f\"  {date.strftime('%Y-Q%q')}: {rate:.2%}\\n\"\n",
        "            analysis += \"\\n\"\n",
        "\n",
        "        avg_growth_rates = {metric: rates.mean() for metric, rates in growth_rates.items()}\n",
        "        analysis += \"Average Quarter-over-Quarter Growth Rates:\\n\"\n",
        "        for metric, avg_rate in avg_growth_rates.items():\n",
        "            analysis += f\"  {metric}: {avg_rate:.2%}\\n\"\n",
        "\n",
        "        analysis += \"\\nTrends and Patterns:\\n\"\n",
        "        for metric, rates in growth_rates.items():\n",
        "            if len(rates) >= 4:\n",
        "                recent_trend = rates.iloc[-4:].mean()\n",
        "                previous_trend = rates.iloc[-8:-4].mean() if len(rates) >= 8 else None\n",
        "                if previous_trend is not None:\n",
        "                    if recent_trend > previous_trend:\n",
        "                        analysis += f\"  {metric} shows accelerating growth over the last 4 quarters.\\n\"\n",
        "                    elif recent_trend < previous_trend:\n",
        "                        analysis += f\"  {metric} shows decelerating growth over the last 4 quarters.\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  {metric} shows stable growth over the last 4 quarters.\\n\"\n",
        "                else:\n",
        "                    if recent_trend > 0:\n",
        "                        analysis += f\"  {metric} shows positive growth trend over the last 4 quarters.\\n\"\n",
        "                    elif recent_trend < 0:\n",
        "                        analysis += f\"  {metric} shows negative growth trend over the last 4 quarters.\\n\"\n",
        "                    else:\n",
        "                        analysis += f\"  {metric} shows stable growth over the last 4 quarters.\\n\"\n",
        "    else:\n",
        "        analysis += \"No quarterly financial data available.\\n\\n\"\n",
        "\n",
        "    # Institutional holders analysis\n",
        "    if institutional_holders is not None and not institutional_holders.empty:\n",
        "        total_shares_held = institutional_holders['Shares'].sum()\n",
        "        top_10_holders = institutional_holders.nlargest(10, 'Shares')\n",
        "\n",
        "        analysis += f\"\"\"\n",
        "    Institutional Holders Analysis:\n",
        "    - Total number of institutional holders: {len(institutional_holders)}\n",
        "    - Total shares held by institutions: {total_shares_held:,}\n",
        "\n",
        "    Top 10 Institutional Holders:\n",
        "    \"\"\"\n",
        "\n",
        "        for _, holder in top_10_holders.iterrows():\n",
        "            shares = holder['Shares']\n",
        "            percentage = (shares / shares_outstanding) * 100 if shares_outstanding > 0 else 0\n",
        "            analysis += f\"- {holder['Holder']}: {shares:,} shares ({percentage:.2f}% of outstanding)\\n\"\n",
        "\n",
        "        total_institutional_ownership = (total_shares_held / shares_outstanding) * 100 if shares_outstanding > 0 else 0\n",
        "        analysis += f\"\\nTotal institutional ownership: {total_institutional_ownership:.2f}%\\n\"\n",
        "    else:\n",
        "        analysis += \"No institutional holders data available.\\n\"\n",
        "\n",
        "    # Mutual fund holders analysis\n",
        "    if mutual_fund_holders is not None and not mutual_fund_holders.empty:\n",
        "        total_shares_held = mutual_fund_holders['Shares'].sum()\n",
        "        top_10_holders = mutual_fund_holders.nlargest(10, 'Shares')\n",
        "\n",
        "        analysis += f\"\"\"\n",
        "    Mutual Fund Holders Analysis:\n",
        "    - Total number of mutual fund holders: {len(mutual_fund_holders)}\n",
        "    - Total shares held by mutual funds: {total_shares_held:,}\n",
        "\n",
        "    Top 10 Mutual Fund Holders:\n",
        "    \"\"\"\n",
        "\n",
        "        for _, holder in top_10_holders.iterrows():\n",
        "            shares = holder['Shares']\n",
        "            percentage = (shares / shares_outstanding) * 100 if shares_outstanding > 0 else 0\n",
        "            analysis += f\"- {holder['Holder']}: {shares:,} shares ({percentage:.2f}% of outstanding)\\n\"\n",
        "\n",
        "    else:\n",
        "        analysis += \"No mutual fund holders data available.\\n\"\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def fetch_weekly_data(ticker):\n",
        "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={ticker}&apikey={API_KEY}'\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "\n",
        "    if 'Weekly Time Series' not in data:\n",
        "        print(f\"Error: Unable to fetch data for {ticker}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    weekly_data = data['Weekly Time Series']\n",
        "    df = pd.DataFrame.from_dict(weekly_data, orient='index')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Convert columns to numeric\n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col])\n",
        "\n",
        "    # Rename columns\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "    # Filter for the last 25 years\n",
        "    twenty_years_ago = datetime.now() - timedelta(days=25*365)\n",
        "    df = df[df.index >= twenty_years_ago]\n",
        "\n",
        "    return df\n",
        "\n",
        "def kalman_filter_prediction(prices, time_periods):\n",
        "    # Calculate returns\n",
        "    returns = np.diff(np.log(prices))\n",
        "\n",
        "    # Initialize Kalman Filter\n",
        "    kf = KalmanFilter(\n",
        "        initial_state_mean=0,\n",
        "        n_dim_obs=1,\n",
        "        n_dim_state=1,\n",
        "        transition_matrices=[1],\n",
        "        observation_matrices=[1],\n",
        "        observation_covariance=0.1,\n",
        "        transition_covariance=0.01\n",
        "    )\n",
        "\n",
        "    # Fit the filter\n",
        "    state_means, _ = kf.filter(returns)\n",
        "\n",
        "    # Predict future returns\n",
        "    future_returns, _ = kf.smooth(state_means)\n",
        "\n",
        "    # Convert returns to prices for each time period\n",
        "    last_price = prices[-1]\n",
        "    predictions = []\n",
        "    for period in time_periods:\n",
        "        steps = int(period * 52)  # Convert years to weeks\n",
        "        future_price = last_price * np.exp(np.sum(future_returns[-steps:]))\n",
        "        predictions.append(future_price)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "def perform_monte_carlo_price_analysis(ticker, stock_data, target_prices):\n",
        "    weekly_data = fetch_weekly_data(ticker)\n",
        "\n",
        "    if weekly_data.empty:\n",
        "        return f\"Error: Unable to fetch weekly data for {ticker}\", {}, 0\n",
        "\n",
        "    current_price = weekly_data['Close'].iloc[-1]\n",
        "    returns = weekly_data['Close'].pct_change().dropna()\n",
        "\n",
        "    analysis = f\"\"\"\n",
        "    Historical Price Movement Analysis for {ticker}:\n",
        "    Current Price: ${current_price:.2f}\n",
        "\n",
        "    Target Prices and Probabilities:\n",
        "    \"\"\"\n",
        "\n",
        "    time_periods = [1, 2, 3, 4, 5]\n",
        "\n",
        "    historical_volatility = returns.std() * np.sqrt(52)  # Annualized volatility\n",
        "    expected_growth_rate = returns.mean() * 52  # Annualized expected return\n",
        "\n",
        "    num_simulations = 100000\n",
        "\n",
        "    probability_stats = {}\n",
        "    weighted_sum = 0\n",
        "    weight_sum = 0\n",
        "\n",
        "    for i, period in enumerate(time_periods):\n",
        "        target_price = target_prices[period]\n",
        "        target_gain = (target_price / current_price - 1) * 100\n",
        "\n",
        "        if period < 1:\n",
        "            weeks = int(period * 52)\n",
        "            analysis += f\"{weeks} weeks: ${target_price:.2f} (Gain: {target_gain:.2f}%)\\n\"\n",
        "        else:\n",
        "            analysis += f\"{int(period)} years: ${target_price:.2f} (Gain: {target_gain:.2f}%)\\n\"\n",
        "\n",
        "        # Run Monte Carlo simulation\n",
        "        simulated_returns = np.random.normal(\n",
        "            (expected_growth_rate - 0.5 * historical_volatility**2) * period,\n",
        "            historical_volatility * np.sqrt(period),\n",
        "            num_simulations\n",
        "        )\n",
        "        simulated_prices = current_price * np.exp(simulated_returns)\n",
        "\n",
        "        probabilities = (simulated_prices >= target_price).mean()\n",
        "\n",
        "        # Calculate percentiles of probabilities from multiple simulations\n",
        "        prob_samples = []\n",
        "        for _ in range(1000):  # Run 1000 sets of simulations\n",
        "            sim_returns = np.random.normal(\n",
        "                (expected_growth_rate - 0.5 * historical_volatility**2) * period,\n",
        "                historical_volatility * np.sqrt(period),\n",
        "                1000  # Use a smaller number of simulations for each sample\n",
        "            )\n",
        "            sim_prices = current_price * np.exp(sim_returns)\n",
        "            prob_samples.append((sim_prices >= target_price).mean())\n",
        "\n",
        "        stats = {\n",
        "            'min': np.min(prob_samples),\n",
        "            '25th': np.percentile(prob_samples, 25),\n",
        "            'median': np.median(prob_samples),\n",
        "            '75th': np.percentile(prob_samples, 75),\n",
        "            'max': np.max(prob_samples)\n",
        "        }\n",
        "        probability_stats[period] = stats\n",
        "\n",
        "        analysis += f\"Probability of reaching target price: {probabilities:.2%}\\n\"\n",
        "        analysis += f\"25th-75th percentile range: {stats['25th']:.2%} - {stats['75th']:.2%}\\n\\n\"\n",
        "\n",
        "        weight = 1 / (i + 1)\n",
        "        weighted_sum += probabilities * weight\n",
        "        weight_sum += weight\n",
        "\n",
        "    weighted_avg_probability = weighted_sum / weight_sum if weight_sum > 0 else 0\n",
        "\n",
        "    return analysis, probability_stats, weighted_avg_probability\n",
        "\n",
        "\n",
        "def calculate_growth_rate(financials, metric):\n",
        "    if metric in financials.columns:\n",
        "        values = financials[metric].dropna()\n",
        "        if len(values) >= 2:\n",
        "            return np.clip((values.iloc[-1] / values.iloc[0]) ** (1 / len(values)) - 1, -0.5, 0.5)\n",
        "    return np.nan\n",
        "\n",
        "def fractal_dimension(series, eps_range):\n",
        "    series = (series - np.min(series)) / (np.max(series) - np.min(series))\n",
        "    n = len(series)\n",
        "    eps_range = np.logspace(np.log10(eps_range[0]), np.log10(eps_range[1]), num=20)\n",
        "    counts = []\n",
        "    for eps in eps_range:\n",
        "        count = np.sum(np.abs(series[1:] - series[:-1]) > eps)\n",
        "        counts.append(max(count, 1))  # Ensure non-zero counts\n",
        "    counts = np.array(counts)\n",
        "    slope, _, _, _, _ = linregress(np.log(1/eps_range), np.log(counts))\n",
        "    return max(slope, 1)  # Ensure dimension is at least 1\n",
        "\n",
        "def hurst_exponent(series, lags):\n",
        "    tau = []\n",
        "    lagvec = []\n",
        "    for lag in lags:\n",
        "        tau.append(np.sqrt(np.std(np.subtract(series[lag:], series[:-lag]))))\n",
        "        lagvec.append(lag)\n",
        "    slope, _, _, _, _ = linregress(np.log(lagvec), np.log(tau))\n",
        "    return max(min(slope, 1), 0)  # Ensure Hurst is between 0 and 1\n",
        "\n",
        "def lyapunov_exponent(series, lag, iterations):\n",
        "    if len(series) < lag + iterations:\n",
        "        return 0  # Return 0 if not enough data\n",
        "    n = len(series)\n",
        "    lya = np.zeros(iterations)\n",
        "    for i in range(iterations):\n",
        "        x = series[i]\n",
        "        for j in range(lag, n-1, lag):\n",
        "            nearest = np.argmin(np.abs(series[:-lag] - x))\n",
        "            d0 = max(abs(series[nearest] - x), 1e-10)  # Avoid division by zero\n",
        "            d1 = max(abs(series[nearest + lag] - series[j]), 1e-10)\n",
        "            lya[i] = np.log(d1 / d0) / lag\n",
        "    return np.mean(lya)\n",
        "\n",
        "def calculate_historical_avg_growth(weekly_data, years):\n",
        "    \"\"\"Calculate the average annual growth rate over the specified number of years.\"\"\"\n",
        "    if len(weekly_data) < 52 * years:\n",
        "        return 0  # Not enough data\n",
        "\n",
        "    start_price = weekly_data['Close'].iloc[-52 * years]\n",
        "    end_price = weekly_data['Close'].iloc[-1]\n",
        "\n",
        "    return (end_price / start_price) ** (1 / years) - 1\n",
        "\n",
        "def estimate_target_price(ticker, stock_data):\n",
        "    current_price = get_current_price(ticker)\n",
        "    weekly_data = fetch_weekly_data(ticker)\n",
        "    financials = stock_data['financials']\n",
        "    key_stats = stock_data.get('key_stats', {})\n",
        "\n",
        "    if weekly_data.empty or len(weekly_data) < 52:  # Require at least a year of weekly data\n",
        "        return {period: current_price for period in [1, 2, 3, 4, 5]}\n",
        "\n",
        "    returns = weekly_data['Close'].pct_change().dropna()\n",
        "    historical_growth = returns.mean() * 52\n",
        "    historical_volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "    revenue_growth = calculate_growth_rate(financials, 'Total Revenue')\n",
        "    earnings_growth = calculate_growth_rate(financials, 'Net Income')\n",
        "\n",
        "    growth_estimates = [historical_growth, revenue_growth, earnings_growth]\n",
        "    growth_factor = np.nanmean([g for g in growth_estimates if not np.isnan(g)])\n",
        "\n",
        "    # Adjust growth factor based on market cap\n",
        "    market_cap = key_stats.get('marketCap', 0)\n",
        "    if market_cap > 200e9:  # Large cap\n",
        "        growth_factor *= 0.8\n",
        "    elif market_cap > 10e9:  # Mid cap\n",
        "        growth_factor *= 0.9\n",
        "    elif market_cap > 2e9:  # Small cap\n",
        "        growth_factor *= 1.1\n",
        "    else:  # Micro cap\n",
        "        growth_factor *= 1.2\n",
        "\n",
        "    # Cap the growth factor\n",
        "    growth_factor = min(growth_factor, 0.5)  # Maximum 50% annual growth\n",
        "\n",
        "    try:\n",
        "        model = auto_arima(weekly_data['Close'], start_p=1, start_q=1, max_p=5, max_q=5, m=1,\n",
        "                           start_P=0, seasonal=False, d=1, D=1, trace=False,\n",
        "                           error_action='ignore', suppress_warnings=True, stepwise=True)\n",
        "        arima_forecast = model.predict(n_periods=52*5)\n",
        "    except:\n",
        "        arima_forecast = pd.Series([current_price] * (52*5))\n",
        "\n",
        "    try:\n",
        "        scaled_returns = returns * 100\n",
        "        garch_model = arch_model(scaled_returns, vol='Garch', p=1, q=1)\n",
        "        garch_result = garch_model.fit(disp='off')\n",
        "        garch_forecast = garch_result.forecast(horizon=52*5)\n",
        "        garch_volatility = np.sqrt(garch_forecast.variance.values[-1, :]) / 100\n",
        "    except:\n",
        "        garch_volatility = np.array([historical_volatility] * (52*5))\n",
        "\n",
        "    prices = weekly_data['Close'].values\n",
        "    wavelet = 'db8'\n",
        "    level = 5\n",
        "    coeffs = pywt.wavedec(prices, wavelet, level=level)\n",
        "    reconstructed = pywt.waverec([coeffs[0]] + [None] * level, wavelet)\n",
        "    trend = reconstructed[:len(prices)]\n",
        "    trend_growth = (trend[-1] / trend[0]) ** (52 / len(trend)) - 1\n",
        "\n",
        "    scales = np.arange(1, 52)\n",
        "    cwtmatr, _ = pywt.cwt(prices, scales, 'morl')\n",
        "    dominant_period = scales[np.argmax(np.sum(np.abs(cwtmatr)**2, axis=1))]\n",
        "\n",
        "    returns = np.diff(np.log(prices))\n",
        "    lyap_exp = max(lyapunov_exponent(returns, lag=5, iterations=min(1000, len(returns))), 0)\n",
        "    frac_dim = fractal_dimension(prices, eps_range=(0.01, 0.5))\n",
        "    hurst_exp = hurst_exponent(prices, lags=range(2, min(100, len(prices))))\n",
        "\n",
        "    target_prices = {}\n",
        "    time_periods = [1, 2, 3, 4, 5]\n",
        "\n",
        "    # Calculate historical average growth\n",
        "    historical_avg_growth = calculate_historical_avg_growth(weekly_data, 10)  # 5-year historical average\n",
        "\n",
        "    for i, period in enumerate(time_periods):\n",
        "        historical_target = current_price * (1 + growth_factor) ** period\n",
        "\n",
        "        forecast_index = min(int(period * 52) - 1, len(arima_forecast) - 1)\n",
        "        arima_target = arima_forecast.iloc[forecast_index]\n",
        "\n",
        "        num_simulations = 100000\n",
        "        z = np.random.standard_normal(num_simulations)\n",
        "        vol_index = min(int(period * 52) - 1, len(garch_volatility) - 1)\n",
        "        simulated_returns = (growth_factor - 0.5 * garch_volatility[vol_index]**2) * period + \\\n",
        "                            garch_volatility[vol_index] * np.sqrt(period) * z\n",
        "        simulated_prices = current_price * np.exp(simulated_returns)\n",
        "        monte_carlo_target = np.percentile(simulated_prices, 75)\n",
        "\n",
        "        cycle_adjustment = np.sin(2 * np.pi * period / (dominant_period / 52))\n",
        "        wavelet_target = current_price * (1 + trend_growth + 0.1 * cycle_adjustment) ** period\n",
        "\n",
        "        # Improved chaos-fractal target calculation\n",
        "        chaos_factor = np.clip(1 + lyap_exp * period, 0.9, 1.1)\n",
        "        fractal_factor = np.clip(frac_dim, 1, 1.1)\n",
        "        hurst_factor = np.clip(2 * (hurst_exp - 0.5), -0.1, 0.1)\n",
        "\n",
        "        chaos_fractal_growth = growth_factor * chaos_factor * fractal_factor\n",
        "        chaos_fractal_growth *= (1 + hurst_factor)\n",
        "\n",
        "        # Apply a dampening factor for long-term predictions\n",
        "        dampening_factor = np.exp(-0.2 * period)\n",
        "        chaos_fractal_growth *= dampening_factor\n",
        "\n",
        "        chaos_fractal_target = current_price * (1 + chaos_fractal_growth) ** period\n",
        "\n",
        "        # Combine targets using weighted average\n",
        "        combined_target = (historical_target + arima_target + monte_carlo_target +\n",
        "                           wavelet_target + chaos_fractal_target) / 5\n",
        "\n",
        "        # Apply a more aggressive dampening factor for long-term predictions\n",
        "        if period > 1:\n",
        "            long_term_dampening = np.exp(-0.2 * (period - 1))\n",
        "            combined_target *= long_term_dampening\n",
        "\n",
        "        # Apply market cap based adjustment\n",
        "        if market_cap > 200e9:  # Large cap\n",
        "            combined_target *= (1 - 0.1 * (period - 1))\n",
        "        elif market_cap < 2e9:  # Micro cap\n",
        "            combined_target *= (1 + 0.05 * (period - 1))\n",
        "\n",
        "        # Implement a maximum growth cap that decreases over time\n",
        "        max_growth_cap = min(2.0, 2.0 - 0.2 * (period - 1))\n",
        "        max_target = current_price * (1 + max_growth_cap) ** period\n",
        "        combined_target = min(combined_target, max_target)\n",
        "\n",
        "        # Implement a mean reversion component for long-term projections\n",
        "        if period > 1:\n",
        "            mean_reversion_factor = 1 - (0.1 * (period - 1))\n",
        "            combined_target = combined_target * mean_reversion_factor + \\\n",
        "                              (current_price * (1 + historical_avg_growth) ** period) * (1 - mean_reversion_factor)\n",
        "\n",
        "        target_prices[period] = combined_target\n",
        "\n",
        "    return target_prices\n",
        "\n",
        "\n",
        "def get_industry_analysis(ticker, stock_data):\n",
        "    key_stats = stock_data.get('key_stats', {})\n",
        "    industry = key_stats.get('industry', 'Unknown')\n",
        "    sector = key_stats.get('sector', 'Unknown')\n",
        "\n",
        "    system_prompt = f\"\"\"As a seasoned hedge fund manager with a history of high returns and low risk from companies, provide a comprehensive analysis of the industry and the sector to inform investment strategies. Your analysis should include:\n",
        "\n",
        "    1. Industry Overview:\n",
        "       - Analyze the current market size and projected growth rate of the industry.\n",
        "       - Identify the top 3-5 key players and their respective market shares.\n",
        "       - Assess the industry's barriers to entry, technological advancements, and regulatory environment.\n",
        "\n",
        "    2. Emerging Trends and Disruptive Technologies:\n",
        "       - Identify and explain 2-3 major emerging trends shaping the industry.\n",
        "       - Discuss any disruptive technologies that could significantly impact the industry in the next 3-5 years.\n",
        "\n",
        "    3. Competitive Landscape:\n",
        "       - Analyze the competitive positioning of the leading companies, including their strengths and weaknesses.\n",
        "       - Assess the threat of new entrants and the bargaining power of suppliers and customers.\n",
        "\n",
        "    4. Growth Opportunities and Risks:\n",
        "       - Identify specific growth opportunities within the industry, such as new markets or product innovations.\n",
        "       - Outline the key risks and challenges facing the industry, including economic, regulatory, or technological factors.\n",
        "\n",
        "    Your analysis should be data-driven, providing specific figures and percentages where possible. Compare {ticker}'s performance and positioning against the industry benchmarks throughout the analysis.\n",
        "\n",
        "    Industry: {industry}\n",
        "    Sector: {sector}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Conduct a thorough analysis of the industry and the sector, focusing on the key factors mentioned in the system prompt. Your analysis should provide a clear understanding of the industry's dynamics, growth potential, and the most attractive investment opportunities, with a specific focus on how {ticker} compares to industry benchmarks.\"},\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "        \"anthropic-version\": \"2023-06-01\",\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": 'claude-3-haiku-20240307',\n",
        "        \"max_tokens\": 4000,\n",
        "        \"temperature\": 0.1,\n",
        "        \"system\": system_prompt,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        response_json = response.json()\n",
        "\n",
        "        if 'content' in response_json:\n",
        "            response_text = response_json['content'][0]['text']\n",
        "        else:\n",
        "            print(f\"Unexpected API response format for {ticker}. Full response: {response_json}\")\n",
        "            response_text = \"Error: Unable to parse API response.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request for {ticker}: {str(e)}\")\n",
        "        response_text = \"Error: Unable to perform industry analysis.\"\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def analyze_individual_stock(ticker, analyst_ratings, market_news_analysis, sector_peer_analysis, notes_analysis, float_analysis, insider_trades_analysis, historical_employee_analysis, revenue_analysis, ratio_analysis, dcf_analysis, growth_analysis, key_metrics_analysis, income_growth_analysis, cashflow_growth_analysis, balance_sheet_analysis, industry_analysis, options_analysis, financial_data_analysis, monte_carlo_analysis, current_price, target_prices, technical_analysis, risk_metrics, economic_analysis_result, market_crash_analysis, tda_results, summarized_advanced_analysis, financial_ratios, index_summary, string_theory_metrics, decomposition_summary, trend_analysis, transcript_analysis):\n",
        "\n",
        "    # Convert numpy types to Python types\n",
        "    risk_metrics = convert_to_serializable(risk_metrics)\n",
        "    financial_ratios = convert_to_serializable(financial_ratios)\n",
        "    tda_results = convert_to_serializable(tda_results)\n",
        "\n",
        "    # Handle the case where monte_carlo_analysis is a single value\n",
        "    if isinstance(monte_carlo_analysis, (str, float, int)):\n",
        "        monte_carlo_analysis_str = str(monte_carlo_analysis)\n",
        "        monte_carlo_probability_stats = {}\n",
        "        monte_carlo_weighted_avg_probability = 0\n",
        "    elif isinstance(monte_carlo_analysis, tuple) and len(monte_carlo_analysis) == 3:\n",
        "        monte_carlo_analysis_str = json.dumps(monte_carlo_analysis[0], indent=8)\n",
        "        monte_carlo_probability_stats = monte_carlo_analysis[1]\n",
        "        monte_carlo_weighted_avg_probability = monte_carlo_analysis[2]\n",
        "    else:\n",
        "        monte_carlo_analysis_str = \"Error: Unexpected format for monte_carlo_analysis\"\n",
        "        monte_carlo_probability_stats = {}\n",
        "        monte_carlo_weighted_avg_probability = 0\n",
        "\n",
        "    # Convert economic_analysis_result to string if it's not serializable\n",
        "    if callable(economic_analysis_result):\n",
        "        economic_analysis_str = \"Economic analysis function (not callable in this context)\"\n",
        "    else:\n",
        "        economic_analysis_str = str(economic_analysis_result)\n",
        "\n",
        "    system_prompt = f\"\"\"You are a seasoned hedge fund manager with a proven track record of generating high returns with low risk from companies. Your task is to thoroughly analyze the provided data for {ticker} and identify the most attractive investment opportunity with the highest potential for growth and the lowest risk. Your analysis should focus on the following key factors:\n",
        "\n",
        "        1. Analyze insider trades: {json.dumps(insider_trades_analysis, indent=8)}\n",
        "        2. Analyst Ratings: {json.dumps(analyst_ratings, indent=8)}\n",
        "        3. Industry Analysis: {json.dumps(industry_analysis, indent=8)}\n",
        "        4. Options Analysis (including Greeks: Delta, Gamma, Theta, Vega): {json.dumps(options_analysis, indent=8)}\n",
        "        5. Financial Data Analysis: {json.dumps(financial_data_analysis, indent=8)}\n",
        "        6. Monte Carlo Simulation Analysis: {monte_carlo_analysis_str}\n",
        "           Probability Statistics: {json.dumps(monte_carlo_probability_stats, indent=8)}\n",
        "           Weighted Average Probability: {monte_carlo_weighted_avg_probability:.2%}\n",
        "        7. Current Price: {json.dumps(current_price, indent=8)}\n",
        "        8. Estimated Target Prices: {json.dumps(target_prices, indent=8)}\n",
        "        9. Technical Analysis: {json.dumps(technical_analysis, indent=8)}\n",
        "        10. Risk Metrics: {json.dumps(risk_metrics, indent=8)}\n",
        "        11. Financial Ratios: {json.dumps(financial_ratios, indent=8)}\n",
        "        12. Economic Indicators Analysis: {economic_analysis_str}\n",
        "        13. Market Crash Analysis: {json.dumps(market_crash_analysis, indent=8)}\n",
        "        14. Topological Data Analysis Results: {json.dumps(tda_results, indent=8)}\n",
        "        15. Advanced Analysis Results: {json.dumps(summarized_advanced_analysis, indent=8, default=str)}\n",
        "        16. Index Data Summary: {json.dumps(index_summary, indent=8, default=str)}\n",
        "        17. String Theory Metrics: {json.dumps(string_theory_metrics, indent=8)}\n",
        "        18. Time Series Decomposition Summary: {json.dumps(decomposition_summary, indent=8, default=str)}\n",
        "        19. Trend Analysis: {json.dumps(trend_analysis, indent=8)}\n",
        "        20. Analyze Financial Growth: {json.dumps(growth_analysis, indent=8)}\n",
        "        21. Analyze Balance Sheet Growth: {json.dumps(balance_sheet_analysis, indent=8)}\n",
        "        22. Analyze Income Growth: {json.dumps(income_growth_analysis, indent=8)}\n",
        "        23. Analyze Cashflow Growth: {json.dumps(cashflow_growth_analysis, indent=8)}\n",
        "        24. Key Metrics Analysis: {json.dumps(key_metrics_analysis, indent=8)}\n",
        "        25. Ratio Analysis: {json.dumps(ratio_analysis, indent=8)}\n",
        "        26. DCF Analysis: {json.dumps(dcf_analysis, indent=8)}\n",
        "        27. Revenue Analysis: {json.dumps(revenue_analysis, indent=8)}\n",
        "        28. Sector Peer Analysis: {json.dumps(sector_peer_analysis, indent=8)}\n",
        "        29. Historical Employee Data Analysis: {json.dumps(historical_employee_analysis, indent=8)}\n",
        "        30. float analysis: {json.dumps(float_analysis, indent=8)}\n",
        "        31. Company Notes Analysis: {json.dumps(notes_analysis, indent=8)}\n",
        "        32. Market News Analysis: {json.dumps(market_news_analysis, indent=8)}\n",
        "        33. Earnings Call Transcript Analysis: {json.dumps(transcript_analysis, indent=8)}\n",
        "\n",
        "\n",
        "    Based on this comprehensive analysis, provide the following information and make sure it is numbered, single spaced, and in the same format below. The outlook must be accurate and backed by specific data points from the 31 data points:\n",
        "\n",
        "    1. Ticker symbol\n",
        "    2. Current price\n",
        "    3. Estimated upside/downside potential per time period from entire analysis (as a percentage xx.xx%, assign the appropriate value between 0-100%, based on the provided data), in the format below and based on the time lines\n",
        "          - 1 years xx.xx%\n",
        "          - 2 years: xx.xx%\n",
        "          - 3 years: xx.xx%\n",
        "          - 4 years: xx.xx%\n",
        "          - 5 years: xx.xx%\n",
        "    4. Estimated Probabilities of achieving upside/downside potential (as a percentage xx.xx%, assign the appropriate value between 0-100%, based on the Monte Carlo analysis), in the format below and based on the time lines. Include the median probability and the 25th-75th percentile range.\n",
        "          - 1 years: xx.xx% (25th-75th: xx.xx% - xx.xx%)\n",
        "          - 2 years: xx.xx% (25th-75th: xx.xx% - xx.xx%)\n",
        "          - 3 years: xx.xx% (25th-75th: xx.xx% - xx.xx%)\n",
        "          - 4 years: xx.xx% (25th-75th: xx.xx% - xx.xx%)\n",
        "          - 5 years: xx.xx% (25th-75th: xx.xx% - xx.xx%)\n",
        "    5. Weighted average probability of achieving the target price: xx.xx%\n",
        "    6. Investment thesis: (Format as a single line, 3-4 sentences, Do not use monte_carlo_analysis and target_prices for this score) Explain specific details why the stock will make a significant price move and based on what logic, citing specific data points from the analysis\n",
        "    7. Key catalyst or trend supporting the thesis: (Format as a single line, 3-4 sentences) Identify a specific, timeline-relevant catalyst or trend. Detail its potential impact on stock performance, quantifying the effect if possible.\n",
        "    8. Investment score: (Score: Format as xx/100, where 100 is highest confidence & Explanation: Format as a single line, 3-4 sentences). Provide a specific detailed explanation citing specific data points that support why you are rating the stock with this score. Include both positive factors and risks, Do not cite the estimate target price or monte carlo analysis.\n",
        "    9. Industry and sector: (Formated In a single line, single spaced. Include brief commentary on the industry's current state and future outlook)\n",
        "    10. Comprehensive Market Score: (Score: Format as x.x/10, where 10 is highest confidence & Explanation: Format as a single line). Calculate a weighted score using:\n",
        "    - Growth Profile (25%): Evaluate revenue growth, earnings growth, and market expansion potential\n",
        "    - Financial Health (25%): Assess balance sheet strength, cash flows, and debt metrics\n",
        "    - Sector Dynamics (20%): Consider industry position, competitive advantages, and market share trends\n",
        "    - Macro Influences (15%): Factor in interest rates, economic indicators, and regulatory environment\n",
        "    - Sentiment Indicators (15%): Include institutional ownership changes, short interest, and technical signals\n",
        "    Present your findings focusing solely on the requested information and in the requested format and make sure it is single spaced and number exactly as aboved, without additional commentary. Do Not Hallucinate or make up factual information. Ensure that your price target, expected return, and probabilities are based on a comprehensive analysis of all points provided. Do not use the monte_carlo_analysis and target_prices in the investment score\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Based on the comprehensive 33-point analysis in the system prompt, provide a clear recommendation on whether {ticker} is an attractive investment opportunity. Include an accurate price target, expected return, and probabilities of reaching the target price at different time intervals. Ensure your analysis takes into account all provided data points\"},\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "        \"anthropic-version\": \"2023-06-01\",\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": 'claude-3-haiku-20240307',\n",
        "        \"max_tokens\": 4000,\n",
        "        \"temperature\": 0.1,\n",
        "        \"system\": system_prompt,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "    response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "    response_json = response.json()\n",
        "\n",
        "    if 'content' in response_json:\n",
        "        response_text = response_json['content'][0]['text']\n",
        "        # Replace double newlines with single newlines\n",
        "        response_text = response_text.replace('\\n\\n', '\\n')\n",
        "    else:\n",
        "        print(f\"Unexpected API response format for {ticker}. Full response: {response_json}\")\n",
        "        response_text = \"Error: Unable to parse API response.\"\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def log_error(ticker, error_message):\n",
        "    with open('error_log.txt', 'a') as f:\n",
        "        f.write(f\"Error analyzing {ticker}: {error_message}\\n\")\n",
        "\n",
        "\n",
        "# Use the provided list of stocks\n",
        "tickers = ['QQQ','SPY','MGK','DIA']\n",
        "\n",
        "# User input for analysis parameters\n",
        "years = 5\n",
        "\n",
        "# Define the list of indices\n",
        "indices = [\n",
        "    '^GSPC', '^DJI', '^IXIC', '^RUT', '^FTSE', '^GDAXI', '^FCHI', '^N225', '^HSI', '000001.SS', '^BSESN', '^AXJO', '^GSPTSE',\n",
        "    'XLK', 'XLF', 'XLV', 'XLE', 'XLY', 'XLI', 'XLP', 'XLB', 'XLU', 'XLRE', 'XLC',  # Sector ETFs\n",
        "    'VTI'  #total Market ETF\n",
        "]\n",
        "\n",
        "# Calculate start and end dates\n",
        "end_date = datetime.now().date()\n",
        "start_date = end_date - timedelta(days=years*252)\n",
        "\n",
        "# Fetch index data once\n",
        "index_data = get_index_data(indices, start_date, end_date)\n",
        "\n",
        "# Summarize index data once\n",
        "index_summary = summarize_index_data(index_data=index_data)\n",
        "\n",
        "# Fetch economic indicators once, outside the loop\n",
        "economic_data = get_economic_indicators()\n",
        "economic_analysis_result = analyze_economic_indicators(economic_data)\n",
        "economic_analysis_str = str(economic_analysis_result)\n",
        "\n",
        "# Fetch VIX data once\n",
        "market_crash_analysis = get_market_crash_risk()\n",
        "\n",
        "#Fetch Market news\n",
        "categorized_articles, market_news_analysis = analyze_market_news(api_key)\n",
        "\n",
        "# Perform analysis for each company\n",
        "analyses = {}\n",
        "prices = {}\n",
        "stock_data = {}\n",
        "error_box = {}\n",
        "with tqdm(total=len(tickers), desc=\"Analyzing Tickers\", unit=\"ticker\") as pbar:\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            pbar.set_description(f\"Analyzing {ticker}\")\n",
        "            ticker_data = get_stock_data(ticker, years)\n",
        "            if ticker_data is None or ticker_data['hist_data'].empty:\n",
        "                raise ValueError(f\"Unable to retrieve data or historical data is empty for {ticker}\")\n",
        "            stock_data[ticker] = ticker_data\n",
        "\n",
        "            # Add correlations to index summary\n",
        "            ticker_index_summary = copy.deepcopy(index_summary)\n",
        "            for index in ticker_index_summary:\n",
        "                ticker_index_summary[index]['correlation'] = ticker_data['index_correlations'][index]\n",
        "\n",
        "            current_price = get_current_price(ticker)\n",
        "            prices[ticker] = current_price\n",
        "\n",
        "            trend_analysis = get_trend_analysis(ticker, api_key)\n",
        "            analyst_ratings = get_analyst_ratings(ticker, ticker_data)\n",
        "            industry_analysis = get_industry_analysis(ticker, ticker_data)\n",
        "            financial_ratios = calculate_financial_ratios(ticker_data)\n",
        "            options_analysis = analyze_options_data(ticker_data['options_data'], current_price)\n",
        "            financial_data_analysis = analyze_financial_data(ticker_data)\n",
        "            ticker_data['financial_data_analysis'] = financial_data_analysis\n",
        "            ticker_data['financial_ratios'] = financial_ratios\n",
        "            decomposition_results = perform_time_series_decomposition(ticker_data['hist_data'])\n",
        "            decomposition_summary = summarize_decomposition(decomposition_results)\n",
        "            ticker_data['decomposition_summary'] = decomposition_summary\n",
        "            insider_trades_analysis = analyze_insider_trades(ticker, api_key)\n",
        "            growth_analysis = analyze_financial_growth(ticker, api_key)\n",
        "            balance_sheet_analysis = analyze_balance_sheet_growth(ticker, api_key)\n",
        "            income_growth_analysis = analyze_income_growth(ticker, api_key)\n",
        "            cashflow_growth_analysis = analyze_cashflow_growth(ticker, api_key)\n",
        "            key_metrics_analysis = analyze_key_metrics(ticker, api_key)\n",
        "            ratio_analysis = analyze_financial_ratios(ticker, api_key)\n",
        "            dcf_analysis = perform_dcf_analysis(ticker, api_key)\n",
        "            historical_employee_analysis = analyze_historical_employee_data(ticker, api_key)\n",
        "            float_analysis = analyze_historical_share_float(ticker, api_key)\n",
        "            notes_analysis = analyze_company_notes(ticker, api_key)\n",
        "\n",
        "            technical_analysis = analyze_technical_indicators(ticker_data['hist_data'])\n",
        "            risk_metrics = ticker_data['risk_metrics']\n",
        "            target_prices = estimate_target_price(ticker, ticker_data)\n",
        "            monte_carlo_analysis = (ticker_data['monte_carlo_analysis'], ticker_data['probability_stats'], ticker_data['weighted_avg_probability'])\n",
        "\n",
        "            # Handle the case where monte_carlo_analysis is a single value\n",
        "            if isinstance(monte_carlo_analysis, (str, float, int)):\n",
        "                monte_carlo_analysis_str = str(monte_carlo_analysis)\n",
        "                monte_carlo_probability_stats = {}\n",
        "                monte_carlo_weighted_avg_probability = 0\n",
        "            elif isinstance(monte_carlo_analysis, tuple) and len(monte_carlo_analysis) == 3:\n",
        "                monte_carlo_analysis_str = json.dumps(monte_carlo_analysis[0], indent=8)\n",
        "                monte_carlo_probability_stats = monte_carlo_analysis[1]\n",
        "                monte_carlo_weighted_avg_probability = monte_carlo_analysis[2]\n",
        "            else:\n",
        "                monte_carlo_analysis_str = \"Error: Unexpected format for monte_carlo_analysis\"\n",
        "                monte_carlo_probability_stats = {}\n",
        "                monte_carlo_weighted_avg_probability = 0\n",
        "\n",
        "            tda_results = perform_tda(ticker_data['hist_data'])\n",
        "            advanced_analysis_results = perform_advanced_analysis(ticker_data['hist_data'])\n",
        "            summarized_advanced_analysis = summarize_advanced_analysis(advanced_analysis_results)\n",
        "            ticker_data['advanced_analysis_results'] = summarized_advanced_analysis\n",
        "            sector_peer_analysis = analyze_sector_peers(ticker, api_key)\n",
        "\n",
        "            # Add the new revenue analysis\n",
        "            revenue_analysis = analyze_revenue_by_segment_and_location(ticker, api_key)\n",
        "\n",
        "\n",
        "            # Fetch earning call transcripts\n",
        "            transcripts = get_earning_call_transcripts(ticker, api_key, months=18)\n",
        "\n",
        "            # Analyze transcripts\n",
        "            transcript_analysis = analyze_transcripts_with_claude(transcripts, ticker)\n",
        "\n",
        "            # Calculate string theory metrics\n",
        "            string_theory_metrics = calculate_string_theory_metrics(ticker_data['hist_data'])\n",
        "            ticker_data['string_theory_metrics'] = string_theory_metrics\n",
        "\n",
        "            individual_analysis = analyze_individual_stock(\n",
        "                ticker,\n",
        "                analyst_ratings,\n",
        "                market_news_analysis,\n",
        "                industry_analysis,\n",
        "                options_analysis,\n",
        "                financial_data_analysis,\n",
        "                monte_carlo_analysis,\n",
        "                current_price,\n",
        "                target_prices,\n",
        "                technical_analysis,\n",
        "                risk_metrics,\n",
        "                economic_analysis_result,\n",
        "                market_crash_analysis,\n",
        "                tda_results,\n",
        "                summarized_advanced_analysis,\n",
        "                financial_ratios,\n",
        "                ticker_index_summary,\n",
        "                string_theory_metrics,\n",
        "                decomposition_summary,\n",
        "                trend_analysis,\n",
        "                insider_trades_analysis,\n",
        "                growth_analysis,\n",
        "                balance_sheet_analysis,\n",
        "                income_growth_analysis,\n",
        "                cashflow_growth_analysis,\n",
        "                key_metrics_analysis,\n",
        "                ratio_analysis,\n",
        "                dcf_analysis,\n",
        "                revenue_analysis,\n",
        "                sector_peer_analysis,\n",
        "                historical_employee_analysis,\n",
        "                float_analysis,\n",
        "                notes_analysis,\n",
        "                transcript_analysis,\n",
        "            )\n",
        "\n",
        "\n",
        "            # Print the analysis immediately after it's completed\n",
        "            print(f\"\\nAnalyses for {ticker}:\")\n",
        "\n",
        "            print(individual_analysis.replace('\\n\\n', '\\n'))\n",
        "\n",
        "            analyses[ticker] = individual_analysis\n",
        "\n",
        "            #tester\n",
        "            #print(\"12. Economic Indicators Analysis:\", economic_analysis_str)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error analyzing {ticker}: {str(e)}\"\n",
        "            print(error_message)\n",
        "            log_error(ticker, str(e))\n",
        "            error_box[ticker] = error_message\n",
        "\n",
        "            import traceback\n",
        "            print(f\"Full traceback for {ticker}:\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "        finally:\n",
        "            pbar.update(1)\n",
        "\n",
        "# Print error box\n",
        "if error_box:\n",
        "    print(\"\\nError Box:\")\n",
        "    for ticker, error in error_box.items():\n",
        "        print(f\"{ticker}: {error}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}